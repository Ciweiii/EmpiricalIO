---
title: 'Assignment 8: Dynamic Game'
author: "Kohei Kawaguchi"
date: "2019/1/29"
output:
  pdf_document: default
  html_document: default
---

```{r, include = FALSE}
rm(list = ls())
library(EmpiricalIO)
library(magrittr)
library(stargazer)
library(knitr)
library(foreach)
library(ggplot2)
library(latex2exp)
library(codetools)
library(doParallel)
registerDoParallel()
knitr::opts_chunk$set(cache = FALSE, echo = TRUE, fig.align = "center")
dir.create(
  "lecture/data/a8" %>% here::here(), 
  showWarnings = FALSE, 
  recursive = TRUE
)
```

## Simulate data

Suppose that there are $m = 1, \cdots, M$ markets and in each market there are $i = 1, \cdots, N$ firms and each firm makes decisions for $t = 1, \cdots, \infty$. In the following, I suppress the index of market, $m$. We solve the model under the infinite-horizon assumption, but generate data only for $t = 1, \cdots, T$. There are $L = 5$ state $\{1, 2, 3, 4, 5\}$ states for each firm. Each firm can choose $K + 1 = 2$ actions $\{0, 1\}$. Thus, $m_a := (K + 1)^N$ and $m_s = L^N$. Let $a_i$ and $s_i$ be firm $i$'s action and state and $a$ and $s$ are vectors of individual actions and states. 

The mean period payoff to firm $i$ is:
$$
\pi_i(a, s) := \tilde{\pi}(a_i, s_i, \overline{s}) :=  \alpha \ln s_i - \eta \ln s_i \sum_{j \neq i} \ln s_j - \beta a_i,
$$
where $\alpha, \beta, \eta> 0$, and $\alpha > \eta$. The term $\eta$ means that the returns to investment decreases as rival's average state profile improves. The period payoff is:
$$
\tilde{\pi}(a_i, s_i, \overline{s})+ \epsilon_i(a_i),
$$
and $\epsilon_i(a_i)$ is an i.i.d. type-I extreme random variable that is independent of all the other variables.

At the beginning of each period, the state $s$ is realized and publicly observed. Then choice-specific shocks $\epsilon_i(a_i), a_i = 0, 1$ are realized and privately observed by firm $i = 1, \cdots, N$. Then each firm simultaneously chooses her action. Then, the game moves to next period. 

State transition is independent across firms conditional on individual state and action.

Suppose that $s_i > 1$ and $s_i < L$. If $a_i = 0$, the state stays at the same state with probability $1 - \kappa$ and moves down by 1 with probability $\kappa$. If $a = 1$, the state moves up by 1 with probability $\gamma$, moves down by 1 with probability $\kappa$, and stays at the same with probability $1 - \kappa - \gamma$. 

Suppose that $s_i = 1$. If $a_i = 0$, the state stays at the same state with probability 1. If $a_i = 1$, the state moves up by 1 with probability $\gamma$ and stays at the same with probability $1 - \gamma$. 

Suppose that $s_i = L$. If $a_i = 0$, the state stays at the same state with probability $1 - \kappa$ and moves down by 1 with probability $\kappa$. If $a = 1$, the state moves down by 1 with probability $\kappa$, and stays at the same with probability $1 - \kappa$. 

The mean period profit is summarized in $\Pi$ as:

$$
\Pi :=
\begin{pmatrix}
\pi(1, 1)\\
\vdots\\
\pi(m_a, 1)\\
\vdots \\
\pi(1, m_s)\\
\vdots\\
\pi(m_a, m_s)\\
\end{pmatrix}
$$

The transition law is summarized in $G$ as:

$$
g(a, s, s') := \mathbb{P}\{s_{t + 1} = s'|s_t = s, a_t = a\},
$$

$$
G := 
\begin{pmatrix}
g(1, 1, 1) & \cdots & g(1, 1, m_s)\\
\vdots & & \vdots \\
g(m_a, 1, 1) & \cdots & g(m_a, 1, m_s)\\
& \vdots & \\
g(1, m_s, 1) & \cdots & g(1, m_s, m_s)\\
\vdots & & \vdots \\
g(m_a, m_s, 1) & \cdots & g(m_a, m_s, m_s)\\
\end{pmatrix}.
$$
The discount factor is denoted by $\delta$. We simulate data for $M$ markets with $N$ firms for $T$ periods.

1. Set constants and parameters as follows:
```{r}
# set seed
set.seed(1)
# set constants 
L <- 5
K <- 1
T <- 100
N <- 3
M <- 1000
lambda <- 1e-10
# set parameters
alpha <- 1
eta <- 0.3
beta <- 2
kappa <- 0.1
gamma <- 0.6
delta <- 0.95
```

2. Write a function `compute_action_state_space(K, L, N)` that returns a data frame for action and state space. Returned objects are list of data frame `A` and `S`. In `A`, column `k` is the index of an action profile, `i` is the index of a firm, and `a` is the action of the firm. In `S`, column `l` is the index of an state profile, `i` is the index of a firm, and `s` is the state of the firm.

```{r}
output <- 
  compute_action_state_space(
    L = L,
    K = K, 
    N = N
    )
A <- output$A
head(A)
tail(A)
S <- output$S
head(S)
tail(S)
# dimension
m_a <- max(A$k); 
m_a
m_s <- max(S$l); 
m_s
```

3. Write function `compute_PI_game(alpha, beta, eta, A, S)` that returns a list of $\Pi_i$.

```{r}
PI <- 
  compute_PI_game(
    alpha = alpha, 
    beta = beta, 
    eta = eta, 
    A = A,
    S = S
    )
head(PI[[N]])
dim(PI[[N]])[1] == m_s * m_a
```

4. Write function `compute_G_game(g, A, S)` that converts an individual transition probability matrix into a joint transition probability matrix $G$.

```{r}
G_marginal <- 
  compute_G(
    kappa = kappa,
    gamma = gamma, 
    L = L, 
    K = K
    )
G <- 
  compute_G_game(
    G_marginal = G_marginal, 
    A = A, 
    S = S
    )
head(G)
dim(G)[1] == m_s * m_a
dim(G)[2] == m_s
```

The ex-ante-value function for a firm is written as a function of a conditional choice probability as follows:
$$
\varphi_i^{(\theta_1, \theta_2)}(p) := [I - \delta \Sigma(p) G]^{-1}[\Sigma(p)\Pi_i + D_i(p)],
$$
where $\theta_1 = (\alpha, \beta, \eta)$ and $\theta_2 = (\kappa, \gamma)$, $p_i(a_i|s)$ is the probability that firm $i$ choose action $a_i$ when the state profile is $s$, and:
$$
p(a|s) = \prod_{i = 1}^N p_i(a_i|s), 
$$

$$
p(s) = 
\begin{pmatrix}
p(1|s) \\
\vdots \\
p(m_a|s)
\end{pmatrix},
$$

$$
p = 
\begin{pmatrix}
p(1)\\
\vdots\\
p(m_s)
\end{pmatrix},
$$


$$
\Sigma(p) =
\begin{pmatrix}
p(1)' & & \\
 & \ddots & \\
 & & p(L)'
\end{pmatrix}
$$

and:

$$
D_i(p) =
\begin{pmatrix}
\sum_{k = 0}^K \mathbb{E}\{\epsilon_i^k|a_i = k, 1\}p_i(a_i = k|1)\\
\vdots\\
\sum_{k = 0}^K \mathbb{E}\{\epsilon_i^k|a_i = k, m_s\}p_i(a_i = k|m_s)
\end{pmatrix}.
$$


5. Write a function `initialize_p_marginal(A, S)` that defines an initial marginal condition choice probability. In the output `p_marginal`, `p` is the probability for firm `i` to take action `a` conditional on the state profile being `l`. Next, write a function `compute_p_joint(p_marginal, A, S)` that computes a corresponding joint conditional choice probability from a marginal conditional choice probability. In the output `p_joint`, `p` is the joint probability that firms take action profile `k` condition on the state profile being `l`. Finally, write a function `compute_p_marginal(p_joint, A, S)` that compute a corresponding marginal conditional choice probability from a joint conditional choice probability.

```{r}
# define a conditional choice probability for each firm
p_marginal <- 
  initialize_p_marginal(
    A = A,
    S = S
    )
p_marginal
dim(p_marginal)[1] == N * m_s * (K + 1)
# compute joint conditional choice probability from marginal probability
p_joint <- 
  compute_p_joint(
    p_marginal = p_marginal, 
    A = A, 
    S = S
    )
p_joint
dim(p_joint)[1] == m_s * m_a
# compute marginal conditional choice probability from joint probability
p_marginal_2 <- 
  compute_p_marginal(
    p_joint = p_joint, 
    A = A,
    S = S
    )
max(abs(p_marginal - p_marginal_2))
```


6. Write a function `compute_sigma(p_marginal, A, S)` that computes $\Sigma(p)$ given a joint conditional choice probability. Then, write a function `compute_D(p_marginal)` that returns a list of $D_i(p)$.

```{r}
# compute Sigma for ex-ante value function calculation
sigma <- 
  compute_sigma(
    p_marginal = p_marginal,
    A = A,
    S = S
    )
head(sigma)
dim(sigma)[1] == m_s
dim(sigma)[2] == m_s * m_a
# compute D for ex-ante value function calculation
D <- compute_D(p_marginal)
head(D[[N]])
dim(D[[N]])[1] == m_s
```


7. Write a function `compute_exante_value_game(p_marginal, A, S, PI, G, delta)` that returns a list of matrices whose $i$-th element represents the ex-ante value function given a conditional choice probability for firm $i$. 

```{r}
# compute ex-ante value funciton for each firm
V <- 
  compute_exante_value_game(
    p_marginal = p_marginal,
    A = A,
    S = S,
    PI = PI,
    G = G,
    delta = delta
    )
head(V[[N]])
dim(V[[N]])[1] == m_s
```

The optimal conditional choice probability is written as a function of an ex-ante value function and a conditional choice probability of others as follows:
$$
\Lambda_i^{(\theta_1, \theta_2)}(V_i, p_{-i})(a_i, s) := \frac{\exp\{\sum_{a_{-i}}p_{-i}(a_{-i}|s)[\pi_i(a_i, a_{-i}, s) + \delta \sum_{s'}V_i(s')g(a_i, a_{-i}, s, s')]\}}{\sum_{a_i'}\exp\{\sum_{a_{-i}}p_{-i}(a_{-i}|s)[\pi_i(a_i', a_{-i}, s) + \delta \sum_{s'}V_i(s')g(a_i', a_{-i}, s, s')]\}},
$$
where $V$ is an ex-ante value function.

8. Write a function `compute_profile_value_game(V, PI, G, delta, S, A)` that returns a data frame that contains information on value function at a state and action profile for each firm. In the output `value`, `i` is the index of a firm, `l` is the index of a state profile, `k` is the index of an action profile, and `value` is the value for the firm at the state and action profile.

```{r}
# compute state-action-profile value function
value <- 
  compute_profile_value_game(
    V = V, 
    PI = PI,
    G = G, 
    delta = delta, 
    S = S, 
    A = A
    )
value
dim(value)[1] == N * m_s * m_a
```

9. Write a function `compute_choice_value_game(p_marginal, V, PI, G, delta, A, S)` that computes a data frame that contains information on a choice-specific value function given an ex-ante value function and a conditional choice probability of others.

```{r}
# compute choice-specific value function
value <- 
  compute_choice_value_game(
    p_marginal = p_marginal,
    V = V,
    PI = PI,
    G = G, 
    delta = delta, 
    A = A, 
    S = S
    )
value
```

10. Write a function `compute_ccp_game(p_marginal, V, PI, G, delta, A, S)` that computes a data frame that contains information on a conditional choice probability given an ex-ante value function and a conditional choice probability of others.

```{r}
# compute conditional choice probability 
p_marginal <- 
  compute_ccp_game(
    p_marginal = p_marginal,
    V = V,
    PI = PI,
    G = G,
    delta = delta,
    A = A,
    S = S
    )
p_marginal
```





11. Write a function `solve_dynamic_game(PI, G, L, K, delta, lambda, A, S)` that find the equilibrium conditional choice probability and ex-ante value function by iterating the update of an ex-ante value function and a best-response conditional choice probability. The iteration should stop when $\max_s|V^{(r + 1)}(s) - V^{(r)}(s)| < \lambda$ with $\lambda = 10^{-10}$. There is no theoretical guarantee for the convergence.

```{r, eval = TRUE}
# solve the dynamic game model
output <-
  solve_dynamic_game(
    PI = PI,
    G = G, 
    L = L,
    K = K,
    delta = delta,
    lambda = lambda,
    A = A,
    S = S
    )
saveRDS(
  output, 
  file = "lecture/data/a8/equilibrium.rds" %>% here::here()
)
```

```{r}
output <- 
  readRDS(
    file = "lecture/data/a8/equilibrium.rds" %>% here::here()
  )
p_marginal <- output$p_marginal;
head(p_marginal)
V <- output$V[[N]];
head(V)
# compute joint conitional choice probability

p_joint <- 
  compute_p_joint(
    p_marginal = p_marginal, 
    A = A, 
    S = S
    );
head(p_joint)
```

12. Write a function `simulate_dynamic_game(p_joint, l, G, N, T, S, A, seed)` that simulate the data for a market starting from an initial state for $T$ periods. The function should accept a value of seed and set the seed at the beginning of the procedure inside the function, because the process is stochastic. To match the generated random numbers, for each period, generate action using `rmultinom` and then state using `rmultinom`.

```{r}
# simulate a dynamic game
# set initial state profile
l <- 1
# draw simulation for a firm
seed <- 1
df <- 
  simulate_dynamic_game(
    p_joint = p_joint,
    l = l,
    G = G,
    N = N,
    T = T,
    S = S, 
    A = A,
    seed = seed
    )
df
```

13. Write a function `simulate_dynamic_decision_across_markets(p_joint, l, G, N, T, M, S, A, seed)` that returns simulation data for $M$ markets. For firm $m$, set the seed at $m$

```{r, eval = TRUE}
# simulate data across markets
df <- 
  simulate_dynamic_decision_across_markets(
    p_joint = p_joint,
    l = l,
    G = G,
    N = N,
    T = T,
    M = M,
    S = S,
    A = A
    )
saveRDS(
  df,
  file = "lecture/data/a8/df.rds" %>% here::here()
)
```
```{r}
df <- 
  readRDS(
    file = "lecture/data/a8/df.rds" %>% here::here()
  )
df
summary(df)
```



14. Write a function `estimate_ccp_marginal_game(df)` that returns a non-parametric estimate of the marginal conditional choice probability for each firm in the data. Compare the estimated conditional choice probability and the true conditional choice probability by a bar plot.

```{r}
# non-parametrically estimate the conditional choice probability
p_marginal_est <- 
  estimate_ccp_marginal_game(df = df)
check_ccp <- 
  p_marginal_est %>%
  dplyr::rename(estimate = p) %>%
  dplyr::left_join(
    p_marginal, 
    by = c(
           "i",
           "l", 
           "a"
           )
    ) %>%
  dplyr::rename(true = p) %>%
  dplyr::filter(a == 1)
ggplot(
  data = check_ccp, 
  aes(
    x = true, 
    y = estimate
    )
  ) +
  geom_point() +
  labs(fill = "Value") +
  xlab("true") + 
  ylab("estimate")  + 
  theme_classic()
```

15. Write a function `estimate_G_marginal(df)` that returns a non-parametric estimate of the marginal transition probability matrix. Compare the estimated transition matrix and the true transition matrix by a bar plot.


```{r}
# non-parametrically estimate individual transition probability
G_marginal_est <- 
  estimate_G_marginal(df = df)
check_G <- 
  data.frame(
    type = "true",
    reshape2::melt(G_marginal)
    )
check_G_est <- 
  data.frame(
    type = "estimate",
    reshape2::melt(G_marginal_est)
    )
check_G <- 
  rbind(
    check_G, 
    check_G_est
    )
check_G$variable <- 
  paste(
    check_G$Var1,
    check_G$Var2, 
    sep = "_"
    )
ggplot(
  data = check_G, 
  aes(
    x = variable, 
    y = value,
    fill = type
    )
  ) +
    geom_bar(
      stat = "identity",
      position = "dodge"
      ) +
  labs(fill = "Value") + 
  xlab("action/state/state") +
  ylab("probability") +
  theme(axis.text.x = element_blank())  + 
  theme_classic()
```


## Estimate parameters

1. Vectorize the parameters as follows:

```{r}
theta_1 <- 
  c(
    alpha, 
    beta, 
    eta
    )
theta_2 <- 
  c(
    kappa, 
    gamma
    )
theta <- 
  c(
    theta_1, 
    theta_2
    )
```

We estimate the parameters by a CCP approach.

1. Write a function `estimate_theta_2_game(df)` that returns the estimates of $\kappa$ and $\gamma$ directly from data by counting relevant events.
```{r}
# estimate theta_2
theta_2_est <- 
  estimate_theta_2_game(df); 
theta_2_est
```

The objective function of the minimum distance estimator based on the conditional choice probability approach is:
$$
\frac{1}{N K m_s} \sum_{i = 1}^N \sum_{l = 1}^{m_s} \sum_{k = 1}^{K}\{\hat{p}_i(a_k|s_l) - p_i^{(\theta_1, \theta_2)}(a_k|s_l)\}^2,
$$
where $\hat{p}_i$ is the non-parametric estimate of the marginal conditional choice probability and $p_i^{(\theta_1, \theta_2)}$ is the marginal conditional choice probability under parameters $\theta_1$ and $\theta_2$ given $\hat{p}_i$. $a_k$ is $k$-th action for a firm and $s_l$ is $l$-th state profile.

2. Write a function `compute_CCP_objective_game(theta_1, theta_2, p_est, L, K, delta)` that returns the objective function of the above minimum distance estimator given a non-parametric estimate of the conditional choice probability and $\theta_1$ and $\theta_2$.


```{r, eval = TRUE}
# compute the objective function of the minimum distance estimator based on the CCP approach
objective <- 
  compute_CCP_objective_game(
    theta_1 = theta_1,
    theta_2 = theta_2, 
    p_marginal_est = p_marginal_est,
    A = A,
    S = S, 
    delta = delta, 
    lambda = lambda
    )
saveRDS(
  objective,
  file = "lecture/data/a8/objective.rds" %>% here::here()
)
```
```{r}
objective <- 
  readRDS(
    file = "lecture/data/a8/objective.rds" %>% here::here()
  )
objective
```


3. Check the value of the objective function around the true parameter.

```{r, eval = TRUE}
# label
label <- 
  c(
    "\\alpha",
    "\\beta",
    "\\eta"
    )
label <- 
  paste(
    "$",
    label,
    "$", 
    sep = ""
    )
# compute the graph
graph <- 
  foreach (
    i = 1:length(theta_1)
    ) %do% {
  theta_i <- theta_1[i]
  theta_i_list <- 
    theta_i * seq(
      0.5, 
      2,
      by = 0.2
      )
  objective_i <-
    foreach (
      j = 1:length(theta_i_list),
      .combine = "rbind"
      ) %dopar% {
               theta_ij <- theta_i_list[j]
               theta_j <- theta_1
               theta_j[i] <- theta_ij
               objective_ij <- 
                 compute_CCP_objective_game(
                   theta_j,
                   theta_2,
                   p_marginal_est, 
                   A,
                   S,
                   delta,
                   lambda
                   )
               return(objective_ij)
             }
  df_graph <- 
    data.frame(
      x = theta_i_list, 
      y = objective_i
      )
  g <- 
    ggplot(
      data = df_graph, 
      aes(
        x = x,
        y = y
        )
      ) +
    geom_point() +
    geom_vline(
      xintercept = theta_i,
      linetype = "dotted"
      ) +
    ylab("objective function") + 
    xlab(TeX(label[i])) + 
  theme_classic()
  return(g)
}
saveRDS(
  graph,
  file = "lecture/data/a8/CCP_graph.rds" %>% here::here()
)
```
```{r}
graph <- 
  readRDS(
    file = "lecture/data/a8/CCP_graph.rds" %>% here::here()
  )
graph
```

4. Estimate the parameters by minimizing the objective function. To keep the model to be well-defined, impose an ad hoc lower and upper bounds such that $\alpha \in [0, 1], \beta \in [0, 5], \delta \in [0, 1]$.

```{r, eval = TRUE}
lower <- 
  rep(
    0, 
    length(theta_1)
    )
upper <- c(1, 5, 0.3)
CCP_result <-
  optim(
        par = theta_1,
        fn = compute_CCP_objective_game,
        method = "L-BFGS-B",
        lower = lower,
        upper = upper,
        theta_2 = theta_2_est,
        p_marginal_est = p_marginal_est,
        A = A,
        S = S,
        delta = delta,
        lambda = lambda
        )
saveRDS(
  CCP_result, 
  file = "lecture/data/a8/CCP_result.rds" %>% here::here()
)
```

```{r}
CCP_result <- 
  readRDS(
    file = "lecture/data/a8/CCP_result.rds" %>% here::here()
  )

CCP_result
compare <-
  data.frame(
    true = theta_1,
    estimate = CCP_result$par
  ); 
compare
```





