[["intro.html", "Chapter 2 Introduction 2.1 What are structural models? 2.2 Example: Linear demand and supply model 2.3 Ordinary-Least-Squares (OLS) estimation of the demand and supply model 2.4 Maximum-Likelihood Estimation (MLE) of the demand and supply model 2.5 Simulation-based (Simulated-Likelihood or Simulated-Method-of-Moments) methods 2.6 Two-Stage Least Squares (2SLS) estimation of the demand and supply model 2.7 Instrumental-Variables (IV) and Generalized Method of Moments (GMM) estimation of the demand and supply model 2.8 General Case 2.9 Structural Estimation and Counterfactual Analysis", " Chapter 2 Introduction 2.1 What are structural models? An economic model is a mapping from exogenous variables (including shocks and parameters) to endogenous variables. This mapping is derived from a solution concept (optimality and equilibrium). The structural estimation approach starts from an economic model and derives an econometric model. The structural-form of an economic model is the equilibrium condition for the endogenous variables given the exogenous variables. The reduced-form of an economic model is the solution of the structural-form for the endogenous variables in terms of the exogenous variables. 2.2 Example: Linear demand and supply model Consider a linear demand and supply model: \\[\\begin{align} Q_i^d = \\alpha + \\beta P_i + \\gamma X_i + \\epsilon_i \\\\ Q_i^s = \\delta + \\theta P_i + \\zeta Z_i + \\nu_i \\end{align}\\] The parameters of the model are: \\(\\alpha, \\beta, \\gamma, \\delta, \\theta, \\zeta\\) The observed exogenous variables are: \\(X_i, Z_i\\) The unobserved exogenous variables (shocks) are: \\(\\epsilon_i, \\nu_i\\) (assume standard deviations of 1 for simplicity and mutually i.i.d.) The endogenous variables are: \\(P_i, Q_i\\) The solution concept is the market clearing condition: \\[\\begin{align} &amp;Q_i^d = Q_i^s \\\\ &amp;\\Leftrightarrow \\alpha + \\beta P_i + \\gamma X_i + \\epsilon_i = \\delta + \\theta P_i + \\zeta Z_i + \\nu_i \\end{align}\\] So far this is the structural-form of the model. The solution is obtained by solving the endogenous variables for the exogenous variables: \\[\\begin{align} P_i = \\frac{\\alpha - \\delta + \\gamma X_i - \\zeta Z_i + \\epsilon_i - \\nu_i}{\\theta - \\beta} \\\\ Q_i = \\frac{\\alpha \\theta - \\beta \\delta + \\theta \\gamma X_i - \\beta \\zeta Z_i + \\theta \\epsilon_i - \\beta \\nu_i}{\\theta - \\beta} \\end{align}\\] This is the mapping from the exogenous variables to the endogenous variables in the structural model. This is the reduced-form of the model. 2.3 Ordinary-Least-Squares (OLS) estimation of the demand and supply model OLS is applied to the reduced-form of the model. The reduced-form model is represented by the following equation: \\[\\begin{align} P_i &amp;= \\pi_1 + \\pi_2 X_i + \\pi_3 Z_i + \\sigma_1 \\varepsilon_i\\\\ Q_i &amp;= \\pi_4 + \\pi_5 X_i + \\pi_6 Z_i + \\sigma_2 \\upsilon_i \\end{align}\\] The parameters of the reduced-form model are: \\(\\pi_0, \\pi_1, \\pi_2, \\pi_3, \\pi_4, \\pi_5, \\sigma_1, \\sigma_2\\) are related to the parameters of the structural-form model as follows: \\[\\begin{align} \\pi_1 = \\frac{\\alpha - \\delta}{\\theta - \\beta} \\\\ \\pi_2 = \\frac{\\gamma}{\\theta - \\beta} \\\\ \\pi_3 = \\frac{-\\zeta}{\\theta - \\beta} \\\\ \\pi_4 = \\frac{\\alpha \\theta - \\beta \\delta}{\\theta - \\beta} \\\\ \\pi_5 = \\frac{\\theta \\gamma}{\\theta - \\beta} \\\\ \\pi_6 = \\frac{- \\beta \\zeta}{\\theta - \\beta} \\\\ \\sigma_1 = \\frac{1}{\\theta - \\beta} \\\\ \\sigma_2 = \\frac{\\sqrt{\\theta^2 + \\beta^2}}{\\theta - \\beta} \\end{align}\\] Because the right-hand side of the reduced-form model only includes exogenous variables, the OLS estimators can consistently estimate the reduced-form parameters. From the known 6 reduced-form parameters \\(\\pi_1, \\pi_2, \\pi_3, \\pi_4, \\pi_5, \\pi_6\\), we can recover 6 structural-form parameters \\(\\alpha, \\beta, \\gamma, \\delta, \\theta, \\zeta\\). This is the original idea of OLS in the context of structural estimation. You may be concerned that the right-hand side of the reduced-form model may include some endogenous variables — it’s just because you have not fully solved the model. 2.4 Maximum-Likelihood Estimation (MLE) of the demand and supply model The MLE is not very different from the above approach. We derive the reduced-form model. Then, under the distributional assumptions on the shocks, we can derive the likelihood function: \\[\\begin{align} L(\\alpha, \\beta, \\gamma, \\delta, \\theta, \\zeta) = \\prod_{i=1}^n f(P_i, Q_i | X_i, Z_i) \\end{align}\\] The distributional assumptions on the shocks could be: \\[\\begin{align} \\epsilon_i \\sim N(0, 1) \\\\ \\nu_i \\sim N(0, 1) \\end{align}\\] If the structural-form parameters could be identified by the OLS estimator, then the MLE estimator should also identify them because you have the same information with a stronger distributional assumption. 2.5 Simulation-based (Simulated-Likelihood or Simulated-Method-of-Moments) methods In the current example, the reduced-form model is analytically derived and the likelihood function is also analytically derived. However, in many cases, the reduced-form model is not analytically derived and the likelihood function is not analytically derived. Even in this case, if we can simulate the endogenous variables solving the equilibrium condition, we can use the simulation-based methods to estimate the parameters. For example, by simulating shocks \\(\\epsilon_i\\) and \\(\\nu_i\\) from the distributional assumptions, we can simulate the endogenous variables \\(P_i\\) and \\(Q_i\\). Then, we can approximate the likelihood function by the empirical distribution of the simulated data. Or, we can find parameters that minimize the distance between the observed and simulated endogenous variables. 2.6 Two-Stage Least Squares (2SLS) estimation of the demand and supply model The OLS and ML estimators relied on the reduced-form model. The 2SLS estimator is a method to estimate the structural-form parameters of the equation of interest by representing the other equations in the reduced-form. Suppose that we are interested in estimating the structural-form parameters of the demand equation. The demand equation is: \\[\\begin{align} Q_i = \\alpha + \\beta P_i + \\gamma X_i + \\epsilon_i \\end{align}\\] The 2SLS estimator first runs the reduced-form model for the other equation: \\[\\begin{align} P_i = \\pi_1 + \\pi_2 X_i + \\pi_3 Z_i + \\sigma_1 \\varepsilon_i \\end{align}\\] Then, the 2SLS estimator uses the predicted values of the endogenous variables from the reduced-form model for the supply equation to estimate the structural-form parameters of the demand equation. In this estimation, \\(Z_i\\) is used as an instrument for \\(P_i\\), which is an exogenous variable excluded from the demand equation. Instead, suppose that we are interested in estimating the structural-form parameters of the supply equation. The supply equation is: \\[\\begin{align} Q_i = \\delta + \\theta P_i + \\zeta Z_i + \\nu_i \\end{align}\\] The 2SLS estimator first runs the reduced-form model for the other equation: \\[\\begin{align} P_i = \\pi_1 + \\pi_2 X_i + \\pi_3 Z_i + \\sigma_1 \\varepsilon_i \\end{align}\\] Then, the 2SLS estimator uses the predicted values of the endogenous variables from the reduced-form model for the demand equation to estimate the structural-form parameters of the supply equation. In this estimation, \\(X_i\\) is used as an instrument for \\(P_i\\), which is an exogenous variable excluded from the supply equation. 2.7 Instrumental-Variables (IV) and Generalized Method of Moments (GMM) estimation of the demand and supply model The IV and GMM estimators rely on another representation of the economic model. It relies on the moment condition of the economic model. In the current example, the moment condition is: \\[\\begin{align} &amp; E[\\epsilon_i | X_i, Z_i] = 0\\\\ &amp; E[\\nu_i | X_i, Z_i] = 0 \\end{align}\\] The IV and GMM estimators find the parameters that make the empirical-analogue of the moment condition zero. To compute the empirical-analogue of the moment condition, this time, we solve the equilibrium condition for the shocks (not for the endogenous variables). For example, for the demand equation, we solve the following equation for \\(\\epsilon_i\\): \\[\\begin{align} \\epsilon_i &amp;= Q_i - \\alpha - \\beta P_i - \\gamma X_i\\\\ \\nu_i &amp;= Q_i - \\delta - \\theta P_i - \\zeta Z_i \\end{align}\\] Then, the right-hand side includes only observed variables and the parameters. Then, we can compute the implied shock from the data for candidate parameters as: \\[\\begin{align} \\hat{\\epsilon}_i &amp;= Q_i - \\hat{\\alpha} - \\hat{\\beta} P_i - \\hat{\\gamma} X_i\\\\ \\hat{\\nu}_i &amp;= Q_i - \\hat{\\delta} - \\hat{\\theta} P_i - \\hat{\\zeta} Z_i \\end{align}\\] Then, we can compute the empirical-analogue of the moment condition: \\[\\begin{align} \\frac{1}{n} \\sum_{i=1}^n \\hat{\\epsilon}_i A(Z_i, X_i) = 0\\\\ \\frac{1}{n} \\sum_{i=1}^n \\hat{\\nu}_i B(Z_i, X_i) = 0 \\end{align}\\] where \\(A(Z_i, X_i)\\) and \\(B(Z_i, X_i)\\) are some known functions of the observed variables and the parameters. The IV and GMM estimators find the parameters that make the empirical-analogue of the moment condition zero. 2.8 General Case In general, the structural-form model is represented by the following equation: \\[\\begin{align} f(Y_i, X_i, \\epsilon_i, \\theta) = 0 \\end{align}\\] for endogenous variables \\(Y_i\\), exogenous variables \\(X_i\\), shocks \\(\\epsilon_i\\), and the parameters \\(\\theta\\). The reduced-form model is represented by the following equation: \\[\\begin{align} Y_i = g(X_i, \\epsilon_i, \\theta) = f_1^{-1}(X_i, \\epsilon_i, \\theta) \\end{align}\\] for exogenous variables \\(X_i\\), shocks \\(\\epsilon_i\\), and the parameters \\(\\theta\\). We can apply OLS, ML, Simulated-Likelihood to this form. The moment condition is: \\[\\begin{align} E[h(Y_i, X_i, \\theta) | X_i, \\nu_i] = E[f_3^{-1}(Y_i, X_i, \\theta) | X_i, \\nu_i] = 0 \\end{align}\\] We can apply IV and GMM to this form. Thus, structural estimation is a general framework to estimate the parameters of the economic model by transforming it in either form and applying the corresponding estimation method. 2.9 Structural Estimation and Counterfactual Analysis 2.9.1 Example Igami (2017) “Estimating the Innovator’s Dilemma: Structural Analysis of Creative Destruction in the Hard Disk Drive Industry, 1981-1998” Question: Does the “Innovator’s Dilemma” (Christensen, 1997) or the delay of innovation among incumbents exist? Christensen argued that old winners tend to lag behind entrants even when introducing a new technology is not too difficult, with a case study of the HDD industry. Apple’s smartphones vs. Nokia’s feature phones Amazon vs. Borders Kodak’s digital cameras If it exists, what is the reason for that? How do we empirically answer this question? Figure 2.1: Figure 1 of Igam (2017) Hypotheses: Identify potentially competing hypotheses to explain the phenomenon. Cannibalization: Because of cannibalization, the benefits of introducing a new product are smaller for incumbents than for entrants. Different costs: The incumbents may have higher costs for innovation due to organizational inertia, but at the same time they may have some cost advantage due to accumulated R&amp;D and better financial access. Preemption: The incumbents have additional incentive for innovation to preempt potential rivals. Institutional environment: The impacts of the three components differ across different institutional contexts such as the rules governing patents and market size. Casual empiricists pick up their favorite factors to make up a story. Serious empiricists should try to separate the contributions of each factor from data. To do so, the author develops an economic model that explicitly incorporates the above-mentioned factors, while keeping the model parameters flexible enough to let the data tell the sign and size of the effects of each factor on innovation. Economic model: The time is discrete with finite horizon \\(t = 1, \\cdots, T\\). In each year, there is a finite number of firms indexed by \\(i\\). Each firm is in one of the technological states: \\[\\begin{equation} s_{it} \\in \\{\\text{old only, both, new only, potential entrant}\\}, \\end{equation}\\] where the first two states are for incumbents (stick to the old technology or start using the new technology) and the last two states are for actual and potential entrants (enter with the new technology or stay outside the market). In each year: Pre-innovation incumbent (\\(s_{it} =\\) old): exit or innovate by paying a sunk cost \\(\\kappa^{inc}\\) (to be \\(s_{i, t + 1} =\\) both). Post-innovation incumbent (\\(s_{it} =\\) both): exit or stay to be both. Potential entrant (\\(s_{it} =\\) potential entrant): give up entry or enter with the new technology by paying a sunk cost \\(\\kappa^{net}\\) (to be \\(s_{i, t + 1} =\\) new). Actual entrant (\\(s_{it} =\\) new): exit or stay to be new. Given the industry state \\(s_t = \\{s_{it}\\}_i\\), the product market competition opens and the profit of firm \\(i\\), \\(\\pi_t(s_{it}, s_{-it})\\), is realized for each active firm. As the product market competition closes: Pre-innovation incumbents draw private cost shocks and make decisions: \\(a_t^{pre}\\). Observing this, post-innovation incumbents draw private cost shocks and make decisions: \\(a_t^{post}\\). Observing this, actual entrants draw private cost shocks and make decisions: \\(a_t^{act}\\). Observing this, potential entrants draw private cost shocks and make decisions: \\(a_t^{pot}\\). This is a dynamic game. The equilibrium is defined by the concept of Markov-perfect equilibrium (Maskin &amp; Tirole, 1988). The representation of the competing theories in the model: The existence of cannibalization is represented by the assumption that an incumbent maximizes the joint profits of old and new technology products. The size of cannibalization is captured by the shape of profit function. The difference in the cost of innovation is captured by the difference in the sunk costs of innovation. The preemptive incentive for incumbents is embodied in the dynamic optimization problem for each incumbent. Econometric model: The author then turns the economic model into an econometric model. This amounts to specifying which part of the economic model is observed/known and which part is unobserved/unknown. The author collects the data set of the HDD industry during 1977-99. Based on the data, the author specifies the identities of active firms and their products and the technologies embodied in the products in each year to code their state variables. Moreover, by tracking the change in the state, the author codes their action variables. Thus, the state and action variables, \\(s_t\\) and \\(a_t\\). These are the observables. The author does not observe: Profit function \\(\\pi_t(\\cdot)\\). Sunk cost of innovation for pre-innovation incumbents \\(\\kappa^{inc}\\). Sunk cost of entry for potential entrants \\(\\kappa^{net}\\). Private cost shocks. These are the unobservables. Among the unobservables, the profit function and sunk costs are the parameters of interest and the private cost shocks are nuisance parameters in the sense only the knowledge about the distribution of the latter is demanded. Identification: Can we infer the unobservables from the observables and the restrictions on the distribution of observables by the economic theory? The profit function is identified from estimating the demand function for each firm’s product, and estimating the cost function for each firm from using their price setting behavior. The sunk costs of innovation are identified from the conditional probability of innovation across various states. If the cost is low, the probability should be high. Estimation: The identification established that in principle we can uncover the parameters of interest from observables under the restrictions of economic theory. Finally, we apply a statistical method to the econometric model and infer the parameters of interest. Counterfactual analysis: If we can uncover the parameters of interest, we can conduct comparative statics: study the change in the endogenous variables when the exogenous variables including the model parameters are set differently. In the current framework, this exercise is often called the counterfactual analysis. What if there was no cannibalization?: An incumbent separately maximizes the profit from old technology and new technology instead of jointly maximizing the profits. Solve the model under this new assumption everything else being equal. Free of cannibalization concerns, 8.95 incumbents start producing new HDDs in the first 10 years, compared with 6.30 in the baseline. The cumulative numbers of innovators among incumbents and entrants differ only by 2.8 compared with 6.45 in the baseline. Thus cannibalization can explain a significant part of the incumbent-entrant innovation gap. What if there was no preemption?: A potential entrant ignores the incumbents’ innovations upon making entry decisions. Without the preemptive motives, only 6.02 incumbents would innovate in the first 10 years, compared with 6.30 in the baseline. The cumulative incumbent-entrant innovation gap widens to 8.91 compared with 6.45 in the baseline. The sunk cost of entry is smaller for incumbents than for entrants in the baseline. Interpretations and policy/managerial implications: Despite the cost advantage and the preemptive motives, the speed of innovation is slower among incumbents due to the strong cannibalization effect. Incumbents that attempt to avoid the “innovator’s dilemma” should separate the decision making between old and new sections inside the organization so that they can avoid the concern for cannibalization. 2.9.2 Recap The structural approach in empirical industrial organization consists of the following components: Research question. Competing hypotheses. Economic model. Econometric model. Identification. Data collection. Data cleaning. Estimation. Counterfactual analysis. Coding. Interpretations and policy/managerial implications. The goal of this course is to be familiar with the standard methodology to complete this process. The methodology covered in this class is mostly developed to analyze the standard framework of dynamic or oligopoly competition. The policy implications are centered around competition policies. But the basic idea can be extended to different classes of situations such as auction, matching, voting, contract, marketing, and so on. Note that the depth of the research question and the relevance of the policy/managerial implications are the most important parts of the research. Focusing on the methodology in this class is to minimize the time allocated to less important issues and maximize the attention and time to the most valuable part in future research. Given a research question, what kind of data is necessary to answer the question? Given data, what kind of research questions can you address? Which questions can be credibly answered? Which questions can be an over-stretch? Given a research question and data, what is the best way to answer the question? What type of problems can you avoid using the method? What is the limitation of your approach? How will you defend against possible referee comments? Given a result, what kinds of interpretations can you credibly derive? What kinds of interpretations can be contested by potential opponents? What kinds of contributions can you claim? To address these issues is necessary to publish a paper and it is necessary to be familiar with the methodology to do so. References Christensen, C. M. (1997). The innovator’s dilemma : When new technologies cause great firms to fail. New York: HarperBusiness. Igami, M. (2017). Estimating the Innovator’s Dilemma: Structural Analysis of Creative Destruction in the Hard Disk Drive Industry, 1981. Journal of Political Economy, 125(3), 798–847. Maskin, E., &amp; Tirole, J. (1988). A Theory of Dynamic Oligopoly, I: Overview and Quantity Competition with Large Fixed Costs (No. 3) (Vol. 56, p. 549). "],["assignment1.html", "Chapter 11 Assignment 1: Basic Programming in R 11.1 Simulate data 11.2 Estimate the parameter", " Chapter 11 Assignment 1: Basic Programming in R Report the following results in html format using R markdown. In other words, replicate this document. You write functions in a separate R file and put in R folder in the project folder. Build the project as a package and load it from the R markdown file. The execution code sholuld be written in R markdown file. You submit: R file containing functions. R markdown file containing your answers and executing codes. HTML(or PDF) report generated from the R markdown. 11.1 Simulate data Consider to simulate data from the following model and estimate the parameters from the simulated data. \\[ y_{ij} = 1\\{j = \\text{argmax}_{k = 1, 2} \\beta x_k + \\epsilon_{ik} \\}, \\] where \\(\\epsilon_{ik}\\) follows i.i.d. type-I extreme value distribution, \\(\\beta = 0.2\\), \\(x_1 = 0\\) and \\(x_2 = 1\\). To simulate data, first make a data frame as follows: df ## # A tibble: 2,000 × 3 ## i k x ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 1 2 1 ## 3 2 1 0 ## 4 2 2 1 ## 5 3 1 0 ## 6 3 2 1 ## 7 4 1 0 ## 8 4 2 1 ## 9 5 1 0 ## 10 5 2 1 ## # ℹ 1,990 more rows Second, draw type-I extreme value random variables. Set the seed at 1. You can use evd package to draw the variables. You should get exactly the same realization if the seed is correctly set. set.seed(1) df ## # A tibble: 2,000 × 4 ## i k x e ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.281 ## 2 1 2 1 -0.167 ## 3 2 1 0 1.93 ## 4 2 2 1 1.97 ## 5 3 1 0 0.830 ## 6 3 2 1 -1.06 ## 7 4 1 0 -0.207 ## 8 4 2 1 0.617 ## 9 5 1 0 0.0444 ## 10 5 2 1 1.92 ## # ℹ 1,990 more rows Third, compute the latent value of each option to obtain the following data frame: beta &lt;- 0.2 df ## # A tibble: 2,000 × 5 ## i k x e latent ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.281 0.281 ## 2 1 2 1 -0.167 0.0331 ## 3 2 1 0 1.93 1.93 ## 4 2 2 1 1.97 2.17 ## 5 3 1 0 0.830 0.830 ## 6 3 2 1 -1.06 -0.863 ## 7 4 1 0 -0.207 -0.207 ## 8 4 2 1 0.617 0.817 ## 9 5 1 0 0.0444 0.0444 ## 10 5 2 1 1.92 2.12 ## # ℹ 1,990 more rows Finally, compute \\(y\\) by comparing the latent values of \\(k = 1, 2\\) for each \\(i\\) to obtain the following result: df ## # A tibble: 2,000 × 6 ## i k x e latent y ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.281 0.281 1 ## 2 1 2 1 -0.167 0.0331 0 ## 3 2 1 0 1.93 1.93 0 ## 4 2 2 1 1.97 2.17 1 ## 5 3 1 0 0.830 0.830 1 ## 6 3 2 1 -1.06 -0.863 0 ## 7 4 1 0 -0.207 -0.207 0 ## 8 4 2 1 0.617 0.817 1 ## 9 5 1 0 0.0444 0.0444 0 ## 10 5 2 1 1.92 2.12 1 ## # ℹ 1,990 more rows 11.2 Estimate the parameter Now you generated simulated data. Suppose you observe \\(x_k\\) and \\(y_{ik}\\) for each \\(i\\) and \\(k\\) and estimate \\(\\beta\\) by a maximum likelihood estimator. The likelihood for \\(i\\) to choose \\(k\\) (\\(y_{ik} = 1\\)) can be shown to be: \\[ p_{ik}(\\beta) = \\frac{\\exp(\\beta x_k)}{\\exp(\\beta x_1) + \\exp(\\beta x_2)}. \\] Then, the likelihood of observing \\(\\{y_{ik}\\}_{i, k}\\) is: \\[ L(\\beta) = \\prod_{i = 1}^{1000} p_{i1}(\\beta)^{y_{i1}} [1 - p_{i1}(\\beta)]^{1 - y_{i1}}, \\] and the log likelihood is: \\[ l(\\beta) = \\sum_{i = 1}^{1000}\\{y_{i1}\\log p_{i1}(\\beta) + (1 - y_{i1})\\log [1 - p_{i1}(\\beta)\\}. \\] Write a function to compute the likelihood for a given \\(\\beta\\) and data and name the function compute_loglikelihood_a1(b, df). compute_loglikelihood_a1( b = 1, df = df ) ## [1] -0.7542617 Compute the value of log likelihood for \\(\\beta = 0, 0.1, \\cdots, 1\\) and plot the result using ggplot2 packages. You can use latex2exp package to use LaTeX math symbol in the label: b_seq &lt;- seq(0, 1, 0.1) output &lt;- foreach ( b = b_seq, .combine = &quot;rbind&quot; ) %do% { l &lt;- compute_loglikelihood_a1( b, df ) return(l) } output &lt;- data.frame( x = b_seq, y = output ) output %&gt;% ggplot( aes( x = x, y = y ) ) + geom_point() + xlab(TeX(&quot;$\\\\beta$&quot;)) + ylab(&quot;Loglikelihood&quot;) + theme_classic() Find and report \\(\\beta\\) that maximizes the log likelihood for the simulated data. You can use optim function to achieve this. You will use Brent method and set the lower bound at -1 and upper bound at 1 for the parameter search. ## $par ## [1] 0.2371046 ## ## $value ## [1] -0.6861689 ## ## $counts ## function gradient ## NA NA ## ## $convergence ## [1] 0 ## ## $message ## NULL "],["assignment2.html", "Chapter 12 Assignment 2: Production Function Estimation 12.1 Simulate data 12.2 Estimate the parameters", " Chapter 12 Assignment 2: Production Function Estimation 12.1 Simulate data Consider the following production and investment process for \\(j = 1, \\cdots, 1000\\) firms across \\(t = 1, \\cdots, 10\\) periods. The log production function is of the form: \\[ y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\omega_{jt} + \\eta_{jt}, \\] where \\(\\omega_{jt}\\) is an anticipated shock and \\(\\eta_{jt}\\) is an ex post shock. The anticipated shocks evolve as: \\[ \\omega_{jt} = \\alpha \\omega_{j, t - 1} + \\nu_{jt}, \\] where \\(\\nu_{jt}\\) is an i.i.d. normal random variable with mean 0 and standard deviation \\(\\sigma_\\nu\\). The ex post shock is an i.i.d. normal random variable with mean 0 and standard deviation \\(\\sigma_{\\eta}\\). The product price the same across firms and normalized at 1. The price is normalized at 1. The wage \\(w_t\\) is constant at 0.5. Finally, the capital accumulate according to: \\[ K_{j, t + 1} = (1 - \\delta) K_{jt} + I_{jt}. \\] We set the parameters as follows: parameter variable value \\(\\beta_0\\) beta_0 1 \\(\\beta_l\\) beta_l 0.2 \\(\\beta_k\\) beta_k 0.7 \\(\\alpha\\) alpha 0.7 \\(\\sigma_{\\eta}\\) sigma_eta 0.2 \\(\\sigma_{\\nu}\\) sigma_nu 0.5 \\(\\sigma_{w}\\) sigma_w 0.1 \\(\\delta\\) delta 0.05 Define the parameter variables as above. Write a function that returns the log output given \\(l_{jt}\\), \\(k_{jt}\\), \\(\\omega_{jt}\\), and \\(\\eta_{jt}\\) under the given parameter values according to the above production function and name it log_production(l, k, omega, eta, beta_0, beta_l, beta_k). log_production( l = 1, k = 1, omega = 1, eta = 1, beta_0 = beta_0, beta_l = beta_l, beta_k = beta_k ) ## [1] 3.9 Suppose that the labor is determined after \\(\\omega_{jt}\\) is observed, but before \\(\\eta_{jt}\\) is observed, given the log capital level \\(k_{jt}\\). Derive the optimal log labor as a function of \\(\\omega_{jt}\\), \\(\\eta_{jt}\\), \\(k_{jt}\\), and wage. Write a function to return the optimal log labor given the variables and parameters and name it log_labor_choice(k, wage, omega, beta_0, beta_l, beta_k, sigma_eta). log_labor_choice( k = 1, wage = 1, omega = 1, beta_0 = beta_0, beta_l = beta_l, beta_k = beta_k, sigma_eta = sigma_eta ) ## [1] 1.388203 As discussed in the class, if there is no additional variation in labor, the coefficient on the labor \\(\\beta_l\\) is not identified. Thus, if we generate labor choice from the previous function, \\(\\beta_l\\) will not be identified from the simulated data. To see this, we write a modified version of the previous function in which \\(\\omega_{jt}\\) is replaced with \\(\\omega_{jt} + \\iota_{jt}\\), where \\(\\iota_{jt}\\) is an optimization error that follows an i.i.d. normal distribution with mean 0 and standard deviation 0.05. That is, the manager of the firm perceives as if the shock is \\(\\omega_{jt} + \\iota_{jt}\\), even though the true shock is \\(\\omega_{jt}\\). Modify the previous function by including \\(\\iota_{jt}\\) as an additional input and name it log_labor_choice_error(k, wage, omega, beta_0, beta_l, beta_k, iota, sigma_eta). log_labor_choice_error( k = 1, wage = 1, omega = 1, beta_0 = beta_0, beta_l = beta_l, beta_k = beta_k, iota = 1, sigma_eta = sigma_eta ) ## [1] 2.638203 Consider an investment process such that: \\[ I_{jt} = (\\delta + \\gamma \\omega_{jt}) K_{jt}, \\] where \\(I_{jt}\\) and \\(K_{jt}\\) are investment and capital in level. Set \\(\\gamma = 0.1\\), i.e., the investment is strictly increasing in \\(\\omega_{jt}\\). The investment function should be derived by solving the dynamic problem of a firm. But here, we just specify it in a reduced-form. Define variable \\(\\gamma\\) and assign it the value. Write a function that returns the investment given \\(K_{jt}\\), \\(\\omega_{jt}\\), and parameter values, according to the previous equation, and name it investment_choice(k, omega, gamma, delta). gamma &lt;- 0.1 investment_choice( k = 1, omega = 1, gamma = gamma, delta = delta ) ## [1] 0.4077423 Simulate the data first using the labor choice without optimization error and second using the labor choice with optimization error. To do so, we specify the initial values for the state variables \\(k_{jt}\\) and \\(\\omega_{jt}\\) as follows. Draw \\(k_{j1}\\) from an i.i.d. normal distribution with mean 1 and standard deviation 0.5. Draw \\(\\omega_{j1}\\) from its stationary distribution (check the stationary distribution of AR(1) process). Draw a wage. Before simulating the rest of the data, set the seed at 1. df ## # A tibble: 1,000 × 5 ## j t k omega wage ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 ## 2 2 1 1.09 0.779 0.5 ## 3 3 1 0.582 -0.610 0.5 ## 4 4 1 1.80 0.148 0.5 ## 5 5 1 1.16 0.0486 0.5 ## 6 6 1 0.590 -1.16 0.5 ## 7 7 1 1.24 0.568 0.5 ## 8 8 1 1.37 -1.34 0.5 ## 9 9 1 1.29 -0.873 0.5 ## 10 10 1 0.847 0.699 0.5 ## # ℹ 990 more rows Draw optimization error \\(\\iota_{jt}\\) and compute the labor and investment choice of period 1. For labor choice, compute both types of labor choices. ## # A tibble: 1,000 × 9 ## j t k omega wage iota l l_error inv ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 -0.0443 1.72 1.67 0.257 ## 2 2 1 1.09 0.779 0.5 -0.0961 2.06 1.94 0.381 ## 3 3 1 0.582 -0.610 0.5 0.0810 -0.123 -0.0218 -0.0196 ## 4 4 1 1.80 0.148 0.5 0.0260 1.89 1.92 0.391 ## 5 5 1 1.16 0.0486 0.5 -0.00279 1.21 1.21 0.176 ## 6 6 1 0.590 -1.16 0.5 0.0348 -0.809 -0.766 -0.120 ## 7 7 1 1.24 0.568 0.5 0.00268 1.93 1.93 0.370 ## 8 8 1 1.37 -1.34 0.5 -0.0655 -0.346 -0.428 -0.330 ## 9 9 1 1.29 -0.873 0.5 -0.106 0.165 0.0327 -0.135 ## 10 10 1 0.847 0.699 0.5 -0.0104 1.74 1.73 0.280 ## # ℹ 990 more rows Draw ex post shock and compute the output according to the production function for both labor without optimization error and with optimization error. Name the output without optimization error y and the one with optimization error y_error. ## # A tibble: 1,000 × 12 ## j t k omega wage iota l l_error inv eta y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 -0.0443 1.72 1.67 0.257 0.148 2.77 ## 2 2 1 1.09 0.779 0.5 -0.0961 2.06 1.94 0.381 0.0773 3.03 ## 3 3 1 0.582 -0.610 0.5 0.0810 -0.123 -0.0218 -0.0196 0.259 1.03 ## 4 4 1 1.80 0.148 0.5 0.0260 1.89 1.92 0.391 -0.161 2.62 ## 5 5 1 1.16 0.0486 0.5 -0.00279 1.21 1.21 0.176 -0.321 1.79 ## 6 6 1 0.590 -1.16 0.5 0.0348 -0.809 -0.766 -0.120 0.187 0.274 ## 7 7 1 1.24 0.568 0.5 0.00268 1.93 1.93 0.370 0.361 3.19 ## 8 8 1 1.37 -1.34 0.5 -0.0655 -0.346 -0.428 -0.330 -0.0113 0.539 ## 9 9 1 1.29 -0.873 0.5 -0.106 0.165 0.0327 -0.135 0.377 1.44 ## 10 10 1 0.847 0.699 0.5 -0.0104 1.74 1.73 0.280 0.316 2.96 ## # ℹ 990 more rows ## # ℹ 1 more variable: y_error &lt;dbl&gt; Repeat this procedure for \\(t = 1, \\cdots 10\\) by updating the capital and anticipated shocks, and name the resulting data frame df_all. Use the previously generated df as the data for t = 1. In each perio, generate nu, iota, and eta in this order, if you want to match the realization of the random variables. df_all ## # A tibble: 10,000 × 13 ## j t k omega wage iota l l_error inv eta y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 -0.0443 1.72 1.67 0.257 0.148 2.77 ## 2 2 1 1.09 0.779 0.5 -0.0961 2.06 1.94 0.381 0.0773 3.03 ## 3 3 1 0.582 -0.610 0.5 0.0810 -0.123 -0.0218 -0.0196 0.259 1.03 ## 4 4 1 1.80 0.148 0.5 0.0260 1.89 1.92 0.391 -0.161 2.62 ## 5 5 1 1.16 0.0486 0.5 -0.00279 1.21 1.21 0.176 -0.321 1.79 ## 6 6 1 0.590 -1.16 0.5 0.0348 -0.809 -0.766 -0.120 0.187 0.274 ## 7 7 1 1.24 0.568 0.5 0.00268 1.93 1.93 0.370 0.361 3.19 ## 8 8 1 1.37 -1.34 0.5 -0.0655 -0.346 -0.428 -0.330 -0.0113 0.539 ## 9 9 1 1.29 -0.873 0.5 -0.106 0.165 0.0327 -0.135 0.377 1.44 ## 10 10 1 0.847 0.699 0.5 -0.0104 1.74 1.73 0.280 0.316 2.96 ## # ℹ 9,990 more rows ## # ℹ 2 more variables: y_error &lt;dbl&gt;, nu &lt;dbl&gt; Check the simulated data by making summary table. N Mean Sd Min Max j 10000 500.5000000 288.6894251 1.0000000 1000.0000000 t 10000 5.5000000 2.8724249 1.0000000 10.0000000 k 10000 0.9797900 0.5838949 -1.2822534 3.2332312 omega 10000 -0.0055826 0.7025102 -2.5894171 2.6281307 wage 10000 0.5000000 0.0000000 0.5000000 0.5000000 iota 10000 -0.0000696 0.0502883 -0.1841453 0.1715419 l 10000 0.9799746 1.0965108 -3.3281023 4.9679634 l_error 10000 0.9798876 1.0971595 -3.3765433 4.9520674 inv 10000 0.1793502 0.3006526 -1.2722627 3.2975332 eta 10000 0.0015825 0.2001539 -0.7650371 0.7455922 y 10000 1.8778479 1.1171035 -2.4680251 6.1228291 y_error 10000 1.8778305 1.1169266 -2.4777133 6.1196499 nu 10000 -0.0021155 0.4984324 -2.1513907 1.8253882 12.2 Estimate the parameters For now, use the labor choice with optimization error. First, simply regress \\(y_{jt}\\) on \\(l_{jt}\\) and \\(k_{jt}\\) using the least square method. ## ## Call: ## lm(formula = y_error ~ l_error + k, data = df_all) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73002 -0.14117 -0.00071 0.13743 0.87983 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.892542 0.004058 219.966 &lt;2e-16 *** ## l_error 0.997913 0.002396 416.454 &lt;2e-16 *** ## k 0.007599 0.004503 1.688 0.0915 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2068 on 9997 degrees of freedom ## Multiple R-squared: 0.9657, Adjusted R-squared: 0.9657 ## F-statistic: 1.408e+05 on 2 and 9997 DF, p-value: &lt; 2.2e-16 Second, take within-transformation on \\(y_{jt}\\), \\(l_{jt}\\), and \\(k_{jt}\\) and let \\(\\Delta y_{jt}\\), \\(\\Delta l_{jt}\\), and \\(\\Delta k_{jt}\\) denote them. Then, regress \\(\\Delta y_{jt}\\) on \\(\\Delta l_{jt}\\), and \\(\\Delta k_{jt}\\) by the least squares method. ## ## Call: ## lm(formula = dy_error ~ -1 + dl_error + dk, data = df_all_within) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.72450 -0.13285 -0.00244 0.12931 0.77657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## dl_error 0.9910916 0.0029548 335.413 &lt;2e-16 *** ## dk -0.0009029 0.0127539 -0.071 0.944 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1961 on 9998 degrees of freedom ## Multiple R-squared: 0.9184, Adjusted R-squared: 0.9184 ## F-statistic: 5.629e+04 on 2 and 9998 DF, p-value: &lt; 2.2e-16 Next, we attempt to estimate the parameters using Olley-Pakes method. Estimate the first-step model of Olley-Pakes method: \\[ y_{jt} = \\beta_1 l_{jt} + \\phi(k_{jt}, I_{jt}) + \\eta_{jt}, \\] by approximating \\(\\phi_t\\) by a kernel function. Remark that \\(\\phi\\) in general depends on observed and unobserved state variables. For this reason, in theory, \\(\\phi\\) should be estimated for each period. In this exercise, we assume \\(\\phi\\) is common across periods because we know that there is no unobserved state variables in the true data generating process. Moreover, we do not include \\(w_t\\) because we know that it is time -invariant. Do not forget to consider them in the actual data analysis. You can use npplreg function of np package to estimate a partially linear model with a multivariate kernel. You first use npplregbw to obtain the optimal band width and then use npplreg to estimate the model under the optimal bandwidth. The computation of the optimal bandwidth is time consuming. Return the summary of the first stage estimation and plot the fitted values against the data points. ## ## Partially Linear Model ## Regression data: 10000 training points, in 3 variable(s) ## With 1 linear parametric regressor(s), 2 nonparametric regressor(s) ## ## y(z) ## Bandwidth(s): 0.07355058 0.01435558 ## ## x(z) ## Bandwidth(s): 0.03908594 0.01191551 ## ## l_error ## Coefficient(s): 0.2493396 ## ## Kernel Regression Estimator: Local-Constant ## Bandwidth Type: Fixed ## ## Residual standard error: 0.1934778 ## R-squared: 0.9700706 ## ## Continuous Kernel Type: Second-Order Gaussian ## No. Continuous Explanatory Vars.: 2 Check that \\(\\beta_l\\) is not identified with the data without optimization error. Estimate the first stage model of Olley-Pakes with the labor choice without optimization error and report the result. ## ## Partially Linear Model ## Regression data: 10000 training points, in 3 variable(s) ## With 1 linear parametric regressor(s), 2 nonparametric regressor(s) ## ## y(z) ## Bandwidth(s): 0.07347226 0.01437256 ## ## x(z) ## Bandwidth(s): 0.02960262 0.009987366 ## ## l ## Coefficient(s): 1.187301 ## ## Kernel Regression Estimator: Local-Constant ## Bandwidth Type: Fixed ## ## Residual standard error: 0.1932366 ## R-squared: 0.9701184 ## ## Continuous Kernel Type: Second-Order Gaussian ## No. Continuous Explanatory Vars.: 2 Then, we estimate the second stage model of Olley-Pakes method: \\[ y_{jt} - \\hat{\\beta_l} l_{jt} = \\beta_0 + \\beta_k k_{jt} + \\alpha[\\hat{\\phi}(k_{j, t - 1}, I_{j, t - 1}) - \\beta_0 - \\beta_k k_{j, t-1}] + \\nu_{jt} + \\eta_{jt}. \\] In this model, we do not have to non-parametetrically estimate the conditional expectation of \\(\\omega_{jt}\\) on \\(\\omega_{j, t - 1}\\), because we know that the anticipated shock follows an AR(1) process. Remark that we in general have to non-parametrically estimate it. The model is non-linear in parameters, because of the term \\(\\alpha \\beta_0\\) and \\(\\alpha \\beta_k\\). We estimate \\(\\alpha\\), \\(\\beta_0\\), and \\(\\beta_k\\) by a GMM estimator. The moment is: \\[ g_{JT}(\\alpha, \\beta_0, \\beta_k) \\equiv \\frac{1}{JT}\\sum_{j = 1}^J \\sum_{t = 1}^T \\{y_{jt} - \\hat{\\beta_l} l_{jt} - \\beta_0 - \\beta_k k_{jt} - \\alpha[\\hat{\\phi}(k_{j, t - 1}, I_{j, t - 1}) - \\beta_0 - \\beta_k k_{j, t-1}]\\} \\begin{bmatrix} k_{jt} \\\\ k_{j, t - 1} \\\\ I_{j, t - 1} \\end{bmatrix}. \\] Using the estimates in the first step, compute: \\[ y_{jt} - \\hat{\\beta_l} l_{jt}, \\] and: \\[ \\hat{\\phi}(k_{j, t - 1}, I_{j, t - 1}), \\] for each \\(j\\) and \\(t\\) and save it as a data frame names df_all_1st. df_all_1st ## # A tibble: 10,000 × 4 ## j t y_error_tilde phi_t_1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2.34 NA ## 2 1 2 1.37 2.21 ## 3 1 3 0.621 1.49 ## 4 1 4 0.447 0.880 ## 5 1 5 0.878 0.611 ## 6 1 6 1.62 0.926 ## 7 1 7 0.558 1.40 ## 8 1 8 0.684 0.439 ## 9 1 9 0.939 0.525 ## 10 1 10 1.49 0.838 ## # ℹ 9,990 more rows Compute a function that returns the value of \\(g_{JT}(\\alpha, \\beta_0, \\beta_k)\\) given parameter values, data, and df_all_1st, and name it moment_olleypakes_2nd. Show the values of the moments evaluated at the true parameters. moment_olleypakes_2nd( alpha = alpha, beta_0 = beta_0, beta_k = beta_k, df_all = df_all, df_all_1st = df_all_1st ) ## [1] -0.018459481 -0.019010969 -0.003815687 Based on the moment, we can define the objective function of a generalized method of moments estimator with a weighting matrix \\(W\\) as: \\[ Q_{JT}(\\alpha, \\beta_0, \\beta_k) \\equiv g_{JT}(\\alpha, \\beta_0, \\beta_k)&#39; W g_{JT}(\\alpha, \\beta_0, \\beta_k). \\] Write a function that returns the value of \\(Q_{JT}(\\alpha, \\beta_0, \\beta_k)\\) given the vector of parameter values, data, and df_all_1st, and name it objective_olleypakes_2nd. Setting \\(W\\) at the identity matrix, show the value of the objective function evaluated at the true parameters. W &lt;- diag(3) theta &lt;- c( alpha, beta_0, beta_k ) objective_olleypakes_2nd( theta = theta, df_all = df_all, df_all_1st = df_all_1st, W = W ) ## [,1] ## [1,] 0.0007167288 Draw the graph of the objective function when one of \\(\\alpha\\), \\(\\beta_0\\), and \\(\\beta_k\\) are changed from 0 to 1 by 0.1 while the others are set at the true value. Is the objective function minimized at around the true value? Find the parameters that minimize the objective function using optim. You may use L-BFGS-B method to solve it. ## $par ## [1] 0.7023647 0.9766275 0.6694232 ## ## $value ## [1] 2.027934e-07 ## ## $counts ## function gradient ## 10 10 ## ## $convergence ## [1] 0 ## ## $message ## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; "],["assignment3.html", "Chapter 13 Assignment 3: Demand Function Estimation I 13.1 Simulate data 13.2 Estimate the parameters", " Chapter 13 Assignment 3: Demand Function Estimation I 13.1 Simulate data We simulate data from a discrete choice model. There are \\(T\\) markets and each market has \\(N\\) consumers. There are \\(J\\) products and the indirect utility of consumer \\(i\\) in market \\(t\\) for product \\(j\\) is: \\[ u_{itj} = \\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt} + \\epsilon_{ijt}, \\] where \\(\\epsilon_{ijt}\\) is an i.i.d. type-I extreme random variable. \\(x_j\\) is \\(K\\)-dimensional observed characteristics of the product. \\(p_{jt}\\) is the retail price of the product in the market. \\(\\xi_{jt}\\) is product-market specific fixed effect. \\(p_{jt}\\) can be correlated with \\(\\xi_{jt}\\) but \\(x_{jt}\\)s are independent of \\(\\xi_{jt}\\). \\(j = 0\\) is an outside option whose indirect utility is: \\[ u_{it0} = \\epsilon_{i0t}, \\] where \\(\\epsilon_{i0t}\\) is an i.i.d. type-I extreme random variable. \\(\\beta_{it}\\) and \\(\\alpha_{it}\\) are different across consumers, and they are distributed as: \\[ \\beta_{itk} = \\beta_{0k} + \\sigma_k \\nu_{itk}, \\] \\[ \\alpha_{it} = - \\exp(\\mu + \\omega \\upsilon_{it}) = - \\exp(\\mu + \\frac{\\omega^2}{2}) + [- \\exp(\\mu + \\omega \\upsilon_{it}) + \\exp(\\mu + \\frac{\\omega^2}{2})] \\equiv \\alpha_0 + \\tilde{\\alpha}_{it}, \\] where \\(\\nu_{itk}\\) for \\(k = 1, \\cdots, K\\) and \\(\\upsilon_{it}\\) are i.i.d. standard normal random variables. \\(\\alpha_0\\) is the mean of \\(\\alpha_i\\) and \\(\\tilde{\\alpha}_i\\) is the deviation from the mean. Given a choice set in the market, \\(\\mathcal{J}_t \\cup \\{0\\}\\), a consumer chooses the alternative that maximizes her utility: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] The choice probability of product \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[ \\sigma_{jt}(p_t, x_t, \\xi_t) = \\mathbb{P}\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] Suppose that we only observe the share data: \\[ s_{jt} = \\frac{1}{N} \\sum_{i = 1}^N q_{ijt}, \\] along with the product-market characteristics \\(x_{jt}\\) and the retail prices \\(p_{jt}\\) for \\(j \\in \\mathcal{J}_t \\cup \\{0\\}\\) for \\(t = 1, \\cdots, T\\). We do not observe the choice data \\(q_{ijt}\\) nor shocks \\(\\xi_{jt}, \\nu_{it}, \\upsilon_{it}, \\epsilon_{ijt}\\). In this assignment, we consider a model with \\(\\xi_{jt} = 0\\), i.e., the model without the unobserved fixed effects. However, the code to simulate data should be written for general \\(\\xi_{jt}\\), so that we can use the same code in the next assignment in which we consider a model with the unobserved fixed effects. Set the seed, constants, and parameters of interest as follows. # set the seed set.seed(1) # number of products J &lt;- 10 # dimension of product characteristics including the intercept K &lt;- 3 # number of markets T &lt;- 100 # number of consumers per market N &lt;- 500 # number of Monte Carlo L &lt;- 500 # set parameters of interests beta &lt;- rnorm(K); beta[1] &lt;- 4 beta ## [1] 4.0000000 0.1836433 -0.8356286 sigma &lt;- abs(rnorm(K)); sigma ## [1] 1.5952808 0.3295078 0.8204684 mu &lt;- 0.5 omega &lt;- 1 Generate the covariates as follows. The product-market characteristics: \\[ x_{j1} = 1, x_{jk} \\sim N(0, \\sigma_x), k = 2, \\cdots, K, \\] where \\(\\sigma_x\\) is referred to as sd_x in the code. The product-market-specific unobserved fixed effect: \\[ \\xi_{jt} = 0. \\] The marginal cost of product \\(j\\) in market \\(t\\): \\[ c_{jt} \\sim \\text{logNormal}(0, \\sigma_c), \\] where \\(\\sigma_c\\) is referred to as sd_c in the code. The retail price: \\[ p_{jt} - c_{jt} \\sim \\text{logNorm}(\\gamma \\xi_{jt}, \\sigma_p), \\] where \\(\\gamma\\) is referred to as price_xi and \\(\\sigma_p\\) as sd_p in the code. This price is not the equilibrium price. We will revisit this point in a subsequent assignment. The value of the auxiliary parameters are set as follows: # set auxiliary parameters price_xi &lt;- 1 prop_jt &lt;- 0.6 sd_x &lt;- 0.5 sd_c &lt;- 0.05 sd_p &lt;- 0.05 X is the data frame such that a row contains the characteristics vector \\(x_{j}\\) of a product and columns are product index and observed product characteristics. The dimension of the characteristics \\(K\\) is specified above. Add the row of the outside option whose index is \\(0\\) and all the characteristics are zero. # make product characteristics data X &lt;- matrix( sd_x * rnorm(J * (K - 1)), nrow = J ) X &lt;- cbind( rep(1, J), X ) colnames(X) &lt;- paste(&quot;x&quot;, 1:K, sep = &quot;_&quot;) X &lt;- data.frame( j = 1:J, X ) %&gt;% tibble::as_tibble() # add outside option X &lt;- rbind( rep(0, dim(X)[2]), X ) X ## # A tibble: 11 × 4 ## j x_1 x_2 x_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 1 1 0.244 -0.00810 ## 3 2 1 0.369 0.472 ## 4 3 1 0.288 0.411 ## 5 4 1 -0.153 0.297 ## 6 5 1 0.756 0.459 ## 7 6 1 0.195 0.391 ## 8 7 1 -0.311 0.0373 ## 9 8 1 -1.11 -0.995 ## 10 9 1 0.562 0.310 ## 11 10 1 -0.0225 -0.0281 M is the data frame such that a row contains the price \\(\\xi_{jt}\\), marginal cost \\(c_{jt}\\), and price \\(p_{jt}\\). After generating the variables, drop 1 - prop_jt products from each market using dplyr::sample_frac function. The variation in the available products is important for the identification of the distribution of consumer-level unobserved heterogeneity. Add the row of the outside option to each market whose index is \\(0\\) and all the variables take value zero. # make market-product data M &lt;- expand.grid( j = 1:J, t = 1:T ) %&gt;% tibble::as_tibble() %&gt;% dplyr::mutate( xi = 0, c = exp(sd_c * rnorm(J*T)), p = exp(price_xi * xi + sd_p * rnorm(J*T)) + c ) M &lt;- M %&gt;% dplyr::group_by(t) %&gt;% dplyr::sample_frac(prop_jt) %&gt;% dplyr::ungroup() # add outside option outside &lt;- data.frame( j = 0, t = 1:T, xi = 0, c = 0, p = 0 ) M &lt;- rbind( M, outside ) %&gt;% dplyr::arrange( t, j ) M ## # A tibble: 700 × 5 ## j t xi c p ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 2 1 0 0.929 1.97 ## 3 3 1 0 0.976 1.91 ## 4 4 1 0 1.02 2.05 ## 5 6 1 0 0.995 1.98 ## 6 7 1 0 1.02 1.98 ## 7 8 1 0 0.997 1.99 ## 8 0 2 0 0 0 ## 9 1 2 0 0.980 1.97 ## 10 4 2 0 1.04 2.04 ## # ℹ 690 more rows Generate the consumer-level heterogeneity. V is the data frame such that a row contains the vector of shocks to consumer-level heterogeneity, \\((\\nu_{i}&#39;, \\upsilon_i)\\). They are all i.i.d. standard normal random variables. # make consumer-market data V &lt;- matrix( rnorm(N * T * (K + 1)), nrow = N * T ) colnames(V) &lt;- c(paste(&quot;v_x&quot;, 1:K, sep = &quot;_&quot;), &quot;v_p&quot;) V &lt;- data.frame( expand.grid( i = 1:N, t = 1:T ), V ) %&gt;% tibble::as_tibble() V ## # A tibble: 50,000 × 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.32 -0.0670 -0.758 -0.590 ## 2 2 1 0.930 1.34 1.16 0.0684 ## 3 3 1 1.08 0.768 -0.725 -0.130 ## 4 4 1 -0.300 0.156 -0.641 -0.896 ## 5 5 1 1.04 1.17 -0.224 1.31 ## 6 6 1 0.709 -0.286 0.948 1.15 ## 7 7 1 -1.27 0.193 -0.212 -0.156 ## 8 8 1 -0.932 -0.780 1.49 -0.362 ## 9 9 1 1.37 -0.205 0.594 -0.998 ## 10 10 1 -0.461 -0.834 0.221 -1.62 ## # ℹ 49,990 more rows Join X, M, V using dplyr::left_join and name it df. df is the data frame such that a row contains variables for a consumer about a product that is available in a market. # make choice data df &lt;- expand.grid( t = 1:T, i = 1:N, j = 0:J ) %&gt;% tibble::as_tibble() %&gt;% dplyr::left_join( V, by = c(&quot;i&quot;, &quot;t&quot;) ) %&gt;% dplyr::left_join( X, by = c(&quot;j&quot;) ) %&gt;% dplyr::left_join( M, by = c(&quot;j&quot;, &quot;t&quot;) ) %&gt;% dplyr::filter(!is.na(p)) %&gt;% dplyr::arrange( t, i, j ) df ## # A tibble: 350,000 × 13 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1.32 -0.0670 -0.758 -0.590 0 0 0 0 ## 2 1 1 2 1.32 -0.0670 -0.758 -0.590 1 0.369 0.472 0 ## 3 1 1 3 1.32 -0.0670 -0.758 -0.590 1 0.288 0.411 0 ## 4 1 1 4 1.32 -0.0670 -0.758 -0.590 1 -0.153 0.297 0 ## 5 1 1 6 1.32 -0.0670 -0.758 -0.590 1 0.195 0.391 0 ## 6 1 1 7 1.32 -0.0670 -0.758 -0.590 1 -0.311 0.0373 0 ## 7 1 1 8 1.32 -0.0670 -0.758 -0.590 1 -1.11 -0.995 0 ## 8 1 2 0 0.930 1.34 1.16 0.0684 0 0 0 0 ## 9 1 2 2 0.930 1.34 1.16 0.0684 1 0.369 0.472 0 ## 10 1 2 3 0.930 1.34 1.16 0.0684 1 0.288 0.411 0 ## # ℹ 349,990 more rows ## # ℹ 2 more variables: c &lt;dbl&gt;, p &lt;dbl&gt; Draw a vector of preference shocks e whose length is the same as the number of rows of df. # draw idiosyncratic shocks e &lt;- evd::rgev(dim(df)[1]) head(e) ## [1] 0.32969177 -1.03771238 1.41496338 5.02048718 -0.01302806 0.57956001 Write a function compute_indirect_utility(df, beta, sigma, mu, omega) that returns a vector whose element is the mean indirect utility of a product for a consumer in a market. The output should have the same length with \\(e\\). # compute indirect utility u &lt;- compute_indirect_utility( df, beta, sigma, mu, omega ) head(u) ## u ## [1,] 0.000000 ## [2,] 3.681882 ## [3,] 3.807728 ## [4,] 3.779036 ## [5,] 3.764454 ## [6,] 4.193113 Write a function compute_choice(X, M, V, e, beta, sigma, mu, omega) that first construct df from X, M, V, second call compute_indirect_utility to obtain the vector of mean indirect utilities u, third compute the choice vector q based on the vector of mean indirect utilities and e, and finally return the data frame to which u and q are added as columns. # compute choice df_choice &lt;- compute_choice( X, M, V, e, beta, sigma, mu, omega ) df_choice ## # A tibble: 350,000 × 16 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1.32 -0.0670 -0.758 -0.590 0 0 0 0 ## 2 1 1 2 1.32 -0.0670 -0.758 -0.590 1 0.369 0.472 0 ## 3 1 1 3 1.32 -0.0670 -0.758 -0.590 1 0.288 0.411 0 ## 4 1 1 4 1.32 -0.0670 -0.758 -0.590 1 -0.153 0.297 0 ## 5 1 1 6 1.32 -0.0670 -0.758 -0.590 1 0.195 0.391 0 ## 6 1 1 7 1.32 -0.0670 -0.758 -0.590 1 -0.311 0.0373 0 ## 7 1 1 8 1.32 -0.0670 -0.758 -0.590 1 -1.11 -0.995 0 ## 8 1 2 0 0.930 1.34 1.16 0.0684 0 0 0 0 ## 9 1 2 2 0.930 1.34 1.16 0.0684 1 0.369 0.472 0 ## 10 1 2 3 0.930 1.34 1.16 0.0684 1 0.288 0.411 0 ## # ℹ 349,990 more rows ## # ℹ 5 more variables: c &lt;dbl&gt;, p &lt;dbl&gt;, u &lt;dbl&gt;, e &lt;dbl&gt;, q &lt;dbl&gt; summary(df_choice) ## t i j v_x_1 ## Min. : 1.00 Min. : 1.0 Min. : 0.000 Min. :-4.302781 ## 1st Qu.: 25.75 1st Qu.:125.8 1st Qu.: 2.000 1st Qu.:-0.683141 ## Median : 50.50 Median :250.5 Median : 5.000 Median : 0.001562 ## Mean : 50.50 Mean :250.5 Mean : 4.791 Mean :-0.002669 ## 3rd Qu.: 75.25 3rd Qu.:375.2 3rd Qu.: 8.000 3rd Qu.: 0.669657 ## Max. :100.00 Max. :500.0 Max. :10.000 Max. : 3.809895 ## v_x_2 v_x_3 v_p x_1 ## Min. :-4.542122 Min. :-3.957618 Min. :-4.218131 Min. :0.0000 ## 1st Qu.:-0.680086 1st Qu.:-0.673321 1st Qu.:-0.671449 1st Qu.:1.0000 ## Median : 0.002374 Median : 0.004941 Median : 0.001083 Median :1.0000 ## Mean :-0.000890 Mean : 0.003589 Mean :-0.002011 Mean :0.8571 ## 3rd Qu.: 0.673194 3rd Qu.: 0.676967 3rd Qu.: 0.671318 3rd Qu.:1.0000 ## Max. : 4.313621 Max. : 4.244194 Max. : 4.017246 Max. :1.0000 ## x_2 x_3 xi c ## Min. :-1.10735 Min. :-0.9947 Min. :0 Min. :0.0000 ## 1st Qu.:-0.15269 1st Qu.: 0.0000 1st Qu.:0 1st Qu.:0.9404 ## Median : 0.19492 Median : 0.2970 Median :0 Median :0.9841 ## Mean : 0.05989 Mean : 0.1128 Mean :0 Mean :0.8560 ## 3rd Qu.: 0.36916 3rd Qu.: 0.3911 3rd Qu.:0 3rd Qu.:1.0260 ## Max. : 0.75589 Max. : 0.4719 Max. :0 Max. :1.2099 ## p u e q ## Min. :0.000 Min. :-190.479 Min. :-2.6364 Min. :0.0000 ## 1st Qu.:1.913 1st Qu.: -2.177 1st Qu.:-0.3301 1st Qu.:0.0000 ## Median :1.981 Median : 0.000 Median : 0.3635 Median :0.0000 ## Mean :1.712 Mean : -1.291 Mean : 0.5763 Mean :0.1429 ## 3rd Qu.:2.039 3rd Qu.: 1.978 3rd Qu.: 1.2418 3rd Qu.:0.0000 ## Max. :2.293 Max. : 10.909 Max. :14.0966 Max. :1.0000 Write a function compute_share(X, M, V, e, beta, sigma, mu, omega) that first construct df from X, M, V, second call compute_choice to obtain a data frame with u and q, third compute the share of each product at each market s and the log difference in the share from the outside option, \\(\\ln(s_{jt}/s_{0t})\\), denoted by y, and finally return the data frame that is summarized at the product-market level, dropped consumer-level variables, and added s and y. # compute share df_share &lt;- compute_share( X, M, V, e, beta, sigma, mu, omega ) df_share ## # A tibble: 700 × 11 ## t j x_1 x_2 x_3 xi c p q s y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 0 0 144 0.288 0 ## 2 1 2 1 0.369 0.472 0 0.929 1.97 47 0.094 -1.12 ## 3 1 3 1 0.288 0.411 0 0.976 1.91 37 0.074 -1.36 ## 4 1 4 1 -0.153 0.297 0 1.02 2.05 35 0.07 -1.41 ## 5 1 6 1 0.195 0.391 0 0.995 1.98 60 0.12 -0.875 ## 6 1 7 1 -0.311 0.0373 0 1.02 1.98 44 0.088 -1.19 ## 7 1 8 1 -1.11 -0.995 0 0.997 1.99 133 0.266 -0.0795 ## 8 2 0 0 0 0 0 0 0 170 0.34 0 ## 9 2 1 1 0.244 -0.00810 0 0.980 1.97 84 0.168 -0.705 ## 10 2 4 1 -0.153 0.297 0 1.04 2.04 35 0.07 -1.58 ## # ℹ 690 more rows summary(df_share) ## t j x_1 x_2 ## Min. : 1.00 Min. : 0.000 Min. :0.0000 Min. :-1.10735 ## 1st Qu.: 25.75 1st Qu.: 2.000 1st Qu.:1.0000 1st Qu.:-0.15269 ## Median : 50.50 Median : 5.000 Median :1.0000 Median : 0.19492 ## Mean : 50.50 Mean : 4.791 Mean :0.8571 Mean : 0.05989 ## 3rd Qu.: 75.25 3rd Qu.: 8.000 3rd Qu.:1.0000 3rd Qu.: 0.36916 ## Max. :100.00 Max. :10.000 Max. :1.0000 Max. : 0.75589 ## x_3 xi c p ## Min. :-0.9947 Min. :0 Min. :0.0000 Min. :0.000 ## 1st Qu.: 0.0000 1st Qu.:0 1st Qu.:0.9404 1st Qu.:1.913 ## Median : 0.2970 Median :0 Median :0.9841 Median :1.981 ## Mean : 0.1128 Mean :0 Mean :0.8560 Mean :1.712 ## 3rd Qu.: 0.3911 3rd Qu.:0 3rd Qu.:1.0260 3rd Qu.:2.039 ## Max. : 0.4719 Max. :0 Max. :1.2099 Max. :2.293 ## q s y ## Min. : 26.00 Min. :0.0520 Min. :-1.89354 ## 1st Qu.: 43.00 1st Qu.:0.0860 1st Qu.:-1.35812 ## Median : 52.00 Median :0.1040 Median :-1.16644 ## Mean : 71.43 Mean :0.1429 Mean :-0.99214 ## 3rd Qu.: 73.00 3rd Qu.:0.1460 3rd Qu.:-0.81801 ## Max. :195.00 Max. :0.3900 Max. : 0.05196 13.2 Estimate the parameters Estimate the parameters assuming there is no consumer-level heterogeneity, i.e., by assuming: \\[ \\ln \\frac{s_{jt}}{s_{0t}} = \\beta&#39; x_{jt} + \\alpha p_{jt}. \\] This can be implemented using lm function. Print out the estimate results. ## ## Call: ## lm(formula = y ~ -1 + x_1 + x_2 + x_3 + p, data = df_share) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57415 -0.09781 0.00000 0.10491 0.51149 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x_1 1.20614 0.18150 6.646 6.10e-11 *** ## x_2 0.21995 0.02841 7.741 3.49e-14 *** ## x_3 -0.92818 0.03364 -27.589 &lt; 2e-16 *** ## p -1.12982 0.09086 -12.435 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1677 on 696 degrees of freedom ## Multiple R-squared: 0.9778, Adjusted R-squared: 0.9777 ## F-statistic: 7678 on 4 and 696 DF, p-value: &lt; 2.2e-16 We estimate the model using simulated share. When optimizing an objective function that uses the Monte Carlo simulation, it is important to keep the realizations of the shocks the same across the evaluations of the objective function. If the realization of the shocks differ across the objective function evaluations, the optimization algorithm will not converge because it cannot distinguish the change in the value of the objective function due to the difference in the parameters and the difference in the realized shocks. The best practice to avoid this problem is to generate the shocks outside the optimization algorithm as in the current case. If the size of the shocks can be too large to store in the memory, the second best practice is to make sure to set the seed inside the optimization algorithm so that the realized shocks are the same across function evaluations. For this reason, we first draw Monte Carlo consumer-level heterogeneity V_mcmc and Monte Carlo preference shocks e_mcmc. The number of simulations is L. This does not have to be the same with the actual number of consumers N. # mixed logit estimation ## draw mcmc V V_mcmc &lt;- matrix( rnorm(L * T * (K + 1)), nrow = L * T ) colnames(V_mcmc) &lt;- c(paste(&quot;v_x&quot;, 1:K, sep = &quot;_&quot;), &quot;v_p&quot;) V_mcmc &lt;- data.frame( expand.grid( i = 1:L, t = 1:T ), V_mcmc ) %&gt;% tibble::as_tibble() V_mcmc ## # A tibble: 50,000 × 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 -0.715 -0.798 -0.548 1.09 ## 2 2 1 0.0243 -0.943 -1.88 0.471 ## 3 3 1 -0.183 0.660 0.669 1.45 ## 4 4 1 0.0207 3.36 -0.363 1.58 ## 5 5 1 -1.94 0.221 1.02 0.943 ## 6 6 1 0.876 -0.636 0.0432 0.324 ## 7 7 1 1.96 -0.195 -0.0950 1.39 ## 8 8 1 1.11 0.571 1.77 0.497 ## 9 9 1 1.45 0.154 1.96 0.740 ## 10 10 1 -1.00 -0.413 0.0542 -0.469 ## # ℹ 49,990 more rows ## draw mcmc e df_mcmc &lt;- expand.grid( t = 1:T, i = 1:L, j = 0:J ) %&gt;% tibble::as_tibble() %&gt;% dplyr::left_join( V_mcmc, by = c(&quot;i&quot;, &quot;t&quot;) ) %&gt;% dplyr::left_join( X, by = c(&quot;j&quot;) ) %&gt;% dplyr::left_join( M, by = c(&quot;j&quot;, &quot;t&quot;) ) %&gt;% dplyr::filter(!is.na(p)) %&gt;% dplyr::arrange( t, i, j ) # draw idiosyncratic shocks e_mcmc &lt;- evd::rgev(dim(df_mcmc)[1]) head(e_mcmc) ## [1] 0.265602821 0.652183731 -0.006127627 -0.317288366 3.145022297 ## [6] -1.277026910 Use compute_share to check the simulated share at the true parameter using the Monte Carlo shocks. Remember that the number of consumers should be set at L instead of N. df_share_mcmc ## # A tibble: 700 × 11 ## t j x_1 x_2 x_3 xi c p q s y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 0 0 171 0.342 0 ## 2 1 2 1 0.369 0.472 0 0.929 1.97 46 0.092 -1.31 ## 3 1 3 1 0.288 0.411 0 0.976 1.91 37 0.074 -1.53 ## 4 1 4 1 -0.153 0.297 0 1.02 2.05 38 0.076 -1.50 ## 5 1 6 1 0.195 0.391 0 0.995 1.98 38 0.076 -1.50 ## 6 1 7 1 -0.311 0.0373 0 1.02 1.98 52 0.104 -1.19 ## 7 1 8 1 -1.11 -0.995 0 0.997 1.99 118 0.236 -0.371 ## 8 2 0 0 0 0 0 0 0 194 0.388 0 ## 9 2 1 1 0.244 -0.00810 0 0.980 1.97 62 0.124 -1.14 ## 10 2 4 1 -0.153 0.297 0 1.04 2.04 37 0.074 -1.66 ## # ℹ 690 more rows Vectorize the parameters to a vector theta because optim requires the maximiand to be a vector. # set parameters theta &lt;- c( beta, sigma, mu, omega ) theta ## [1] 4.0000000 0.1836433 -0.8356286 1.5952808 0.3295078 0.8204684 0.5000000 ## [8] 1.0000000 Write a function compute_nlls_objective_a3(theta, df_share, X, M, V_mcmc, e_mcmc) that first computes the simulated share and then compute the mean-squared error between the share data. nlls_objective ## [1] 0.0004815657 Draw a graph of the objective function that varies each parameter from 0.5, 0.6, \\(\\cdots\\), 1.5 of the true value. First try with the actual shocks V and e and then try with the Monte Carlo shocks V_mcmc and e_mcmc. You will some of the graph does not look good with the Monte Carlo shocks. It will cause the approximation error. Because this takes time, you may want to parallelize the computation using %dopar functionality of foreach loop. To do so, first install doParallel package and then load it and register the workers as follows: registerDoParallel() This automatically detect the number of cores available at your computer and registers them as the workers. Then, you only have to change %do% to %dopar in the foreach loop as follows: foreach ( i = 1:4 ) %dopar% { # this part is parallelized y &lt;- 2 * i return(y) } ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 6 ## ## [[4]] ## [1] 8 In windows, you may have to explicitly pass packages, functions, and data to the worker by using .export and .packages options as follows: temp_func &lt;- function(x) { y &lt;- 2 * x return(y) } foreach ( i = 1:4, .export = &quot;temp_func&quot;, .packages = &quot;magrittr&quot; ) %dopar% { # this part is parallelized y &lt;- temp_func(i) return(y) } ## Warning in e$fun(obj, substitute(ex), parent.frame(), e$data): already ## exporting variable(s): temp_func ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 6 ## ## [[4]] ## [1] 8 If you have called a function in a package in this way dplyr::mutate, then you will not have to pass dplyr by .packages option. This is one of the reasons why I prefer to explicitly call the package every time I call a function. If you have compiled your functions in a package, you will just have to pass the package as follows: # this function is compiled in the package EmpiricalIO # temp_func &lt;- function(x) { # y &lt;- 2 * x # return(y) # } foreach ( i = 1:4, .packages = c( &quot;EmpiricalIO&quot;, &quot;magrittr&quot;) ) %dopar% { # this part is parallelized y &lt;- temp_func(i) return(y) } ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 6 ## ## [[4]] ## [1] 8 The graphs with the true shocks: ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] ## ## [[6]] ## ## [[7]] ## ## [[8]] The graphs with the Monte Carlo shocks: ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] ## ## [[6]] ## ## [[7]] ## ## [[8]] Use optim to find the minimizer of the objective function using Nelder-Mead method. You can start from the true parameter values. Compare the estimates with the true parameters. ## $par ## [1] 3.9907237 0.1778582 -0.8244977 1.5326774 0.3602383 0.9336009 0.4883030 ## [8] 1.0462494 ## ## $value ## [1] 0.0004678286 ## ## $counts ## function gradient ## 237 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## true estimates ## 1 4.0000000 3.9907237 ## 2 0.1836433 0.1778582 ## 3 -0.8356286 -0.8244977 ## 4 1.5952808 1.5326774 ## 5 0.3295078 0.3602383 ## 6 0.8204684 0.9336009 ## 7 0.5000000 0.4883030 ## 8 1.0000000 1.0462494 "],["assignment4.html", "Chapter 14 Assignment 4: Demand Function Estimation II 14.1 Simulate data 14.2 Estimate the parameters", " Chapter 14 Assignment 4: Demand Function Estimation II 14.1 Simulate data Be carefull that some parameters are changed from assignment 3. We simulate data from a discrete choice model that is the same with in assignment 3 except for the existence of unobserved product-specific fixed effects. There are \\(T\\) markets and each market has \\(N\\) consumers. There are \\(J\\) products and the indirect utility of consumer \\(i\\) in market \\(t\\) for product \\(j\\) is: \\[ u_{itj} = \\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt} + \\epsilon_{ijt}, \\] where \\(\\epsilon_{ijt}\\) is an i.i.d. type-I extreme random variable. \\(x_j\\) is \\(K\\)-dimensional observed characteristics of the product. \\(p_{jt}\\) is the retail price of the product in the market. \\(\\xi_{jt}\\) is product-market specific fixed effect. \\(p_{jt}\\) can be correlated with \\(\\xi_{jt}\\) but \\(x_{jt}\\)s are independent of \\(\\xi_{jt}\\). \\(j = 0\\) is an outside option whose indirect utility is: \\[ u_{it0} = \\epsilon_{i0t}, \\] where \\(\\epsilon_{i0t}\\) is an i.i.d. type-I extreme random variable. \\(\\beta_{it}\\) and \\(\\alpha_{it}\\) are different across consumers, and they are distributed as: \\[ \\beta_{itk} = \\beta_{0k} + \\sigma_k \\nu_{itk}, \\] \\[ \\alpha_{it} = - \\exp(\\mu + \\omega \\upsilon_{it}) = - \\exp(\\mu + \\frac{\\omega^2}{2}) + [- \\exp(\\mu + \\omega \\upsilon_{it}) + \\exp(\\mu + \\frac{\\omega^2}{2})] \\equiv \\alpha_0 + \\tilde{\\alpha}_{it}, \\] where \\(\\nu_{itk}\\) for \\(k = 1, \\cdots, K\\) and \\(\\upsilon_{it}\\) are i.i.d. standard normal random variables. \\(\\alpha_0\\) is the mean of \\(\\alpha_i\\) and \\(\\tilde{\\alpha}_i\\) is the deviation from the mean. Given a choice set in the market, \\(\\mathcal{J}_t \\cup \\{0\\}\\), a consumer chooses the alternative that maximizes her utility: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] The choice probability of product \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[ \\sigma_{jt}(p_t, x_t, \\xi_t) = \\mathbb{P}\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] Suppose that we only observe the share data: \\[ s_{jt} = \\frac{1}{N} \\sum_{i = 1}^N q_{ijt}, \\] along with the product-market characteristics \\(x_{jt}\\) and the retail prices \\(p_{jt}\\) for \\(j \\in \\mathcal{J}_t \\cup \\{0\\}\\) for \\(t = 1, \\cdots, T\\). We do not observe the choice data \\(q_{ijt}\\) nor shocks \\(\\xi_{jt}, \\nu_{it}, \\upsilon_{it}, \\epsilon_{ijt}\\). We draw \\(\\xi_{jt}\\) from i.i.d. normal distribution with mean 0 and standard deviation \\(\\sigma_{\\xi}\\). Set the seed, constants, and parameters of interest as follows. # set the seed set.seed(1) # number of products J &lt;- 10 # dimension of product characteristics including the intercept K &lt;- 3 # number of markets T &lt;- 100 # number of consumers per market N &lt;- 500 # number of Monte Carlo L &lt;- 500 # set parameters of interests beta &lt;- rnorm(K); beta[1] &lt;- 4 beta ## [1] 4.0000000 0.1836433 -0.8356286 sigma &lt;- abs(rnorm(K)); sigma ## [1] 1.5952808 0.3295078 0.8204684 mu &lt;- 0.5 omega &lt;- 1 Generate the covariates as follows. The product-market characteristics: \\[ x_{j1} = 1, x_{jk} \\sim N(0, \\sigma_x), k = 2, \\cdots, K, \\] where \\(\\sigma_x\\) is referred to as sd_x in the code. The product-market-specific unobserved fixed effect: \\[ \\xi_{jt} \\sim N(0, \\sigma_\\xi), \\] where \\(\\sigma_xi\\) is referred to as sd_xi in the code. The marginal cost of product \\(j\\) in market \\(t\\): \\[ c_{jt} \\sim \\text{logNormal}(0, \\sigma_c), \\] where \\(\\sigma_c\\) is referred to as sd_c in the code. The retail price: \\[ p_{jt} - c_{jt} \\sim \\text{logNorm}(\\gamma \\xi_{jt}, \\sigma_p^2), \\] where \\(\\gamma\\) is referred to as price_xi and \\(\\sigma_p\\) as sd_p in the code. This price is not the equilibrium price. We will revisit this point in a subsequent assignment. The value of the auxiliary parameters are set as follows: # set auxiliary parameters price_xi &lt;- 1 sd_x &lt;- 2 sd_xi &lt;- 0.1 sd_c &lt;- 0.5 sd_p &lt;- 0.01 X is the data frame such that a row contains the characteristics vector \\(x_{j}\\) of a product and columns are product index and observed product characteristics. The dimension of the characteristics \\(K\\) is specified above. Add the row of the outside option whose index is \\(0\\) and all the characteristics are zero. # make product characteristics data X &lt;- matrix( sd_x * rnorm(J * (K - 1)), nrow = J ) X &lt;- cbind( rep(1, J), X ) colnames(X) &lt;- paste(&quot;x&quot;, 1:K, sep = &quot;_&quot;) X &lt;- data.frame(j = 1:J, X) %&gt;% tibble::as_tibble() # add outside option X &lt;- rbind( rep(0, dim(X)[2]), X ) X ## # A tibble: 11 × 4 ## j x_1 x_2 x_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 1 1 0.975 -0.0324 ## 3 2 1 1.48 1.89 ## 4 3 1 1.15 1.64 ## 5 4 1 -0.611 1.19 ## 6 5 1 3.02 1.84 ## 7 6 1 0.780 1.56 ## 8 7 1 -1.24 0.149 ## 9 8 1 -4.43 -3.98 ## 10 9 1 2.25 1.24 ## 11 10 1 -0.0899 -0.112 M is the data frame such that a row contains the price \\(\\xi_{jt}\\), marginal cost \\(c_{jt}\\), and price \\(p_{jt}\\). After generating the variables, drop some products in each market. In this assignment, we drop products in a different way from the last assignment. In order to change the number of available products in each market, for each market, first draw \\(J_t\\) from a discrete uniform distribution between \\(1\\) and \\(J\\). Then, drop products from each market using dplyr::sample_frac function with the realized number of available products. The variation in the available products is important for the identification of the distribution of consumer-level unobserved heterogeneity. Add the row of the outside option to each market whose index is \\(0\\) and all the variables take value zero. # make market-product data M &lt;- expand.grid( j = 1:J, t = 1:T ) %&gt;% tibble::as_tibble() %&gt;% dplyr::mutate( xi = sd_xi * rnorm(J * T), c = exp(sd_c * rnorm(J * T)), p = exp(price_xi * xi + sd_p * rnorm(J * T)) + c ) M &lt;- M %&gt;% dplyr::group_by(t) %&gt;% dplyr::sample_frac(size = purrr::rdunif(1, J) / J) %&gt;% dplyr::ungroup() # add outside option outside &lt;- data.frame( j = 0, t = 1:T, xi = 0, c = 0, p = 0 ) M &lt;- rbind( M, outside ) %&gt;% dplyr::arrange( t, j ) M ## # A tibble: 633 × 5 ## j t xi c p ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 1 1 -0.0156 0.604 1.58 ## 3 4 1 0.0418 1.30 2.35 ## 4 0 2 0 0 0 ## 5 1 2 -0.0394 0.890 1.87 ## 6 3 2 0.110 2.29 3.40 ## 7 4 2 0.0763 0.997 2.09 ## 8 6 2 -0.0253 1.15 2.13 ## 9 7 2 0.0697 0.613 1.68 ## 10 8 2 0.0557 0.629 1.67 ## # ℹ 623 more rows Generate the consumer-level heterogeneity. V is the data frame such that a row contains the vector of shocks to consumer-level heterogeneity, \\((\\nu_{i}&#39;, \\upsilon_i)\\). They are all i.i.d. standard normal random variables. # make consumer-market data V &lt;- matrix( rnorm(N * T * (K + 1)), nrow = N * T ) colnames(V) &lt;- c( paste(&quot;v_x&quot;, 1:K, sep = &quot;_&quot;), &quot;v_p&quot; ) V &lt;- data.frame( expand.grid( i = 1:N, t = 1:T ), V ) %&gt;% tibble::as_tibble() V ## # A tibble: 50,000 × 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 -1.37 0.211 1.65 0.0141 ## 2 2 1 1.37 0.378 1.35 0.387 ## 3 3 1 -2.06 -0.0662 -2.45 -1.17 ## 4 4 1 -0.992 -0.727 -1.33 -1.42 ## 5 5 1 0.252 1.87 0.751 0.317 ## 6 6 1 -1.06 -0.531 1.34 -0.224 ## 7 7 1 -0.217 1.03 0.909 -0.593 ## 8 8 1 -0.838 -0.861 -0.612 1.54 ## 9 9 1 0.659 -1.43 -1.77 0.340 ## 10 10 1 0.452 -0.239 0.138 0.695 ## # ℹ 49,990 more rows Join X, M, V using dplyr::left_join and name it df. df is the data frame such that a row contains variables for a consumer about a product that is available in a market. # make choice data df &lt;- expand.grid( t = 1:T, i = 1:N, j = 0:J ) %&gt;% tibble::as_tibble() %&gt;% dplyr::left_join( V, by = c(&quot;i&quot;, &quot;t&quot;) ) %&gt;% dplyr::left_join( X, by = c(&quot;j&quot;) ) %&gt;% dplyr::left_join( M, by = c(&quot;j&quot;, &quot;t&quot;) ) %&gt;% dplyr::filter(!is.na(p)) %&gt;% dplyr::arrange( t, i, j ) df ## # A tibble: 316,500 × 13 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 -1.37 0.211 1.65 0.0141 0 0 0 0 ## 2 1 1 1 -1.37 0.211 1.65 0.0141 1 0.975 -0.0324 -0.0156 ## 3 1 1 4 -1.37 0.211 1.65 0.0141 1 -0.611 1.19 0.0418 ## 4 1 2 0 1.37 0.378 1.35 0.387 0 0 0 0 ## 5 1 2 1 1.37 0.378 1.35 0.387 1 0.975 -0.0324 -0.0156 ## 6 1 2 4 1.37 0.378 1.35 0.387 1 -0.611 1.19 0.0418 ## 7 1 3 0 -2.06 -0.0662 -2.45 -1.17 0 0 0 0 ## 8 1 3 1 -2.06 -0.0662 -2.45 -1.17 1 0.975 -0.0324 -0.0156 ## 9 1 3 4 -2.06 -0.0662 -2.45 -1.17 1 -0.611 1.19 0.0418 ## 10 1 4 0 -0.992 -0.727 -1.33 -1.42 0 0 0 0 ## # ℹ 316,490 more rows ## # ℹ 2 more variables: c &lt;dbl&gt;, p &lt;dbl&gt; Draw a vector of preference shocks e whose length is the same as the number of rows of df. # draw idiosyncratic shocks e &lt;- evd::rgev(dim(df)[1]) head(e) ## [1] 0.1917775 -0.3312816 0.2428217 1.0164097 1.4761643 2.8297340 Write a function compute_indirect_utility(df, beta, sigma, mu, omega) that returns a vector whose element is the mean indirect utility of a product for a consumer in a market. The output should have the same length with \\(e\\). (This function is the same with assignment 3. You can use the function.) # compute indirect utility u &lt;- compute_indirect_utility( df = df, beta = beta, sigma = sigma, mu = mu, omega = omega ) head(u) ## u ## [1,] 0.0000000 ## [2,] -0.6238974 ## [3,] -1.6225075 ## [4,] 0.0000000 ## [5,] 2.6171184 ## [6,] 0.6490776 In the previous assingment, we computed predicted share by simulating choice and taking their average. Instead, we compute the actual share by: \\[ s_{jt} = \\frac{1}{N} \\sum_{i = 1}^N \\frac{\\exp[\\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt}]}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp[\\beta_{it}&#39; x_k + \\alpha_{it} p_{kt} + \\xi_{jt}]} \\] and the predicted share by: \\[ \\widehat{\\sigma}_{j}(x, p_t, \\xi_t) = \\frac{1}{L} \\sum_{l = 1}^L \\frac{\\exp[\\beta_{t}^{(l)\\prime} x_j + \\alpha_{t}^{(l)} p_{jt} + \\xi_{jt}]}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp[\\beta_{t}^{(l)\\prime} x_k + \\alpha_{t}^{(l)} p_{kt} + \\xi_{jt}]}. \\] To do so, write a function compute_choice_smooth(X, M, V, beta, sigma, mu, omega) in which the choice of each consumer is not: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}, \\] but \\[ \\tilde{q}_{ijt} = \\frac{\\exp(u_{ijt})}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp(u_{ikt})}. \\] df_choice_smooth &lt;- compute_choice_smooth( X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) summary(df_choice_smooth) ## t i j v_x_1 ## Min. : 1.00 Min. : 1.0 Min. : 0.00 Min. :-4.302781 ## 1st Qu.: 23.00 1st Qu.:125.8 1st Qu.: 2.00 1st Qu.:-0.685539 ## Median : 48.00 Median :250.5 Median : 4.00 Median : 0.001041 ## Mean : 49.67 Mean :250.5 Mean : 4.49 Mean :-0.002541 ## 3rd Qu.: 77.00 3rd Qu.:375.2 3rd Qu.: 7.00 3rd Qu.: 0.673061 ## Max. :100.00 Max. :500.0 Max. :10.00 Max. : 3.809895 ## v_x_2 v_x_3 v_p x_1 ## Min. :-4.542122 Min. :-3.957618 Min. :-4.218131 Min. :0.000 ## 1st Qu.:-0.679702 1st Qu.:-0.672701 1st Qu.:-0.669446 1st Qu.:1.000 ## Median : 0.000935 Median : 0.003104 Median : 0.001976 Median :1.000 ## Mean : 0.000478 Mean : 0.003428 Mean : 0.000017 Mean :0.842 ## 3rd Qu.: 0.673109 3rd Qu.: 0.678344 3rd Qu.: 0.670699 3rd Qu.:1.000 ## Max. : 4.313621 Max. : 4.244194 Max. : 4.074300 Max. :1.000 ## x_2 x_3 xi c ## Min. :-4.4294 Min. :-3.9787 Min. :-0.2996949 Min. :0.0000 ## 1st Qu.:-0.6108 1st Qu.: 0.0000 1st Qu.:-0.0527368 1st Qu.:0.5250 ## Median : 0.7797 Median : 1.1878 Median : 0.0000000 Median :0.8775 ## Mean : 0.3015 Mean : 0.5034 Mean :-0.0005149 Mean :0.9507 ## 3rd Qu.: 1.4766 3rd Qu.: 1.6424 3rd Qu.: 0.0556663 3rd Qu.:1.3203 ## Max. : 3.0236 Max. : 1.8877 Max. : 0.3810277 Max. :4.3043 ## p u q ## Min. :0.000 Min. :-345.287 Min. :0.000000 ## 1st Qu.:1.516 1st Qu.: -3.030 1st Qu.:0.001435 ## Median :1.916 Median : 0.000 Median :0.030121 ## Mean :1.797 Mean : -1.869 Mean :0.157978 ## 3rd Qu.:2.336 3rd Qu.: 1.559 3rd Qu.:0.161379 ## Max. :5.513 Max. : 22.434 Max. :1.000000 Next, write a function compute_share_smooth(X, M, V, beta, sigma, mu, omega) that calls compute_choice_smooth and then returns the share based on above \\(\\tilde{q}_{ijt}\\). If we use these functions with the Monte Carlo shocks, it gives us the predicted share of the products. df_share_smooth &lt;- compute_share_smooth( X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) summary(df_share_smooth) ## t j x_1 x_2 ## Min. : 1.00 Min. : 0.00 Min. :0.000 Min. :-4.4294 ## 1st Qu.: 23.00 1st Qu.: 2.00 1st Qu.:1.000 1st Qu.:-0.6108 ## Median : 48.00 Median : 4.00 Median :1.000 Median : 0.7797 ## Mean : 49.67 Mean : 4.49 Mean :0.842 Mean : 0.3015 ## 3rd Qu.: 77.00 3rd Qu.: 7.00 3rd Qu.:1.000 3rd Qu.: 1.4766 ## Max. :100.00 Max. :10.00 Max. :1.000 Max. : 3.0236 ## x_3 xi c p ## Min. :-3.9787 Min. :-0.2996949 Min. :0.0000 Min. :0.000 ## 1st Qu.: 0.0000 1st Qu.:-0.0527368 1st Qu.:0.5250 1st Qu.:1.516 ## Median : 1.1878 Median : 0.0000000 Median :0.8775 Median :1.916 ## Mean : 0.5034 Mean :-0.0005149 Mean :0.9507 Mean :1.797 ## 3rd Qu.: 1.6424 3rd Qu.: 0.0556663 3rd Qu.:1.3203 3rd Qu.:2.336 ## Max. : 1.8877 Max. : 0.3810277 Max. :4.3043 Max. :5.513 ## q s y ## Min. : 1.748 Min. :0.003497 Min. :-4.23971 ## 1st Qu.: 17.982 1st Qu.:0.035964 1st Qu.:-1.95098 ## Median : 40.241 Median :0.080483 Median :-1.22808 ## Mean : 78.989 Mean :0.157978 Mean :-1.15979 ## 3rd Qu.:123.006 3rd Qu.:0.246011 3rd Qu.:-0.03626 ## Max. :325.631 Max. :0.651262 Max. : 1.26578 Use this df_share_smooth as the data to estimate the parameters in the following section. 14.2 Estimate the parameters First draw Monte Carlo consumer-level heterogeneity V_mcmc and Monte Carlo preference shocks e_mcmc. The number of simulations is L. This does not have to be the same with the actual number of consumers N. # mixed logit estimation ## draw mcmc V V_mcmc &lt;- matrix( rnorm(L * T * (K + 1)), nrow = L * T ) colnames(V_mcmc) &lt;- c( paste(&quot;v_x&quot;, 1:K, sep = &quot;_&quot;), &quot;v_p&quot; ) V_mcmc &lt;- data.frame( expand.grid( i = 1:L, t = 1:T ), V_mcmc ) %&gt;% tibble::as_tibble() V_mcmc ## # A tibble: 50,000 × 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.488 -1.51 0.528 -0.468 ## 2 2 1 1.16 0.507 -0.527 -0.516 ## 3 3 1 -2.49 -0.318 -0.0996 -0.893 ## 4 4 1 0.0952 -0.133 -2.05 1.92 ## 5 5 1 -1.11 0.103 2.24 0.753 ## 6 6 1 0.903 0.496 0.287 1.53 ## 7 7 1 0.913 -0.144 0.129 -1.17 ## 8 8 1 -1.52 0.357 -0.475 -0.736 ## 9 9 1 0.643 0.219 0.815 -1.27 ## 10 10 1 -0.358 0.272 -0.650 -2.09 ## # ℹ 49,990 more rows ## draw mcmc e df_mcmc &lt;- expand.grid( t = 1:T, i = 1:L, j = 0:J ) %&gt;% tibble::as_tibble() %&gt;% dplyr::left_join( V_mcmc, by = c(&quot;i&quot;, &quot;t&quot;) ) %&gt;% dplyr::left_join( X, by = c(&quot;j&quot;) ) %&gt;% dplyr::left_join( M, by = c(&quot;j&quot;, &quot;t&quot;) ) %&gt;% dplyr::filter(!is.na(p)) %&gt;% dplyr::arrange( t, i, j ) # draw idiosyncratic shocks e_mcmc &lt;- evd::rgev(dim(df_mcmc)[1]) head(e_mcmc) ## [1] 0.1006013 2.7039824 1.0540278 2.4697389 1.6721181 -1.0283872 Vectorize the parameters to a vector theta because optim requires the maximiand to be a vector. # set parameters theta &lt;- c( beta, sigma, mu, omega ) theta ## [1] 4.0000000 0.1836433 -0.8356286 1.5952808 0.3295078 0.8204684 0.5000000 ## [8] 1.0000000 Estimate the parameters assuming there is no product-specific unobserved fixed effects \\(\\xi_{jt}\\), i.e., using the functions in assignment 3. To do so, first modify M to M_no in which xi is replaced with 0 and estimate the model with M_no. Otherwise, your function will compute the share with the true xi. M_no &lt;- M %&gt;% dplyr::mutate(xi = 0) ## $par ## [1] 4.0467164 0.1707430 -0.8092369 1.8086380 0.3718435 0.7275257 0.4892642 ## [8] 1.0453963 ## ## $value ## [1] 0.0003270119 ## ## $counts ## function gradient ## 221 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## true estimates ## 1 4.0000000 4.0467164 ## 2 0.1836433 0.1707430 ## 3 -0.8356286 -0.8092369 ## 4 1.5952808 1.8086380 ## 5 0.3295078 0.3718435 ## 6 0.8204684 0.7275257 ## 7 0.5000000 0.4892642 ## 8 1.0000000 1.0453963 Next, we estimate the model allowing for the product-market-specific unobserved fixed effect \\(\\xi_{jt}\\) using the BLP algorithm. To do so, we slightly modify the compute_indirect_utility, compute_choice_smooth, and compute_share_smooth functions so that they receive \\(\\delta_{jt}\\) to compute the indirect utilities, choices, and shares. Be careful that the treatment of \\(\\alpha_i\\) is slightly different from the lecture note, because we assumed that \\(\\alpha_i\\)s are log-normal random variables. Compute and print out \\(\\delta_{jt}\\) at the true parameters, i.e.: \\[ \\delta_{jt} = \\beta_0&#39; x_j + \\alpha_0&#39; p_{jt} + \\xi_{jt}. \\] delta ## # A tibble: 633 × 3 ## t j delta ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 1 -0.115 ## 3 1 4 -3.46 ## 4 2 0 0 ## 5 2 1 -0.920 ## 6 2 3 -6.29 ## 7 2 4 -2.70 ## 8 2 6 -2.97 ## 9 2 7 -0.846 ## 10 2 8 2.04 ## # ℹ 623 more rows Write a function compute_indirect_utility_delta(df, delta, sigma, mu, omega) that returns a vector whose element is the mean indirect utility of a product for a consumer in a market. The output should have the same length with \\(e\\). Print out the output with \\(\\delta_{jt}\\) evaluated at the true parameters. Check if the output is close to the true indirect utilities. # compute indirect utility from delta u_delta &lt;- compute_indirect_utility_delta( df, delta, sigma, mu, omega ) head(u_delta) ## u ## [1,] 0.0000000 ## [2,] -0.6238974 ## [3,] -1.6225075 ## [4,] 0.0000000 ## [5,] 2.6171184 ## [6,] 0.6490776 summary(u - u_delta) ## u ## Min. :-5.684e-14 ## 1st Qu.:-4.441e-16 ## Median : 0.000e+00 ## Mean : 1.388e-17 ## 3rd Qu.: 3.331e-16 ## Max. : 5.684e-14 Write a function compute_choice_smooth_delta(X, M, V, delta, sigma, mu, omega) that first construct df from X, M, V, second call compute_indirect_utility_delta to obtain the vector of mean indirect utilities u, third compute the (smooth) choice vector q based on the vector of mean indirect utilities, and finally return the data frame to which u and q are added as columns. Print out the output with \\(\\delta_{jt}\\) evaluated at the true parameters. Check if the output is close to the true (smooth) choice vector. # compute choice df_choice_smooth_delta &lt;- compute_choice_smooth_delta( X, M, V, delta, sigma, mu, omega ) df_choice_smooth_delta ## # A tibble: 316,500 × 15 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 -1.37 0.211 1.65 0.0141 0 0 0 0 ## 2 1 1 1 -1.37 0.211 1.65 0.0141 1 0.975 -0.0324 -0.0156 ## 3 1 1 4 -1.37 0.211 1.65 0.0141 1 -0.611 1.19 0.0418 ## 4 1 2 0 1.37 0.378 1.35 0.387 0 0 0 0 ## 5 1 2 1 1.37 0.378 1.35 0.387 1 0.975 -0.0324 -0.0156 ## 6 1 2 4 1.37 0.378 1.35 0.387 1 -0.611 1.19 0.0418 ## 7 1 3 0 -2.06 -0.0662 -2.45 -1.17 0 0 0 0 ## 8 1 3 1 -2.06 -0.0662 -2.45 -1.17 1 0.975 -0.0324 -0.0156 ## 9 1 3 4 -2.06 -0.0662 -2.45 -1.17 1 -0.611 1.19 0.0418 ## 10 1 4 0 -0.992 -0.727 -1.33 -1.42 0 0 0 0 ## # ℹ 316,490 more rows ## # ℹ 4 more variables: c &lt;dbl&gt;, p &lt;dbl&gt;, u &lt;dbl&gt;, q &lt;dbl&gt; summary(df_choice_smooth_delta) ## t i j v_x_1 ## Min. : 1.00 Min. : 1.0 Min. : 0.00 Min. :-4.302781 ## 1st Qu.: 23.00 1st Qu.:125.8 1st Qu.: 2.00 1st Qu.:-0.685539 ## Median : 48.00 Median :250.5 Median : 4.00 Median : 0.001041 ## Mean : 49.67 Mean :250.5 Mean : 4.49 Mean :-0.002541 ## 3rd Qu.: 77.00 3rd Qu.:375.2 3rd Qu.: 7.00 3rd Qu.: 0.673061 ## Max. :100.00 Max. :500.0 Max. :10.00 Max. : 3.809895 ## v_x_2 v_x_3 v_p x_1 ## Min. :-4.542122 Min. :-3.957618 Min. :-4.218131 Min. :0.000 ## 1st Qu.:-0.679702 1st Qu.:-0.672701 1st Qu.:-0.669446 1st Qu.:1.000 ## Median : 0.000935 Median : 0.003104 Median : 0.001976 Median :1.000 ## Mean : 0.000478 Mean : 0.003428 Mean : 0.000017 Mean :0.842 ## 3rd Qu.: 0.673109 3rd Qu.: 0.678344 3rd Qu.: 0.670699 3rd Qu.:1.000 ## Max. : 4.313621 Max. : 4.244194 Max. : 4.074300 Max. :1.000 ## x_2 x_3 xi c ## Min. :-4.4294 Min. :-3.9787 Min. :-0.2996949 Min. :0.0000 ## 1st Qu.:-0.6108 1st Qu.: 0.0000 1st Qu.:-0.0527368 1st Qu.:0.5250 ## Median : 0.7797 Median : 1.1878 Median : 0.0000000 Median :0.8775 ## Mean : 0.3015 Mean : 0.5034 Mean :-0.0005149 Mean :0.9507 ## 3rd Qu.: 1.4766 3rd Qu.: 1.6424 3rd Qu.: 0.0556663 3rd Qu.:1.3203 ## Max. : 3.0236 Max. : 1.8877 Max. : 0.3810277 Max. :4.3043 ## p u q ## Min. :0.000 Min. :-345.287 Min. :0.000000 ## 1st Qu.:1.516 1st Qu.: -3.030 1st Qu.:0.001435 ## Median :1.916 Median : 0.000 Median :0.030121 ## Mean :1.797 Mean : -1.869 Mean :0.157978 ## 3rd Qu.:2.336 3rd Qu.: 1.559 3rd Qu.:0.161379 ## Max. :5.513 Max. : 22.434 Max. :1.000000 summary(df_choice_smooth$q - df_choice_smooth_delta$q) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -9.437e-16 -6.939e-18 0.000e+00 -1.050e-19 1.843e-18 9.992e-16 Write a function compute_share_delta(X, M, V, delta, sigma, mu, omega) that first construct df from X, M, V, second call compute_choice_delta to obtain a data frame with u and q, third compute the share of each product at each market s and the log difference in the share from the outside option, \\(\\ln(s_{jt}/s_{0t})\\), denoted by y, and finally return the data frame that is summarized at the product-market level, dropped consumer-level variables, and added s and y. # compute share df_share_smooth_delta &lt;- compute_share_smooth_delta( X, M, V, delta, sigma, mu, omega ) df_share_smooth_delta ## # A tibble: 633 × 11 ## t j x_1 x_2 x_3 xi c p q s y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 0 0 199. 0.398 0 ## 2 1 1 1 0.975 -0.0324 -0.0156 0.604 1.58 260. 0.521 0.270 ## 3 1 4 1 -0.611 1.19 0.0418 1.30 2.35 40.8 0.0816 -1.58 ## 4 2 0 0 0 0 0 0 0 106. 0.211 0 ## 5 2 1 1 0.975 -0.0324 -0.0394 0.890 1.87 32.9 0.0658 -1.17 ## 6 2 3 1 1.15 1.64 0.110 2.29 3.40 6.78 0.0136 -2.75 ## 7 2 4 1 -0.611 1.19 0.0763 0.997 2.09 14.1 0.0282 -2.01 ## 8 2 6 1 0.780 1.56 -0.0253 1.15 2.13 17.4 0.0347 -1.81 ## 9 2 7 1 -1.24 0.149 0.0697 0.613 1.68 25.2 0.0504 -1.43 ## 10 2 8 1 -4.43 -3.98 0.0557 0.629 1.67 290. 0.580 1.01 ## # ℹ 623 more rows summary(df_share_smooth_delta) ## t j x_1 x_2 ## Min. : 1.00 Min. : 0.00 Min. :0.000 Min. :-4.4294 ## 1st Qu.: 23.00 1st Qu.: 2.00 1st Qu.:1.000 1st Qu.:-0.6108 ## Median : 48.00 Median : 4.00 Median :1.000 Median : 0.7797 ## Mean : 49.67 Mean : 4.49 Mean :0.842 Mean : 0.3015 ## 3rd Qu.: 77.00 3rd Qu.: 7.00 3rd Qu.:1.000 3rd Qu.: 1.4766 ## Max. :100.00 Max. :10.00 Max. :1.000 Max. : 3.0236 ## x_3 xi c p ## Min. :-3.9787 Min. :-0.2996949 Min. :0.0000 Min. :0.000 ## 1st Qu.: 0.0000 1st Qu.:-0.0527368 1st Qu.:0.5250 1st Qu.:1.516 ## Median : 1.1878 Median : 0.0000000 Median :0.8775 Median :1.916 ## Mean : 0.5034 Mean :-0.0005149 Mean :0.9507 Mean :1.797 ## 3rd Qu.: 1.6424 3rd Qu.: 0.0556663 3rd Qu.:1.3203 3rd Qu.:2.336 ## Max. : 1.8877 Max. : 0.3810277 Max. :4.3043 Max. :5.513 ## q s y ## Min. : 1.748 Min. :0.003497 Min. :-4.23971 ## 1st Qu.: 17.982 1st Qu.:0.035964 1st Qu.:-1.95098 ## Median : 40.241 Median :0.080483 Median :-1.22808 ## Mean : 78.989 Mean :0.157978 Mean :-1.15979 ## 3rd Qu.:123.006 3rd Qu.:0.246011 3rd Qu.:-0.03626 ## Max. :325.631 Max. :0.651262 Max. : 1.26578 summary(df_share_smooth$s - df_share_smooth_delta$s) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.665e-16 -1.388e-17 0.000e+00 5.549e-20 6.939e-18 2.220e-16 Write a function solve_delta(df_share_smooth, X, M, V, delta, sigma, mu, omega) that finds \\(\\delta_{jt}\\) that equates the actua share and the predicted share based on compute_share_smooth_delta by the fixed-point algorithm with an operator: \\[ T(\\delta_{jt}^{(r)}) = \\delta_{jt}^{(r)} + \\kappa \\cdot \\log\\left(\\frac{s_{jt}}{\\sigma_{jt}[\\delta^{(r)}]}\\right), \\] where \\(s_{jt}\\) is the actual share of product \\(j\\) in market \\(t\\) and \\(\\sigma_{jt}[\\delta^{(r)}]\\) is the predicted share of product \\(j\\) in market \\(t\\) given \\(\\delta^{(r)}\\). Multiplying \\(\\kappa\\) is for the numerical stability. I set the value at \\(\\kappa = 1\\). Adjust it if the algorithm did not work. Set the stopping criterion at \\(\\max_{jt}|\\delta_{jt}^{(r + 1)} - \\delta_{jt}^{(r)}| &lt; \\lambda\\). Set \\(\\lambda\\) at \\(10^{-6}\\). Make sure that \\(\\delta_{i0t}\\) is always set at zero while the iteration. Start the algorithm with the true \\(\\delta_{jt}\\) and check if the algorithm returns (almost) the same \\(\\delta_{jt}\\) when the actual and predicted smooth share are equated. kappa &lt;- 1 lambda &lt;- 1e-4 delta_new &lt;- solve_delta( df_share_smooth, X, M, V, delta, sigma, mu, omega, kappa, lambda ) head(delta_new) ## # A tibble: 6 × 3 ## t j delta ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 1 -0.115 ## 3 1 4 -3.46 ## 4 2 0 0 ## 5 2 1 -0.920 ## 6 2 3 -6.29 summary(delta_new$delta - delta$delta) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.776e-15 -2.220e-16 0.000e+00 -1.193e-17 0.000e+00 1.776e-15 Check how long it takes to compute the limit \\(\\delta\\) under the Monte Carlo shocks starting from the true \\(\\delta\\) to match with df_share_smooth. This is approximately the time to evaluate the objective function. delta_new &lt;- solve_delta( df_share_smooth, X, M, V_mcmc, delta, sigma, mu, omega, kappa, lambda ) saveRDS( delta_new, file = &quot;lecture/data/a4/delta_new.rds&quot; %&gt;% here::here() ) delta_new &lt;- readRDS( file = &quot;lecture/data/a4/delta_new.rds&quot; %&gt;% here::here() ) summary(delta_new$delta - delta$delta) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.43733 -0.28081 0.00000 -0.01762 0.22132 1.12425 We use the marginal cost \\(c_{jt}\\) as the excluded instrumental variable for \\(p_{jt}\\). Let \\(\\Psi\\) be the weighing matrix for the GMM estimator. For now, let it be the identity matrix. Write a function compute_theta_linear(df_share_smooth, delta, mu, omega, Psi) that returns the optimal linear parameters associated with the data and \\(\\delta\\). Notice that we only obtain \\(\\beta_0\\) in this way because \\(\\alpha_0\\) is directly computed from the non-linear parameters by \\(-\\exp(\\mu + \\omega^2/2)\\). The first order condition for \\(\\beta_0\\) is: \\[\\begin{equation} \\beta_0 = (X&#39;W \\Phi^{-1} W&#39;X)^{-1} X&#39; W \\Phi^{-1} W&#39; [\\delta - \\alpha_0 p], \\end{equation}\\] where \\[\\begin{equation} X = \\begin{pmatrix} x_{11}&#39;\\\\ \\vdots \\\\ x_{J_1 1}&#39;\\\\ \\vdots \\\\ x_{1T}&#39; \\\\ \\vdots \\\\ x_{J_T T} \\end{pmatrix} \\end{equation}\\] \\[\\begin{equation} W = \\begin{pmatrix} x_{11}&#39; &amp; c_{11}\\\\ \\vdots &amp; \\vdots \\\\ x_{J_1 1}&#39; &amp; c_{J_1 1}\\\\ \\vdots &amp; \\vdots \\\\ x_{1T}&#39; &amp; c_{1T}\\\\ \\vdots &amp; \\vdots \\\\ x_{J_T T} &amp; c_{J_T T} \\end{pmatrix}, \\end{equation}\\] \\[\\begin{equation} \\delta = \\begin{pmatrix} \\delta_11\\\\ \\vdots\\\\ \\delta_{J_1 1}\\\\ \\vdots\\\\ \\delta_1T\\\\ \\vdots\\\\ \\delta_{J_T T}\\\\ \\end{pmatrix} \\end{equation}\\], where \\(\\alpha_0 = - \\exp(\\mu + \\omega^2/2)\\). Notice that \\(X\\) and \\(W\\) does not include rows for the outwide option. Psi &lt;- diag(length(beta) + 1) theta_linear &lt;- compute_theta_linear( df_share_smooth, delta, mu, omega, Psi ) cbind( theta_linear, beta ) ## delta beta ## x_1 3.9946731 4.0000000 ## x_2 0.1747411 0.1836433 ## x_3 -0.8244048 -0.8356286 Write a function solve_xi(df_share_smooth, delta, beta, mu, omega) that computes the values of \\(\\xi\\) that are implied from the data, \\(\\delta\\), and the linear parameters. Check that the (almost) true values are returned when true \\(\\delta\\) and the true linear parmaeters are passed to the function. Notice that the returend \\(\\xi\\) should not include rows for the outside option. xi_new &lt;- solve_xi( df_share_smooth, delta, beta, mu, omega ) head(xi_new) ## xi ## [1,] -0.01557955 ## [2,] 0.04179416 ## [3,] -0.03942900 ## [4,] 0.11000254 ## [5,] 0.07631757 ## [6,] -0.02533617 xi_true &lt;- df_share_smooth %&gt;% dplyr::filter(j != 0) %&gt;% dplyr::select(xi) summary(xi_true - xi_new) ## xi ## Min. :-4.233e-16 ## 1st Qu.:-5.551e-17 ## Median : 0.000e+00 ## Mean : 1.347e-17 ## 3rd Qu.: 8.327e-17 ## Max. : 8.604e-16 Write a function compute_gmm_objective_a4(theta_nonlinear, delta, df_share_smooth, Psi, X, M, V_mcmc, kappa, lambda) that returns the value of the GMM objective function as a function of non-linear parameters mu, omega, and sigma: \\[ \\min_{\\theta} \\xi(\\theta)&#39; W \\Phi^{-1} W&#39; \\xi(\\theta), \\] where \\(\\xi(\\theta)\\) is the values of \\(\\xi\\) that solves: \\[ s = \\sigma(p, x, \\xi), \\] given parameters \\(\\theta\\). Note that the row of \\(\\xi(\\theta)\\) and \\(W\\) do not include the rows for the outside options. # non-linear parmaeters theta_nonlinear &lt;- c( mu, omega, sigma ) # compute GMM objective function objective &lt;- compute_gmm_objective_a4( theta_nonlinear, delta, df_share_smooth, Psi, X, M, V_mcmc, kappa, lambda ) saveRDS( objective, file = &quot;lecture/data/a4/objective.rds&quot; %&gt;% here::here() ) objective &lt;- readRDS( file = &quot;lecture/data/a4/objective.rds&quot; %&gt;% here::here() ) objective ## xi ## xi 16.37845 Draw a graph of the objective function that varies each non-linear parameter from 0, 0.2, \\(\\cdots\\), 2.0 of the true value. Try with the actual shocks V. ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] Find non-linear parameters that minimize the GMM objective function. Because standard deviations of the same absolute value with positive and negative values have almost the same implication for the data, you can take the absolute value if the estimates of the standard deviations happened to be negative (Another way is to set the non-negativity constraints on the standard deviations). ## $par ## [1] 0.4928139 1.0228455 1.5913062 0.3913233 0.8707353 ## ## $value ## [1] 7.353949e-08 ## ## $counts ## function gradient ## 140 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## true estimate ## [1,] 0.5000000 0.4928139 ## [2,] 1.0000000 1.0228455 ## [3,] 1.5952808 1.5913062 ## [4,] 0.3295078 0.3913233 ## [5,] 0.8204684 0.8707353 "],["assignment5.html", "Chapter 15 Assignment 5: Merger Simulation 15.1 Simulate data 15.2 Estimate the parameters 15.3 Conduct counterfactual simulation", " Chapter 15 Assignment 5: Merger Simulation 15.1 Simulate data We simulate data from a discrete choice model that is the same with in assignment 4 except for that the price is derived from the Nash equlibrium. There are \\(T\\) markets and each market has \\(N\\) consumers. There are \\(J\\) products and the indirect utility of consumer \\(i\\) in market \\(t\\) for product \\(j\\) is: \\[ u_{itj} = \\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt} + \\epsilon_{ijt}, \\] where \\(\\epsilon_{ijt}\\) is an i.i.d. type-I extreme random variable. \\(x_j\\) is \\(K\\)-dimensional observed characteristics of the product. \\(p_{jt}\\) is the retail price of the product in the market. \\(\\xi_{jt}\\) is product-market specific fixed effect. \\(p_{jt}\\) can be correlated with \\(\\xi_{jt}\\) but \\(x_{jt}\\)s are independent of \\(\\xi_{jt}\\). \\(j = 0\\) is an outside option whose indirect utility is: \\[ u_{it0} = \\epsilon_{i0t}, \\] where \\(\\epsilon_{i0t}\\) is an i.i.d. type-I extreme random variable. \\(\\beta_{it}\\) and \\(\\alpha_{it}\\) are different across consumers, and they are distributed as: \\[ \\beta_{itk} = \\beta_{0k} + \\sigma_k \\nu_{itk}, \\] \\[ \\alpha_{it} = - \\exp(\\mu + \\omega \\upsilon_{it}) = - \\exp(\\mu + \\frac{\\omega^2}{2}) + [- \\exp(\\mu + \\omega \\upsilon_{it}) + \\exp(\\mu + \\frac{\\omega^2}{2})] \\equiv \\alpha_0 + \\tilde{\\alpha}_{it}, \\] where \\(\\nu_{itk}\\) for \\(k = 1, \\cdots, K\\) and \\(\\upsilon_{it}\\) are i.i.d. standard normal random variables. \\(\\alpha_0\\) is the mean of \\(\\alpha_i\\) and \\(\\tilde{\\alpha}_i\\) is the deviation from the mean. Given a choice set in the market, \\(\\mathcal{J}_t \\cup \\{0\\}\\), a consumer chooses the alternative that maximizes her utility: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] The choice probability of product \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[ \\sigma_{ijt}(p_t, x_t, \\xi_t) = \\mathbb{P}\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] Suppose that we only observe the (smooth) share data: \\[ s_{jt}(p_t, x_t, \\xi_t) = \\frac{1}{N} \\sum_{i = 1}^N \\sigma_{ijt}(p_t, x_t, \\xi_t) = \\frac{1}{N} \\sum_{i = 1}^N \\frac{\\exp(u_{ijt})}{1 + \\sum_{k \\in \\mathcal{J}_t \\cup \\{0\\}} \\exp(u_{ikt})}. \\] along with the product-market characteristics \\(x_{jt}\\) and the retail prices \\(p_{jt}\\) for \\(j \\in \\mathcal{J}_t \\cup \\{0\\}\\) for \\(t = 1, \\cdots, T\\). We do not observe the choice data \\(q_{ijt}\\) nor shocks \\(\\xi_{jt}, \\nu_{it}, \\upsilon_{it}, \\epsilon_{ijt}\\). We draw \\(\\xi_{jt}\\) from i.i.d. normal distribution with mean 0 and standard deviation \\(\\sigma_{\\xi}\\). Set the seed, constants, and parameters of interest as follows. # set the seed set.seed(1) # number of products J &lt;- 10 # dimension of product characteristics including the intercept K &lt;- 3 # number of markets T &lt;- 100 # number of consumers per market N &lt;- 500 # number of Monte Carlo L &lt;- 500 # set parameters of interests beta &lt;- rnorm(K); beta[1] &lt;- 4 beta ## [1] 4.0000000 0.1836433 -0.8356286 sigma &lt;- abs(rnorm(K)); sigma ## [1] 1.5952808 0.3295078 0.8204684 mu &lt;- 0.5 omega &lt;- 1 Generate the covariates as follows. The product-market characteristics: \\[ x_{j1} = 1, x_{jk} \\sim N(0, \\sigma_x), k = 2, \\cdots, K, \\] where \\(\\sigma_x\\) is referred to as sd_x in the code. The product-market-specific unobserved fixed effect: \\[ \\xi_{jt} \\sim N(0, \\sigma_\\xi), \\] where \\(\\sigma_xi\\) is referred to as sd_xi in the code. The marginal cost of product \\(j\\) in market \\(t\\): \\[ c_{jt} \\sim \\text{logNormal}(0, \\sigma_c), \\] where \\(\\sigma_c\\) is referred to as sd_c in the code. The price is determined by a Nash equilibrium. Let \\(\\Delta_t\\) be the \\(J_t \\times J_t\\) ownership matrix in which the \\((j, k)\\)-th element \\(\\delta_{tjk}\\) is equal to 1 if product \\(j\\) and \\(k\\) are owned by the same firm and 0 otherwise. Assume that \\(\\delta_{tjk} = 1\\) if and only if \\(j = k\\) for all \\(t = 1, \\cdots, T\\), i.e., each firm owns only one product. Next, define \\(\\Omega_t\\) be \\(J_t \\times J_t\\) matrix such that whose \\((j, k)\\)-the element \\(\\omega_{tjk}(p_t, x_t, \\xi_t, \\Delta_t)\\) is: \\[ \\omega_{tjk}(p_t, x_t, \\xi_t, \\Delta_t) = - \\frac{\\partial s_{jt}(p_t, x_t, \\xi_t)}{\\partial p_{kt}} \\delta_{tjk}. \\] Then, the equilibrium price vector \\(p_t\\) is determined by solving the following equilibrium condition: \\[ p_t = c_t + \\Omega_t(p_t, x_t, \\xi_t, \\Delta_t)^{-1} s_t(p_t, x_t, \\xi_t). \\] The value of the auxiliary parameters are set as follows: # set auxiliary parameters price_xi &lt;- 1 sd_x &lt;- 2 sd_xi &lt;- 0.5 sd_c &lt;- 0.05 sd_p &lt;- 0.05 X is the data frame such that a row contains the characteristics vector \\(x_{j}\\) of a product and columns are product index and observed product characteristics. The dimension of the characteristics \\(K\\) is specified above. Add the row of the outside option whose index is \\(0\\) and all the characteristics are zero. # make product characteristics data X &lt;- matrix( sd_x * rnorm(J * (K - 1)), nrow = J ) X &lt;- cbind( rep(1, J), X ) colnames(X) &lt;- paste(&quot;x&quot;, 1:K, sep = &quot;_&quot;) X &lt;- data.frame(j = 1:J, X) %&gt;% tibble::as_tibble() # add outside option X &lt;- rbind( rep(0, dim(X)[2]), X ) X ## # A tibble: 11 × 4 ## j x_1 x_2 x_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 1 1 0.975 -0.0324 ## 3 2 1 1.48 1.89 ## 4 3 1 1.15 1.64 ## 5 4 1 -0.611 1.19 ## 6 5 1 3.02 1.84 ## 7 6 1 0.780 1.56 ## 8 7 1 -1.24 0.149 ## 9 8 1 -4.43 -3.98 ## 10 9 1 2.25 1.24 ## 11 10 1 -0.0899 -0.112 M is the data frame such that a row contains the price \\(\\xi_{jt}\\), marginal cost \\(c_{jt}\\), and price \\(p_{jt}\\). For now, set \\(p_{jt} = 0\\) and fill the equilibrium price later. After generating the variables, drop some products in each market. In order to change the number of available products in each market, for each market, first draw \\(J_t\\) from a discrete uniform distribution between \\(1\\) and \\(J\\). Then, drop products from each market using dplyr::sample_frac function with the realized number of available products. The variation in the available products is important for the identification of the distribution of consumer-level unobserved heterogeneity. Add the row of the outside option to each market whose index is \\(0\\) and all the variables take value zero. # make market-product data M &lt;- expand.grid( j = 1:J, t = 1:T ) %&gt;% tibble::as_tibble() %&gt;% dplyr::mutate( xi = sd_xi * rnorm(J*T), c = exp(sd_c * rnorm(J*T)), p = 0 ) M &lt;- M %&gt;% dplyr::group_by(t) %&gt;% dplyr::sample_frac(size = purrr::rdunif(1, J)/J) %&gt;% dplyr::ungroup() # add outside option outside &lt;- data.frame( j = 0, t = 1:T, xi = 0, c = 0, p = 0 ) M &lt;- rbind( M, outside ) %&gt;% dplyr::arrange( t, j ) M ## # A tibble: 696 × 5 ## j t xi c p ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 2 1 -0.735 1.04 0 ## 3 6 1 -0.0514 0.980 0 ## 4 7 1 0.194 0.961 0 ## 5 8 1 -0.0269 0.989 0 ## 6 0 2 0 0 0 ## 7 1 2 -0.197 0.988 0 ## 8 2 2 -0.0297 1.04 0 ## 9 4 2 0.382 1.00 0 ## 10 5 2 -0.0823 1.02 0 ## # ℹ 686 more rows Generate the consumer-level heterogeneity. V is the data frame such that a row contains the vector of shocks to consumer-level heterogeneity, \\((\\nu_{i}&#39;, \\upsilon_i)\\). They are all i.i.d. standard normal random variables. # make consumer-market data V &lt;- matrix( rnorm(N * T * (K + 1)), nrow = N * T ) colnames(V) &lt;- c( paste(&quot;v_x&quot;, 1:K, sep = &quot;_&quot;), &quot;v_p&quot; ) V &lt;- data.frame( expand.grid( i = 1:N, t = 1:T ), V ) %&gt;% tibble::as_tibble() V ## # A tibble: 50,000 × 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.448 0.985 0.611 0.408 ## 2 2 1 -0.386 -0.389 -1.11 -0.667 ## 3 3 1 0.0567 0.0510 0.0329 -0.119 ## 4 4 1 0.585 0.303 0.860 -1.20 ## 5 5 1 -0.449 -1.17 0.599 0.212 ## 6 6 1 -0.782 0.0596 1.30 0.485 ## 7 7 1 1.60 -1.62 -1.91 0.669 ## 8 8 1 -1.65 2.10 0.726 -0.108 ## 9 9 1 -0.848 -0.933 2.29 -0.0195 ## 10 10 1 -0.130 -0.897 -1.90 -0.185 ## # ℹ 49,990 more rows We use compute_indirect_utility(df, beta, sigma, mu, omega), compute_choice_smooth(X, M, V, beta, sigma, mu, omega), and compute_share_smooth(X, M, V, beta, sigma, mu, omega) to compute \\(s_t(p_t, x_t, \\xi_t)\\). On top of this, we need a function compute_derivative_share_smooth(X, M, V, beta, sigma, mu, omega) that approximate: \\[ \\frac{\\partial s_{jt}(p_t, x_t, \\xi_t)}{\\partial p_{kt}} = \\begin{cases} \\frac{1}{N} \\sum_{i = 1}^N \\alpha_i \\sigma_{ijt}(p_t, x_t, \\xi_t)[1 - \\sigma_{ijt}(p_t, x_t, \\xi_t)] &amp;\\text{ if } j = k\\\\ - \\frac{1}{N}\\sum_{i = 1}^N \\alpha_i \\sigma_{ijt}(p_t, x_t, \\xi_t)\\sigma_{ikt}(p_t, x_t, \\xi_t)] &amp;\\text{ if } j \\neq k. \\end{cases} \\] The returned object should be a list across markets and each element of the list should be \\(J_t \\times J_t\\) matrix whose \\((j, k)\\)-th element is \\(\\partial s_{jt}/\\partial p_{it}\\) (do not include the outside option). The computation will be looped across markets. I recommend to use a parallel computing for this loop. derivative_share_smooth &lt;- compute_derivative_share_smooth( X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) derivative_share_smooth[[1]] ## [,1] [,2] [,3] [,4] ## [1,] -0.15195456 0.07243405 0.04067066 0.03615775 ## [2,] 0.07243405 -0.21334454 0.06758143 0.06900886 ## [3,] 0.04067066 0.06758143 -0.22465048 0.11247770 ## [4,] 0.03615775 0.06900886 0.11247770 -0.22508246 derivative_share_smooth[[T]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.096590746 0.003580709 0.003713348 0.011204782 0.012198589 ## [2,] 0.003580709 -0.054948229 0.003309376 0.009438704 0.009893682 ## [3,] 0.003713348 0.003309376 -0.055735044 0.009505457 0.009233146 ## [4,] 0.011204782 0.009438704 0.009505457 -0.160285167 0.021922796 ## [5,] 0.012198589 0.009893682 0.009233146 0.021922796 -0.136324417 ## [6,] 0.008403911 0.007465135 0.007249620 0.022645270 0.019978806 ## [7,] 0.006573094 0.003856270 0.004084570 0.016649978 0.009219389 ## [8,] 0.033096170 0.006496123 0.007885498 0.038995016 0.016868502 ## [9,] 0.013495765 0.008866219 0.008606532 0.022541027 0.030984223 ## [10,] 0.003846416 0.001815922 0.001912074 0.006631000 0.005317866 ## [,6] [,7] [,8] [,9] [,10] ## [1,] 0.008403911 0.006573094 0.033096170 0.013495765 0.003846416 ## [2,] 0.007465135 0.003856270 0.006496123 0.008866219 0.001815922 ## [3,] 0.007249620 0.004084570 0.007885498 0.008606532 0.001912074 ## [4,] 0.022645270 0.016649978 0.038995016 0.022541027 0.006631000 ## [5,] 0.019978806 0.009219389 0.016868502 0.030984223 0.005317866 ## [6,] -0.119320239 0.009873958 0.019723897 0.018980139 0.004460069 ## [7,] 0.009873958 -0.101062317 0.035849014 0.010662149 0.003862923 ## [8,] 0.019723897 0.035849014 -0.207076797 0.027381997 0.018831945 ## [9,] 0.018980139 0.010662149 0.027381997 -0.148409509 0.006094688 ## [10,] 0.004460069 0.003862923 0.018831945 0.006094688 -0.053007318 Make a list delta such that each element of the list is \\(J_t \\times J_t\\) matrix \\(\\delta_t\\). delta[[1]] ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 delta[[T]] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 0 0 0 0 0 0 0 0 0 ## [2,] 0 1 0 0 0 0 0 0 0 0 ## [3,] 0 0 1 0 0 0 0 0 0 0 ## [4,] 0 0 0 1 0 0 0 0 0 0 ## [5,] 0 0 0 0 1 0 0 0 0 0 ## [6,] 0 0 0 0 0 1 0 0 0 0 ## [7,] 0 0 0 0 0 0 1 0 0 0 ## [8,] 0 0 0 0 0 0 0 1 0 0 ## [9,] 0 0 0 0 0 0 0 0 1 0 ## [10,] 0 0 0 0 0 0 0 0 0 1 Write a function update_price(logp, X, M, V, beta, sigma, mu, omega, delta) that receives a price vector \\(p_t^{(r)}\\) and returns \\(p_t^{(r + 1)}\\) by: \\[ p_t^{(r + 1)} = c_t + \\Omega_t(p_t^{(r)}, x_t, \\xi_t, \\Delta_t)^{-1} s_t(p_t^{(r)}, x_t, \\xi_t). \\] The returned object should be a vector whose row represents the condition for an inside product of each market. To impose non-negativity constraint on the price vector, we pass log price and exponentiate inside the function. Iterate this until \\(\\max_{jt}|p_{jt}^{(r + 1)} - p_{jt}^{(r)}| &lt; \\lambda\\), for example with \\(\\lambda = 10^{-6}\\). This iteration may or may not converge. The convergence depends on the parameters and the realization of the shocks. If the algorithm does not converge, first check the code. # set the threshold lambda &lt;- 1e-6 # set the initial price p &lt;- M[M$j &gt; 0, &quot;p&quot;] logp &lt;- log(rep(1, dim(p)[1])) p_new &lt;- update_price( logp = logp, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega, delta = delta ) # iterate distance &lt;- 10000 while (distance &gt; lambda) { p_old &lt;- p_new p_new &lt;- update_price( log(p_old), X, M, V, beta, sigma, mu, omega, delta ) distance &lt;- max(abs(p_new - p_old)) print(distance) } # save p_actual &lt;- p_new saveRDS( p_actual, file = &quot;lecture/data/a5/price_actual.rds&quot; %&gt;% here::here() ) # load p_actual &lt;- readRDS( file = &quot;lecture/data/a5/price_actual.rds&quot; %&gt;% here::here() ) p_actual %&gt;% head() ## [,1] ## [1,] 2.009611 ## [2,] 2.024696 ## [3,] 2.099574 ## [4,] 6.374277 ## [5,] 1.664490 ## [6,] 1.808518 15.2 Estimate the parameters Write a function estimate_marginal_cost() that estimate \\(c_t\\) by the equilibrium condition as: \\[ c_t = p_t - \\Omega_t(p_t, x_t, \\xi_t, \\Delta_t)^{-1} s_t(p_t, x_t, \\xi_t) \\] Of course, in reality, we first draw Monte Carlo shocks to approximate the share, estimate the demand parameters, and use these shocks and estimates to estimate the marginal costs. In this assignment, we check the if the estimated marginal costs coincide with the true marginal costs to confirm that the codes are correctly written. # take the logarithm logp &lt;- log(p_actual) # estimate the marginal cost marginal_cost_estimate &lt;- estimate_marginal_cost( logp = logp, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega, delta = delta) marginal_cost_actual &lt;- M[M$j &gt; 0, ]$c # plot the estimate vs actual marginal costs marginal_cost_df &lt;- data.frame( actual = marginal_cost_actual, estimate = marginal_cost_estimate ) ggplot( marginal_cost_df, aes( x = estimate, y = actual ) ) + geom_point() + theme_classic() (Optional) Translate compute_indirect_utility, compute_choice_smooth, compute_derivative_share_smooth, update_price into C++ using Rcpp and Eigen. Check that the outputs coincide at the machine precision level. I give you extra 2 points for this task on top of the usual 10 points for this assignment. 15.3 Conduct counterfactual simulation Suppose that the firm of product 1 owner purchase the firms that own product 2 and 3. Let delta_counterfactual be the relevant ownership matrix. Make delta_counterfactual. delta_counterfactual[[1]] ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 delta_counterfactual[[T]] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 1 1 0 0 0 0 0 0 0 ## [2,] 1 1 1 0 0 0 0 0 0 0 ## [3,] 1 1 1 0 0 0 0 0 0 0 ## [4,] 0 0 0 1 0 0 0 0 0 0 ## [5,] 0 0 0 0 1 0 0 0 0 0 ## [6,] 0 0 0 0 0 1 0 0 0 0 ## [7,] 0 0 0 0 0 0 1 0 0 0 ## [8,] 0 0 0 0 0 0 0 1 0 0 ## [9,] 0 0 0 0 0 0 0 0 1 0 ## [10,] 0 0 0 0 0 0 0 0 0 1 Compute the counterfactual price using the iteration with update_price. You can start the iteration from the equilibrium price. Show the average percentage change in the price for each product. In theory, the price of any product should not drop. But some prices can slightly drop because of the numerical errors. logp &lt;- log(p_actual) p_new &lt;- update_price( logp = logp, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega, delta = delta_counterfactual ) distance &lt;- 10000 while (distance &gt; lambda) { p_old &lt;- p_new p_new &lt;- update_price( log(p_old), X, M, V, beta, sigma, mu, omega, delta_counterfactual ) distance &lt;- max(abs(p_new - p_old)) print(distance) } p_counterfactual &lt;- p_new saveRDS( p_counterfactual, file = &quot;lecture/data/a5/price_counterfactual.rds&quot; %&gt;% here::here() ) j p_change 1 0.0501744 2 0.1179955 3 0.1479700 4 0.0013059 5 0.0004309 6 -0.0002448 7 0.0008458 8 -0.0033448 9 0.0004042 10 0.0015040 Write a function compute_producer_surplus(p, marginal_cost, X, M, V, beta, sigma, mu, omega) that returns the producer surplus for each product in each market. Compute the actual and counterfactual producer surplus under the estimated marginal costs. Show the average percentage change in the producer surplus for each product. # compute actual producer surplus producer_surplus_actual &lt;- compute_producer_surplus( p = p_actual, marginal_cost = marginal_cost_estimate, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) summary(producer_surplus_actual) ## s ## Min. :0.008247 ## 1st Qu.:0.041636 ## Median :0.071153 ## Mean :0.168761 ## 3rd Qu.:0.150643 ## Max. :1.607658 # compute counterfactual producer surplus producer_surplus_counterfactual &lt;- compute_producer_surplus( p = p_counterfactual, marginal_cost = marginal_cost_estimate, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) summary(producer_surplus_counterfactual) ## s ## Min. :0.008212 ## 1st Qu.:0.042687 ## Median :0.073644 ## Mean :0.171809 ## 3rd Qu.:0.153746 ## Max. :1.607658 j producer_surplus_change 1 0.0231358 2 0.0131566 3 0.0073846 4 0.0534174 5 0.0449804 6 0.0563809 7 0.0372859 8 0.0072057 9 0.0421801 10 0.0385013 Write a function compute_consumer_surplus(p, X, M, V, beta, sigma, mu, omega) that returns the consumer surplus for each consumer in each market. Compute the actual and counterfactual consumer surplus under the estimated marginal costs. Show the percentage change in the total consumer surplus. # compute actual consumer surplus consumer_surplus_actual &lt;- compute_consumer_surplus( p = p_actual, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) summary(consumer_surplus_actual) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00000 0.03447 1.10805 4.33269 4.65720 230.98190 # compute counterfactual consumer surplus consumer_surplus_counterfactual &lt;- compute_consumer_surplus( p = p_counterfactual, X = X, M = M, V = V, beta = beta, sigma = sigma, mu = mu, omega = omega ) summary(consumer_surplus_counterfactual) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00000 0.03054 1.07265 4.31048 4.61989 231.02355 consumer_surplus_change &lt;- (sum(consumer_surplus_counterfactual) - sum(consumer_surplus_actual)) / sum(consumer_surplus_actual) consumer_surplus_change ## [1] -0.005127025 "],["assignment6.html", "Chapter 16 Assignment 6: Entry and Exit Analysis 16.1 Simulate data 16.2 Estimate the parameters 16.3 Conduct counterfactual simulations", " Chapter 16 Assignment 6: Entry and Exit Analysis 16.1 Simulate data In this assignment, we consider a Berry-type entry model. Suppose that there are \\(M\\) markets indexed by \\(m = 1, \\cdots, M\\). In each market, there are \\(N_m\\) potential entrants such that \\(N_m \\le \\overline{N}\\). Let \\(x_m\\) be the \\(K\\) dimensional market attributes and \\(z_{im}\\) be the \\(L\\) dimensional potential entrant attributes. The size of Monte Carlo simulations in the estimation is \\(R\\). A random number generation inside %dopar% is not reproducible. Therefore, we use package doRNG to perform reproducible foreach parallel. Set the constants as follows: registerDoParallel() # register doRNG backend registerDoRNG() # set the seed set.seed(1) # number of markets M &lt;- 100 # the upper bound of the number of potential entrants N &lt;- 10 # the dimension of market attributes K &lt;- 2 # the dimension of potential entrant attributes L &lt;- 2 # the number of Monte Carlo simulations R &lt;- 100 The payoff of entrant \\(i\\) in market \\(m\\) is: \\[ \\pi_{im}(y_m) = x_m&#39;\\beta - \\delta \\ln \\left(\\sum_{i = 1}^{N_m} y_{im}\\right) + z_{im}&#39;\\alpha + \\sqrt{1 - \\rho^2} \\nu_{im} + \\rho \\epsilon_{m}, \\] where \\(y_{im} \\in \\{0, 1\\}\\) is the indicator for entrant \\(i\\) in market \\(m\\) to enter the market, and \\(\\nu_{im}\\) and \\(\\epsilon_m\\) are entrant- and market-specific idiosyncratic shocks that are drawn from an i.i.d. standard normal distribution. In each market, all the attributes and idiosyncratic shocks are observed by the potential entrants. \\(N_m\\), \\(x_m\\), \\(z_{im}\\), and \\(y_m\\) are observed to econometrician but \\(\\nu_{im}\\) and \\(\\epsilon_m\\) are not. Set the parameters as follows: # parameters of interest beta &lt;- abs(rnorm(K)); beta ## [1] 0.6264538 0.1836433 alpha &lt;- abs(rnorm(L)); alpha ## [1] 0.8356286 1.5952808 delta &lt;- 1; delta ## [1] 1 rho &lt;- abs(rnorm(1)); rho ## [1] 0.3295078 # auxiliary parameters x_mu &lt;- 1 x_sd &lt;- 3 z_mu &lt;- 0 z_sd &lt;- 4 Draw exogenous variables as follows: # number of potential entrants E &lt;- purrr::rdunif( M, 1, N ); E ## [1] 3 1 5 5 10 6 10 7 9 5 5 9 9 5 5 2 10 9 1 4 3 6 10 10 6 ## [26] 4 4 10 9 7 6 9 8 9 7 8 6 10 7 3 10 6 8 2 2 6 6 1 3 3 ## [51] 8 6 7 6 8 7 1 4 8 9 9 7 4 7 6 1 5 6 1 9 7 7 3 6 2 ## [76] 10 10 7 3 2 10 1 10 10 8 10 5 7 8 5 6 8 1 3 10 3 1 6 6 4 # market attributes X &lt;- matrix( rnorm( M * K, x_mu, x_sd ), nrow = M ) colnames(X) &lt;- paste(&quot;x&quot;, 1:K, sep = &quot;_&quot;) X[1:10, ] ## x_1 x_2 ## [1,] -0.70600620 -2.6939703 ## [2,] 0.59446415 3.9516867 ## [3,] 4.53426099 1.6597744 ## [4,] -3.57070040 -3.4017501 ## [5,] 2.78183856 2.5630682 ## [6,] 1.99885111 0.5237362 ## [7,] 4.18929951 5.3937619 ## [8,] 0.08744823 -1.2982460 ## [9,] 2.11005643 -0.2906353 ## [10,] 1.80129637 -1.7783285 # entrant attributes Z &lt;- foreach ( m = 1:M ) %dorng% { Z_m &lt;- matrix( rnorm( E[m] * L, z_mu, z_sd ), nrow = E[m] ) colnames(Z_m) &lt;- paste(&quot;z&quot;, 1:L, sep = &quot;_&quot;) return(Z_m) } Z[[1]] ## z_1 z_2 ## [1,] -2.3581574 -7.334507 ## [2,] -0.4604547 3.771050 ## [3,] 2.7599294 4.610126 # unobserved market attributes EP &lt;- matrix( rnorm(M), nrow = M ) EP[1:10, ] ## [1] -1.1131230 0.6169665 0.5134937 0.3694591 1.7238941 -0.2061446 ## [7] -1.3141951 0.0634741 -0.2319775 0.6350603 # unobserved entrant attributes NU &lt;- foreach ( m = 1:M ) %dorng% { NU_m &lt;- matrix( rnorm(E[m]), nrow = E[m] ) return(NU_m) } NU[[1]] ## [,1] ## [1,] 0.3934210 ## [2,] 0.2303175 ## [3,] -0.6046126 Write a function compute_payoff(y_m, X_m, Z_m, EP_m, NU_m, beta, alpha, delta, rho) that returns the vector of payoffs of the potential entrants when the vector of entry decisions is y_m. m &lt;- 1 N_m &lt;- dim(Z[[m]])[1] y_m &lt;- as.matrix(rep(1, N_m)) y_m[length(y_m)] &lt;- 0 X_m &lt;- X[m, , drop = FALSE] Z_m &lt;- Z[[m]] EP_m &lt;- EP[m, , drop = FALSE] NU_m &lt;- NU[[m]] compute_payoff( y_m = y_m, X_m = X_m, Z_m = Z_m, EP_m = EP_m, NU_m = NU_m, beta = beta, alpha = alpha, delta = delta, rho = rho ) ## [,1] ## [1,] -15.296633 ## [2,] 3.851629 ## [3,] 0.000000 Assume that the order of entry is predetermined. Assume that the potential entrants sequentially decide entry according to the order of the payoff excluding the competitive effects, i.e.: \\[ x_m&#39;\\beta + z_{im}&#39;\\alpha + \\sqrt{1 - \\rho^2} \\nu_{im} + \\rho \\epsilon_{m}. \\] Write a function compute_order_sequential_entry(X_m, Z_m, EP_m, NU_m, beta, alpha, rho) that returns the order of entry of the potential entrants by the baseline payoff. Note that if the less profitable entrant finds it profitable to enter then the more profitable entrant still finds it profitable to enter. Then write a function compute_sequential_entry(X_m, Z_m, EP_m, NU_m, beta, alpha, delta, rho) that returns the equilibrium vector of entry at a market. To do so, you must find at which entrant the payoff becomes negative for the first time. compute_order_sequential_entry( X_m = X_m, Z_m = Z_m, EP_m = EP_m, NU_m = NU_m, beta = beta, alpha = alpha, rho = rho ) ## [1] 3 2 1 compute_sequential_entry( X_m = X_m, Z_m = Z_m, EP_m = EP_m, NU_m = NU_m, beta = beta, alpha = alpha, delta = delta, rho = rho ) ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 Next, assume \\(\\rho = 0\\). Assume that potential entrants simultaneously decide entry. Write a function compute_best_response_simultaneous_entry that returns the best response function of the potential participant given an entry decision. Then, write a function compute_simultaneous_entry(X_m, Z_m, EP_m, NU_m, beta, alpha, delta) that returns the equilibrium vector of entry at a market, given an initial entry decision where all firms decide to enter. To do so, you need to compute the best response given other firm’s entry decisions, and iterate the best response mapping until convergence. compute_best_response_simultaneous_entry( y_m = y_m, X_m = X_m, Z_m = Z_m, EP_m = EP_m, NU_m = NU_m, beta = beta, alpha = alpha, delta = delta ) ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 compute_simultaneous_entry( X_m = X_m, Z_m = Z_m, EP_m = EP_m, NU_m = NU_m, beta = beta, alpha = alpha, delta = delta ) ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 Write a function compute_sequential_entry_across_markets(X, Z, EP, NU, beta, alpha, delta, rho) compute the equilibrium entry vectors under the assumption of sequential entry. The output should be a list of entry vectors across markets. Write a function to compute the equilibrium payoffs across markets, compute_payoff_across_markets(Y, X, Z, EP, NU, beta, alpha, delta, rho) and check that the payoffs under the equilibrium entry vectors are non-negative. Otherwise, there are some bugs in the code. Y_sequential &lt;- compute_sequential_entry_across_markets( X = X, Z = Z, EP = EP, NU = NU, beta = beta, alpha = alpha, delta = delta, rho = rho ) Y_sequential[[1]] ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 Y_sequential[[M]] ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 ## [4,] 0 payoff_sequential &lt;- compute_payoff_across_markets( Y = Y_sequential, X = X, Z = Z, EP = EP, NU = NU, beta = beta, alpha = alpha, delta = delta, rho = rho ) min(unlist(payoff_sequential)) ## [1] 0 Write a function compute_simultaneous_entry_across_markets(X, Z, EP, NU, beta, alpha, delta, rho = 0) compute the equilibrium entry vectors under the assumption of simultaneous entry. The output should be a list of entry vectors across markets. Check that the payoffs under the equilibrium entry vectors are non-negative. Otherwise, there are some bugs in the code. I also recommend to write this function with # compute simultaneous entry across markets Y_simultaneous &lt;- compute_simultaneous_entry_across_markets( X = X, Z = Z, EP = EP, NU = NU, beta = beta, alpha = alpha, delta = delta) Y_simultaneous[[1]] ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 Y_simultaneous[[M]] ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 ## [4,] 0 payoff_simultaneous &lt;- compute_payoff_across_markets( Y = Y_simultaneous, X = X, Z = Z, EP = EP, NU = NU, beta = beta, alpha = alpha, delta = delta, rho = 0 ) min(unlist(payoff_simultaneous)) ## [1] 0 16.2 Estimate the parameters We estimate the parameters by matching the actual and predicted number of entrants in each market. To do so, we simulate the model for \\(R\\) times. Under the assumption of the sequential entry, we can uniquely predict the equilibrium identify of the entrants. So, we consider the following objective function: \\[ \\frac{1}{RM}\\sum_{r = 1}^R \\sum_{m = 1}^M \\left[\\sum_{i = 1}^{N_m}|y_{im} - y_{im}^{(r)}| \\right]^2, \\] where \\(y_{im}^{(r)}\\) is the entry decision in \\(r\\)-th simulation. On the other hand, under the assumption of the simultaneous entry, we can only uniquely predict the equilibrium number of the entrants. So, we consider the following objective function: \\[ \\frac{1}{RM}\\sum_{r = 1}^R \\sum_{m = 1}^M \\left[\\sum_{i = 1}^{N_m}(y_{im} - y_{im}^{(r)}) \\right]^2, \\] Draw \\(R\\) unobserved shocks: set.seed(1) # unobserved market attributes EP_mc &lt;- foreach ( r = 1:R ) %dorng% { EP &lt;- matrix( rnorm(M), nrow = M ) return(EP) } head(EP_mc[[1]]) ## [,1] ## [1,] 0.74570967 ## [2,] -0.07155828 ## [3,] -0.03724045 ## [4,] 0.69069075 ## [5,] -0.71328829 ## [6,] 0.01630815 # unobserved entrant attributes NU_mc &lt;- foreach ( r = 1:R ) %dorng% { NU &lt;- foreach ( m = 1:M ) %do% { NU_m &lt;- matrix( rnorm(E[m]), nrow = E[m] ) return(NU_m) } return(NU) } NU_mc[[1]][[1]] ## [,1] ## [1,] 0.2226442 ## [2,] -1.1481303 ## [3,] -0.1690491 Write a function compute_monte_carlo_sequential_entry(X, Z, EP_mc, NU_mc, beta, alpha, delta, rho) that returns the Monte Carlo simulation. Then, write function compute_objective_sequential_entry(Y, X, Z, EP_mc, NU_mc, theta) that callscompute_monte_carlo_sequential_entry and returns the value of the objective function given data and parameters under the assumption of sequential entry. # sequential entry theta &lt;- theta_sequential &lt;- c( beta, alpha, delta, rho ) Y &lt;- Y_sequential # compute monte carlo simulations Y_mc &lt;- compute_monte_carlo_sequential_entry( X = X, Z = Z, EP_mc = EP_mc, NU_mc = NU_mc, beta = beta, alpha = alpha, delta = delta, rho = rho ) Y_mc[[1]][[1]] ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 # compute objective function compute_objective_sequential_entry( Y = Y, X = X, Z = Z, EP_mc = EP_mc, NU_mc = NU_mc, theta = theta ) ## [1] 0.34 Write a function compute_objective_simultaneous_entry(Y, X, Z, EP_mc, NU_mc, theta) that returns the value of the objective function given data and parameters under the assumption of simultaneous entry. # simultaneous entry theta &lt;- theta_simultaneous &lt;- c( beta, alpha, delta ) Y &lt;- Y_simultaneous # compute monte carlo simulations Y_mc &lt;- compute_monte_carlo_simultaneous_entry( X = X, Z = Z, EP_mc = EP_mc, NU_mc = NU_mc, beta = beta, alpha = alpha, delta = delta ) Y_mc[[1]][[1]] ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 1 # compute objective function compute_objective_simultaneous_entry( Y = Y, X = X, Z = Z, EP_mc = EP_mc, NU_mc = NU_mc, theta = theta ) ## [1] 0.2456 Check the value of the objective function around the true parameter under the assumption of the sequential entry. # sequential entry theta &lt;- theta_sequential &lt;- c( beta, alpha, delta, rho ) Y &lt;- Y_sequential model &lt;- compute_sequential_entry_across_markets label &lt;- c( paste(&quot;\\\\beta_&quot;, 1:K, sep = &quot;&quot;), paste(&quot;\\\\alpha_&quot;, 1:L, sep = &quot;&quot;), &quot;\\\\delta&quot;, &quot;\\\\rho&quot; ) label &lt;- paste(&quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot;) # compute the graph graph &lt;- foreach ( i = 1:length(theta) ) %do% { theta_i &lt;- theta[i] theta_i_list &lt;- theta_i * seq(0.5, 1.5, by = 0.1) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .combine = &quot;rbind&quot; ) %do% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_objective_sequential_entry( Y, X, Z, EP_mc, NU_mc, theta_j ) return(objective_ij) } df_graph &lt;- data.frame( x = theta_i_list, y = objective_i ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y ) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a6/graph_sequential.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a6/graph_sequential.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] ## ## [[6]] Check the value of the objective function around the true parameter under the assumption of the simultaneous entry. # simultaneous entry theta &lt;- theta_simultaneous &lt;- c( beta, alpha, delta ) Y &lt;- Y_simultaneous model &lt;- compute_simultaneous_entry_across_markets label &lt;- c( paste(&quot;\\\\beta_&quot;, 1:K, sep = &quot;&quot;), paste(&quot;\\\\alpha_&quot;, 1:L, sep = &quot;&quot;), &quot;\\\\delta&quot; ) label &lt;- paste(&quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot;) # compute the graph graph &lt;- foreach ( i = 1:length(theta) ) %do% { theta_i &lt;- theta[i] theta_i_list &lt;- theta_i * seq(0.5, 1.5, by = 0.1) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .combine = &quot;rbind&quot; ) %do% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_objective_simultaneous_entry( Y, X, Z, EP_mc, NU_mc, theta_j ) return(objective_ij) } df_graph &lt;- data.frame( x = theta_i_list, y = objective_i ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y ) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a6/graph_simultaneous.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a6/graph_simultaneous.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] Estimate the parameters under the assumption of the sequential entry. # sequential entry theta &lt;- theta_sequential &lt;- c( beta, alpha, delta, rho ) Y &lt;- Y_sequential result_sequential &lt;- optim( par = theta, fn = compute_objective_sequential_entry, method = &quot;Nelder-Mead&quot;, Y = Y, X = X, Z = Z, EP_mc = EP_mc, NU_mc = NU_mc ) saveRDS( result_sequential, file = &quot;lecture/data/a6/estimate_sequential.rds&quot; %&gt;% here::here() ) result_sequential &lt;- readRDS( file = &quot;lecture/data/a6/estimate_sequential.rds&quot; %&gt;% here::here() ) result_sequential ## $par ## [1] 0.7761887 0.2204343 1.0747480 2.0150864 0.9116741 0.2875538 ## ## $value ## [1] 0.2638 ## ## $counts ## function gradient ## 233 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL comparison &lt;- data.frame( actual = theta, estimate = result_sequential$par ) comparison ## actual estimate ## 1 0.6264538 0.7761887 ## 2 0.1836433 0.2204343 ## 3 0.8356286 1.0747480 ## 4 1.5952808 2.0150864 ## 5 1.0000000 0.9116741 ## 6 0.3295078 0.2875538 Estimate the parameters under the assumption of the simultaneous entry. Set the lower bound for \\(\\delta\\) at 0. # simultaneous entry theta &lt;- theta_simultaneous &lt;- c( beta, alpha, delta ) Y &lt;- Y_simultaneous result_simultaneous &lt;- optim( par = theta, fn = compute_objective_simultaneous_entry, method = &quot;Nelder-Mead&quot;, Y = Y, X = X, Z = Z, EP_mc = EP_mc, NU_mc = NU_mc) saveRDS( result_simultaneous, file = &quot;lecture/data/a6/estimate_simultaneous.rds&quot; %&gt;% here::here() ) result_simultaneous &lt;- readRDS( file = &quot;lecture/data/a6/estimate_simultaneous.rds&quot; %&gt;% here::here() ) result_simultaneous ## $par ## [1] 0.7347919 0.2151107 0.9427083 1.7495325 0.9448480 ## ## $value ## [1] 0.2158 ## ## $counts ## function gradient ## 151 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL comparison &lt;- data.frame( actual = theta, estimate = result_simultaneous$par ) comparison ## actual estimate ## 1 0.6264538 0.7347919 ## 2 0.1836433 0.2151107 ## 3 0.8356286 0.9427083 ## 4 1.5952808 1.7495325 ## 5 1.0000000 0.9448480 16.3 Conduct counterfactual simulations Fix the first draw of the Monte Carlo shocks. Suppose that the competitive effect becomes mild, i.e. \\(\\delta\\) is changed to 0.5. Under these shocks, compute the equilibrium number of entrants across markets and plot the histogram with the estimated and counterfactual parameters. Conduct this analysis under the assumptions of sequential and simultaneous entry. "],["assignment7.html", "Chapter 17 Assignment 7: Dynamic Decision 17.1 Simulate data 17.2 Estimate parameters", " Chapter 17 Assignment 7: Dynamic Decision 17.1 Simulate data Suppose that there is a firm and it makes decisions for \\(t = 1, \\cdots, \\infty\\). We solve the model under the infinite-horizon assumption, but generate data only for \\(t = 1, \\cdots, T\\). There are \\(L = 5\\) state \\(s \\in \\{1, 2, 3, 4, 5\\}\\) states for the player. The firm can choose \\(K + 1 = 2\\) actions \\(a \\in \\{0, 1\\}\\). The mean period payoff to the firm is: \\[ \\pi(a, s) := \\alpha \\ln s - \\beta a, \\] where \\(\\alpha, \\beta &gt; 0\\). The period payoff is: \\[ \\pi(a, s) + \\epsilon(a), \\] and \\(\\epsilon(a)\\) is an i.i.d. type-I extreme random variable that is independent of all the other variables. At the beginning of each period, the state \\(s\\) and choice-specific shocks \\(\\epsilon(a), a = 0, 1\\) are realized, and the the firm chooses her action. Then, the game moves to the next period. Suppose that \\(s &gt; 1\\) and \\(s &lt; L\\). If \\(a = 0\\), the state stays at the same state with probability \\(1 - \\kappa\\) and moves down by 1 with probability \\(\\kappa\\). If \\(a = 1\\), the state moves up by 1 with probability \\(\\gamma\\), moves down by 1 with probability \\(\\kappa\\), and stays at the same with probability \\(1 - \\kappa - \\gamma\\). Suppose that \\(s = 1\\). If \\(a = 0\\), the state stays at the same state with probability 1. If \\(a = 1\\), the state moves up by 1 with probability \\(\\gamma\\) and stays at the same with probability \\(1 - \\gamma\\). Suppose that \\(s = L\\). If \\(a = 0\\), the state stays at the same state with probability \\(1 - \\kappa\\) and moves down by 1 with probability \\(\\kappa\\). If \\(a = 1\\), the state moves down by 1 with probability \\(\\kappa\\), and stays at the same with probability \\(1 - \\kappa\\). The mean period profit is summarized in \\(\\Pi\\) as: \\[ \\Pi := \\begin{pmatrix} \\pi(0, 1)\\\\ \\vdots\\\\ \\pi(K, 1)\\\\ \\vdots \\\\ \\pi(0, L)\\\\ \\vdots\\\\ \\pi(K, L)\\\\ \\end{pmatrix} \\] The transition law is summarized in \\(G\\) as: \\[ g(a, s, s&#39;) := \\mathbb{P}\\{s_{t + 1} = s&#39;|s_t = s, a_t = a\\}, \\] \\[ G := \\begin{pmatrix} g(0, 1, 1) &amp; \\cdots &amp; g(0, 1, L)\\\\ \\vdots &amp; &amp; \\vdots \\\\ g(K, 1, 1) &amp; \\cdots &amp; g(K, 1, L)\\\\ &amp; \\vdots &amp; \\\\ g(0, L, 1) &amp; \\cdots &amp; g(0, L, L)\\\\ \\vdots &amp; &amp; \\vdots \\\\ g(K, L, 1) &amp; \\cdots &amp; g(K, L, L)\\\\ \\end{pmatrix}. \\] The discount factor is denoted by \\(\\delta\\). We simulate data for \\(N\\) firms for \\(T\\) periods each. Set constants and parameters as follows: # set seed set.seed(1) # set constants L &lt;- 5 K &lt;- 1 T &lt;- 100 N &lt;- 1000 lambda &lt;- 1e-10 # set parameters alpha &lt;- 0.5 beta &lt;- 3 kappa &lt;- 0.1 gamma &lt;- 0.6 delta &lt;- 0.95 Write function compute_pi(alpha, beta, L, K) that computes \\(\\Pi\\) given parameters and compute the true \\(\\Pi\\) under the true parameters. Don’t use methods in dplyr and deal with matrix operations. PI &lt;- compute_PI( alpha = alpha, beta = beta, L = L, K = K ); PI ## [,1] ## k0_l1 0.0000000 ## k1_l1 -3.0000000 ## k0_l2 0.3465736 ## k1_l2 -2.6534264 ## k0_l3 0.5493061 ## k1_l3 -2.4506939 ## k0_l4 0.6931472 ## k1_l4 -2.3068528 ## k0_l5 0.8047190 ## k1_l5 -2.1952810 Write function compute_G(kappa, gamma, L, K) that computes \\(G\\) given parameters and compute the true \\(G\\) under the true parameters. Don’t use methods in dplyr and deal with matrix operations. G &lt;- compute_G( kappa = kappa, gamma = gamma, L = L, K = K ); G ## l1 l2 l3 l4 l5 ## k0_l1 1.0 0.0 0.0 0.0 0.0 ## k1_l1 0.4 0.6 0.0 0.0 0.0 ## k0_l2 0.1 0.9 0.0 0.0 0.0 ## k1_l2 0.1 0.3 0.6 0.0 0.0 ## k0_l3 0.0 0.1 0.9 0.0 0.0 ## k1_l3 0.0 0.1 0.3 0.6 0.0 ## k0_l4 0.0 0.0 0.1 0.9 0.0 ## k1_l4 0.0 0.0 0.1 0.3 0.6 ## k0_l5 0.0 0.0 0.0 0.1 0.9 ## k1_l5 0.0 0.0 0.0 0.1 0.9 The exante-value function is written as a function of a conditional choice probability as follows: \\[ \\varphi^{(\\theta_1, \\theta_2)}(p) := [I - \\delta \\Sigma(p) G]^{-1}\\Sigma(p)[\\Pi + E(p)], \\] where \\(\\theta_1 = (\\alpha, \\beta)\\) and \\(\\theta_2 = (\\kappa, \\gamma)\\) and: \\[ \\Sigma(p) = \\begin{pmatrix} p(1)&#39; &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; p(L)&#39; \\end{pmatrix} \\] and: \\[ E(p) = \\gamma - \\ln p. \\] Note that \\(\\gamma\\) in the formula of \\(E(p)\\) refers to the Euler constant, not the parameter defined before. Write a function compute_exante_value(p, PI, G, L, K, delta) that returns the exante value function given a conditional choice probability. Don’t use methods in dplyr and deal with matrix operations. When a choice probability is zero at some element, the corresponding element of \\(E(p)\\) can be set at zero, because anyway we multiply the zero probability to the element and the corresponding element in \\(E(p)\\) does not affect the result. p &lt;- matrix( rep(0.5, L * (K + 1)), ncol = 1 ); p ## [,1] ## [1,] 0.5 ## [2,] 0.5 ## [3,] 0.5 ## [4,] 0.5 ## [5,] 0.5 ## [6,] 0.5 ## [7,] 0.5 ## [8,] 0.5 ## [9,] 0.5 ## [10,] 0.5 V &lt;- compute_exante_value( p = p, PI = PI, G = G, L = L, K = K, delta = delta ); V ## [,1] ## l1 5.777876 ## l2 7.597282 ## l3 9.126304 ## l4 10.115439 ## l5 10.593438 The optimal conditional choice probability is written as a function of an exante value function as follows: \\[ \\Lambda^{(\\theta_1, \\theta_2)}(V)(a, s) := \\frac{\\exp[\\pi(a, s) + \\delta \\sum_{s&#39;}V(s&#39;)g(a, s, s&#39;)]}{\\sum_{a&#39;}\\exp[\\pi(a&#39;, s) + \\delta \\sum_{s&#39;}V(s&#39;)g(a&#39;, s, s&#39;)]}, \\] where \\(V\\) is an exante value function. Write a function compute_ccp(V, PI, G, L, K, delta) that returns the optimal conditional choice probability given an exante value function. Don’t use methods in dplyr and deal with matrix operations. To do so, write a function compute_choice_value(V, PI, G, delta) that returns the choice-specific value function. Use this for debugging by checking if the results are intuitive. value &lt;- compute_choice_value( V = V, PI = PI, G = G, delta = delta ); value ## [,1] ## k0_l1 5.488982 ## k1_l1 3.526044 ## k0_l2 7.391148 ## k1_l2 5.262691 ## k0_l3 9.074038 ## k1_l3 6.637845 ## k0_l4 10.208846 ## k1_l4 7.481306 ## k0_l5 10.823075 ## k1_l5 7.823075 p &lt;- compute_ccp( V = V, PI = PI, G = G, L = L, K = K, delta = delta ); p ## [,1] ## k0_l1 0.87685057 ## k1_l1 0.12314943 ## k0_l2 0.89363847 ## k1_l2 0.10636153 ## k0_l3 0.91954591 ## k1_l3 0.08045409 ## k0_l4 0.93863232 ## k1_l4 0.06136768 ## k0_l5 0.95257413 ## k1_l5 0.04742587 Write a function that find the equilibrium conditional choice probability and ex-ante value function by iterating the update of an exante value function and an optimal conditional choice probability. The iteration should stop when \\(\\max_s|V^{(r + 1)}(s) - V^{(r)}(s)| &lt; \\lambda\\) with \\(\\lambda = 10^{-10}\\). output &lt;- solve_dynamic_decision( PI = PI, G = G, L = L, K = K, delta = delta, lambda = lambda ); output ## $p ## [,1] ## k0_l1 0.82218962 ## k1_l1 0.17781038 ## k0_l2 0.80024354 ## k1_l2 0.19975646 ## k0_l3 0.83074516 ## k1_l3 0.16925484 ## k0_l4 0.87691534 ## k1_l4 0.12308466 ## k0_l5 0.95257413 ## k1_l5 0.04742587 ## ## $V ## [,1] ## l1 15.46000 ## l2 18.03675 ## l3 20.86514 ## l4 23.33721 ## l5 25.15557 p &lt;- output$p V &lt;- output$V value &lt;- compute_choice_value( V = V, PI = PI, G = G, delta = delta ); value ## [,1] ## k0_l1 14.68700 ## k1_l1 13.15574 ## k0_l2 17.23669 ## k1_l2 15.84887 ## k0_l3 20.10249 ## k1_l3 18.51157 ## k0_l4 22.62865 ## k1_l4 20.66511 ## k0_l5 24.52976 ## k1_l5 21.52976 Write a function simulate_dynamic_decision(p, s, PI, G, L, K, T, delta, seed) that simulate the data for a single firm starting from an initial state for \\(T\\) periods. The function should accept a value of seed and set the seed at the beginning of the procedure inside the function, because the process is stochastic. To match the generated random numbers, for each period, generate action using rmultinom and then state using rmultinom. # set initial value s &lt;- 1 # draw simulation for a firm seed &lt;- 1 df &lt;- simulate_dynamic_decision( p = p, s = s, G = G, L = L, K = K, T = T, delta = delta, seed = seed ); df ## # A tibble: 100 × 3 ## t s a ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 2 1 0 ## 3 3 1 0 ## 4 4 1 1 ## 5 5 2 1 ## 6 6 1 0 ## 7 7 1 0 ## 8 8 1 0 ## 9 9 1 0 ## 10 10 1 0 ## # ℹ 90 more rows Write a function simulate_dynamic_decision_across_firms(p, s, PI, G, L, K, T, N, delta) that returns simulation data for \\(N\\) firm. For firm \\(i\\), set the seed at \\(i\\) df &lt;- simulate_dynamic_decision_across_firms( p = p, s = s, G = G, L = L, K = K, T = T, N = N, delta = delta ) saveRDS( df, file = &quot;lecture/data/a7/df.rds&quot; %&gt;% here::here() ) df &lt;- readRDS( file = &quot;lecture/data/a7/df.rds&quot; %&gt;% here::here() ) df ## # A tibble: 100,000 × 4 ## i t s a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 0 ## 2 1 2 1 0 ## 3 1 3 1 0 ## 4 1 4 1 1 ## 5 1 5 2 1 ## 6 1 6 1 0 ## 7 1 7 1 0 ## 8 1 8 1 0 ## 9 1 9 1 0 ## 10 1 10 1 0 ## # ℹ 99,990 more rows Write a function estimate_ccp(df) that returns a non-parametric estimate of the conditional choice probability in the data. Compare the estimated conditional choice probability and the true conditional choice probability by a bar plot. p_est &lt;- estimate_ccp(df = df) check_ccp &lt;- cbind( p, p_est ) colnames(check_ccp) &lt;- c( &quot;true&quot;, &quot;estimate&quot; ) check_ccp &lt;- check_ccp %&gt;% reshape2::melt() ggplot( data = check_ccp, aes( x = Var1, y = value, fill = Var2 ) ) + geom_bar( stat = &quot;identity&quot;, position = &quot;dodge&quot; ) + labs(fill = &quot;Value&quot;) + xlab(&quot;action/state&quot;) + ylab(&quot;probability&quot;) + theme_classic() Write a function estimate_G(df) that returns a non-parametric estiamte of the transition matrix in the data. Compare the estimated transition matrix and the true transition matrix by a bar plot. G_est &lt;- estimate_G( df = df ); G_est ## l1 l2 l3 l4 l5 ## k0_l1 1.0000000 0.00000000 0.0000000 0.00000000 0.0000000 ## k1_l1 0.3930818 0.60691824 0.0000000 0.00000000 0.0000000 ## k0_l2 0.1012162 0.89878384 0.0000000 0.00000000 0.0000000 ## k1_l2 0.1031410 0.31276454 0.5840945 0.00000000 0.0000000 ## k0_l3 0.0000000 0.09660837 0.9033916 0.00000000 0.0000000 ## k1_l3 0.0000000 0.09974569 0.3071489 0.59310540 0.0000000 ## k0_l4 0.0000000 0.00000000 0.1012564 0.89874358 0.0000000 ## k1_l4 0.0000000 0.00000000 0.1039339 0.29966003 0.5964060 ## k0_l5 0.0000000 0.00000000 0.0000000 0.09891400 0.9010860 ## k1_l5 0.0000000 0.00000000 0.0000000 0.09751037 0.9024896 check_G &lt;- data.frame( type = &quot;true&quot;, reshape2::melt(G) ) check_G_est &lt;- data.frame( type = &quot;estimate&quot;, reshape2::melt(G_est) ) check_G &lt;- rbind( check_G, check_G_est ) check_G$variable &lt;- paste( check_G$Var1, check_G$Var2, sep = &quot;_&quot; ) ggplot( data = check_G, aes( x = variable, y = value, fill = type ) ) + geom_bar( stat = &quot;identity&quot;, position = &quot;dodge&quot; ) + labs(fill = &quot;Value&quot;) + xlab(&quot;action/state/state&quot;) + ylab(&quot;probability&quot;) + theme(axis.text.x = element_blank()) + theme_classic() 17.2 Estimate parameters Vectorize the parameters as follows: theta_1 &lt;- c( alpha, beta ) theta_2 &lt;- c( kappa, gamma ) theta &lt;- c( theta_1, theta_2 ) First, we estimate the parameters by a nested fixed-point algorithm. The loglikelihood for \\(\\{a_{it}, s_{it}\\}_{i = 1, \\cdots, N, t = 1, \\cdots, T}\\) is: \\[ \\frac{1}{NT} \\sum_{i = 1}^N \\sum_{t = 1}^T[\\log\\mathbb{P}\\{a_{it}|s_{it}\\} + \\log \\mathbb{P}\\{s_{i, t + 1}|a_{it}, s_{it}\\}], \\] with \\(\\mathbb{P}\\{s_{i, T + 1}|a_{iT}, s_{iT}\\} = 1\\) for all \\(i\\) as \\(s_{i, T + 1}\\) is not observed. Write a function compute_loglikelihood_NFP(theta, df, delta, L, K) that compute the loglikelihood. loglikelihood &lt;- compute_loglikelihood_NFP( theta = theta, df = df, delta = delta, L = L, K = K ); loglikelihood ## [1] -0.7474961 Check the value of the objective function around the true parameter. # label label &lt;- c( &quot;\\\\alpha&quot;, &quot;\\\\beta&quot;, &quot;\\\\kappa&quot;, &quot;\\\\gamma&quot; ) label &lt;- paste( &quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot; ) # compute the graph graph &lt;- foreach ( i = 1:length(theta) ) %do% { theta_i &lt;- theta[i] theta_i_list &lt;- theta_i * seq( 0.8, 1.2, by = 0.05 ) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .combine = &quot;rbind&quot; ) %do% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_loglikelihood_NFP( theta_j, df, delta, L, K ); loglikelihood return(objective_ij) } df_graph &lt;- data.frame( x = theta_i_list, y = objective_i ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y ) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a7/NFP_graph.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a7/NFP_graph.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Estiamte the parameters by maximizing the loglikelihood. To keep the model to be well-defined, impose an ad hoc lower and upper bounds such that \\(\\alpha \\in [0, 1], \\beta \\in [0, 5], \\kappa \\in [0, 0.2], \\gamma \\in [0, 0.7]\\). lower &lt;- rep(0, length(theta)) upper &lt;- c(1, 5, 0.2, 0.7) NFP_result &lt;- optim( par = theta, fn = compute_loglikelihood_NFP, method = &quot;L-BFGS-B&quot;, lower = lower, upper = upper, control = list(fnscale = -1), df = df, delta = delta, L = L, K = K ) saveRDS( NFP_result, file = &quot;lecture/data/a7/NFP_result.rds&quot; %&gt;% here::here() ) NFP_result &lt;- readRDS( file = &quot;lecture/data/a7/NFP_result.rds&quot; %&gt;% here::here() ) NFP_result ## $par ## [1] 0.5273235 3.0652558 0.1000122 0.5955431 ## ## $value ## [1] -0.7474743 ## ## $counts ## function gradient ## 21 21 ## ## $convergence ## [1] 0 ## ## $message ## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; compare &lt;- data.frame( true = theta, estimate = NFP_result$par ); compare ## true estimate ## 1 0.5 0.5273235 ## 2 3.0 3.0652558 ## 3 0.1 0.1000122 ## 4 0.6 0.5955431 Next, we estimate the parameters by CCP approach. Write a function estimate_theta_2(df) that returns the estimates of \\(\\kappa\\) and \\(\\gamma\\) directly from data by counting relevant events. theta_2_est &lt;- estimate_theta_2( df = df ); theta_2_est ## [1] 0.09988488 0.59551895 The objective function of the minimum distance estimator based on the conditional choice probability approach is: \\[ \\frac{1}{KL}\\sum_{s = 1}^L \\sum_{a = 1}^K\\{\\hat{p}(a, s) - p^{(\\theta_1, \\theta_2)}(a, s)\\}^2, \\] where \\(\\hat{p}\\) is the non-parametric estimate of the conditional choice probability and \\(p^{(\\theta_1, \\theta_2)}\\) is the optimal conditional choice probability under parameters \\(\\theta_1\\) and \\(\\theta_2\\). Write a function compute_CCP_objective(theta_1, theta_2, p_est, L, K, delta) that returns the objective function of the above minimum distance estimator given a non-parametric estimate of the conditional choice probability and \\(\\theta_1\\) and \\(\\theta_2\\). compute_CCP_objective( theta_1 = theta_1, theta_2 = theta_2, p_est = p_est, L = L, K = K, delta = delta ) ## [1] 5.000511e-06 Check the value of the objective function around the true parameter. # label label &lt;- c( &quot;\\\\alpha&quot;, &quot;\\\\beta&quot; ) label &lt;- paste( &quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot; ) # compute the graph graph &lt;- foreach ( i = 1:length(theta_1) ) %do% { theta_i &lt;- theta_1[i] theta_i_list &lt;- theta_i * seq( 0.8, 1.2, by = 0.05 ) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .combine = &quot;rbind&quot; ) %do% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta_1 theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_CCP_objective( theta_j, theta_2, p_est, L, K, delta ) return(objective_ij) } df_graph &lt;- data.frame( x = theta_i_list, y = objective_i ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a7/CCP_graph.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a7/CCP_graph.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] Estiamte the parameters by minimizing the objective function. To keep the model to be well-defined, impose an ad hoc lower and upper bounds such that \\(\\alpha \\in [0, 1], \\beta \\in [0, 5]\\). lower &lt;- rep(0, length(theta_1)) upper &lt;- c(1, 5) CCP_result &lt;- optim( par = theta_1, fn = compute_CCP_objective, method = &quot;L-BFGS-B&quot;, lower = lower, upper = upper, theta_2 = theta_2_est, p_est = p_est, L = L, K = K, delta = delta ) saveRDS( CCP_result, file = &quot;lecture/data/a7/CCP_result.rds&quot; %&gt;% here::here() ) CCP_result &lt;- readRDS( file = &quot;lecture/data/a7/CCP_result.rds&quot; %&gt;% here::here() ) CCP_result ## $par ## [1] 0.5271684 3.0644600 ## ## $value ## [1] 1.790528e-06 ## ## $counts ## function gradient ## 11 11 ## ## $convergence ## [1] 0 ## ## $message ## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; compare &lt;- data.frame( true = theta_1, estimate = CCP_result$par ); compare ## true estimate ## 1 0.5 0.5271684 ## 2 3.0 3.0644600 "],["assignment8.html", "Chapter 18 Assignment 8: Dynamic Game 18.1 Simulate data 18.2 Estimate parameters", " Chapter 18 Assignment 8: Dynamic Game 18.1 Simulate data Suppose that there are \\(m = 1, \\cdots, M\\) markets and in each market there are \\(i = 1, \\cdots, N\\) firms and each firm makes decisions for \\(t = 1, \\cdots, \\infty\\). In the following, I suppress the index of market, \\(m\\). We solve the model under the infinite-horizon assumption, but generate data only for \\(t = 1, \\cdots, T\\). There are \\(L = 5\\) state \\(\\{1, 2, 3, 4, 5\\}\\) states for each firm. Each firm can choose \\(K + 1 = 2\\) actions \\(\\{0, 1\\}\\). Thus, \\(m_a := (K + 1)^N\\) and \\(m_s = L^N\\). Let \\(a_i\\) and \\(s_i\\) be firm \\(i\\)’s action and state and \\(a\\) and \\(s\\) are vectors of individual actions and states. The mean period payoff to firm \\(i\\) is: \\[ \\pi_i(a, s) := \\tilde{\\pi}(a_i, s_i, \\overline{s}) := \\alpha \\ln s_i - \\eta \\ln s_i \\sum_{j \\neq i} \\ln s_j - \\beta a_i, \\] where \\(\\alpha, \\beta, \\eta&gt; 0\\), and \\(\\alpha &gt; \\eta\\). The term \\(\\eta\\) means that the returns to investment decreases as rival’s average state profile improves. The period payoff is: \\[ \\tilde{\\pi}(a_i, s_i, \\overline{s})+ \\epsilon_i(a_i), \\] and \\(\\epsilon_i(a_i)\\) is an i.i.d. type-I extreme random variable that is independent of all the other variables. At the beginning of each period, the state \\(s\\) is realized and publicly observed. Then choice-specific shocks \\(\\epsilon_i(a_i), a_i = 0, 1\\) are realized and privately observed by firm \\(i = 1, \\cdots, N\\). Then each firm simultaneously chooses her action. Then, the game moves to next period. State transition is independent across firms conditional on individual state and action. Suppose that \\(s_i &gt; 1\\) and \\(s_i &lt; L\\). If \\(a_i = 0\\), the state stays at the same state with probability \\(1 - \\kappa\\) and moves down by 1 with probability \\(\\kappa\\). If \\(a = 1\\), the state moves up by 1 with probability \\(\\gamma\\), moves down by 1 with probability \\(\\kappa\\), and stays at the same with probability \\(1 - \\kappa - \\gamma\\). Suppose that \\(s_i = 1\\). If \\(a_i = 0\\), the state stays at the same state with probability 1. If \\(a_i = 1\\), the state moves up by 1 with probability \\(\\gamma\\) and stays at the same with probability \\(1 - \\gamma\\). Suppose that \\(s_i = L\\). If \\(a_i = 0\\), the state stays at the same state with probability \\(1 - \\kappa\\) and moves down by 1 with probability \\(\\kappa\\). If \\(a = 1\\), the state moves down by 1 with probability \\(\\kappa\\), and stays at the same with probability \\(1 - \\kappa\\). The mean period profit is summarized in \\(\\Pi\\) as: \\[ \\Pi := \\begin{pmatrix} \\pi(1, 1)\\\\ \\vdots\\\\ \\pi(m_a, 1)\\\\ \\vdots \\\\ \\pi(1, m_s)\\\\ \\vdots\\\\ \\pi(m_a, m_s)\\\\ \\end{pmatrix} \\] The transition law is summarized in \\(G\\) as: \\[ g(a, s, s&#39;) := \\mathbb{P}\\{s_{t + 1} = s&#39;|s_t = s, a_t = a\\}, \\] \\[ G := \\begin{pmatrix} g(1, 1, 1) &amp; \\cdots &amp; g(1, 1, m_s)\\\\ \\vdots &amp; &amp; \\vdots \\\\ g(m_a, 1, 1) &amp; \\cdots &amp; g(m_a, 1, m_s)\\\\ &amp; \\vdots &amp; \\\\ g(1, m_s, 1) &amp; \\cdots &amp; g(1, m_s, m_s)\\\\ \\vdots &amp; &amp; \\vdots \\\\ g(m_a, m_s, 1) &amp; \\cdots &amp; g(m_a, m_s, m_s)\\\\ \\end{pmatrix}. \\] The discount factor is denoted by \\(\\delta\\). We simulate data for \\(M\\) markets with \\(N\\) firms for \\(T\\) periods. Set constants and parameters as follows: # set seed set.seed(1) # set constants L &lt;- 5 K &lt;- 1 T &lt;- 100 N &lt;- 3 M &lt;- 1000 lambda &lt;- 1e-10 # set parameters alpha &lt;- 1 eta &lt;- 0.3 beta &lt;- 2 kappa &lt;- 0.1 gamma &lt;- 0.6 delta &lt;- 0.95 Write a function compute_action_state_space(K, L, N) that returns a data frame for action and state space. Returned objects are list of data frame A and S. In A, column k is the index of an action profile, i is the index of a firm, and a is the action of the firm. In S, column l is the index of an state profile, i is the index of a firm, and s is the state of the firm. output &lt;- compute_action_state_space( L = L, K = K, N = N ) A &lt;- output$A head(A) ## # A tibble: 6 × 3 ## k i a ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 0 ## 2 1 2 0 ## 3 1 3 0 ## 4 2 1 1 ## 5 2 2 0 ## 6 2 3 0 tail(A) ## # A tibble: 6 × 3 ## k i a ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 7 1 0 ## 2 7 2 1 ## 3 7 3 1 ## 4 8 1 1 ## 5 8 2 1 ## 6 8 3 1 S &lt;- output$S head(S) ## # A tibble: 6 × 3 ## l i s ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 ## 2 1 2 1 ## 3 1 3 1 ## 4 2 1 2 ## 5 2 2 1 ## 6 2 3 1 tail(S) ## # A tibble: 6 × 3 ## l i s ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 124 1 4 ## 2 124 2 5 ## 3 124 3 5 ## 4 125 1 5 ## 5 125 2 5 ## 6 125 3 5 # dimension m_a &lt;- max(A$k); m_a ## [1] 8 m_s &lt;- max(S$l); m_s ## [1] 125 Write function compute_PI_game(alpha, beta, eta, A, S) that returns a list of \\(\\Pi_i\\). PI &lt;- compute_PI_game( alpha = alpha, beta = beta, eta = eta, A = A, S = S ) head(PI[[N]]) ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] -2 ## [6,] -2 dim(PI[[N]])[1] == m_s * m_a ## [1] TRUE Write function compute_G_game(g, A, S) that converts an individual transition probability matrix into a joint transition probability matrix \\(G\\). G_marginal &lt;- compute_G( kappa = kappa, gamma = gamma, L = L, K = K ) G &lt;- compute_G_game( G_marginal = G_marginal, A = A, S = S ) head(G) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## [1,] 1.00 0.00 0 0 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0.40 0.60 0 0 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0.40 0.00 0 0 0 0.60 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [4,] 0.16 0.24 0 0 0 0.24 0.36 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [5,] 0.40 0.00 0 0 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [6,] 0.16 0.24 0 0 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 ## [1,] 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [4,] 0 0.00 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [5,] 0 0.60 0.00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [6,] 0 0.24 0.36 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [1,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 ## [1,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 ## [1,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 117 118 119 120 121 122 123 124 125 ## [1,] 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 0 0 0 dim(G)[1] == m_s * m_a ## [1] TRUE dim(G)[2] == m_s ## [1] TRUE The ex-ante-value function for a firm is written as a function of a conditional choice probability as follows: \\[ \\varphi_i^{(\\theta_1, \\theta_2)}(p) := [I - \\delta \\Sigma(p) G]^{-1}[\\Sigma(p)\\Pi_i + D_i(p)], \\] where \\(\\theta_1 = (\\alpha, \\beta, \\eta)\\) and \\(\\theta_2 = (\\kappa, \\gamma)\\), \\(p_i(a_i|s)\\) is the probability that firm \\(i\\) choose action \\(a_i\\) when the state profile is \\(s\\), and: \\[ p(a|s) = \\prod_{i = 1}^N p_i(a_i|s), \\] \\[ p(s) = \\begin{pmatrix} p(1|s) \\\\ \\vdots \\\\ p(m_a|s) \\end{pmatrix}, \\] \\[ p = \\begin{pmatrix} p(1)\\\\ \\vdots\\\\ p(m_s) \\end{pmatrix}, \\] \\[ \\Sigma(p) = \\begin{pmatrix} p(1)&#39; &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; p(L)&#39; \\end{pmatrix} \\] and: \\[ D_i(p) = \\begin{pmatrix} \\sum_{k = 0}^K \\mathbb{E}\\{\\epsilon_i^k|a_i = k, 1\\}p_i(a_i = k|1)\\\\ \\vdots\\\\ \\sum_{k = 0}^K \\mathbb{E}\\{\\epsilon_i^k|a_i = k, m_s\\}p_i(a_i = k|m_s) \\end{pmatrix}. \\] Write a function initialize_p_marginal(A, S) that defines an initial marginal condition choice probability. In the output p_marginal, p is the probability for firm i to take action a conditional on the state profile being l. Next, write a function compute_p_joint(p_marginal, A, S) that computes a corresponding joint conditional choice probability from a marginal conditional choice probability. In the output p_joint, p is the joint probability that firms take action profile k condition on the state profile being l. Finally, write a function compute_p_marginal(p_joint, A, S) that compute a corresponding marginal conditional choice probability from a joint conditional choice probability. # define a conditional choice probability for each firm p_marginal &lt;- initialize_p_marginal( A = A, S = S ) p_marginal ## # A tibble: 750 × 4 ## i l a p ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 0.5 ## 2 1 1 1 0.5 ## 3 1 2 0 0.5 ## 4 1 2 1 0.5 ## 5 1 3 0 0.5 ## 6 1 3 1 0.5 ## 7 1 4 0 0.5 ## 8 1 4 1 0.5 ## 9 1 5 0 0.5 ## 10 1 5 1 0.5 ## # ℹ 740 more rows dim(p_marginal)[1] == N * m_s * (K + 1) ## [1] TRUE # compute joint conditional choice probability from marginal probability p_joint &lt;- compute_p_joint( p_marginal = p_marginal, A = A, S = S ) p_joint ## # A tibble: 1,000 × 3 ## l k p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0.125 ## 2 1 2 0.125 ## 3 1 3 0.125 ## 4 1 4 0.125 ## 5 1 5 0.125 ## 6 1 6 0.125 ## 7 1 7 0.125 ## 8 1 8 0.125 ## 9 2 1 0.125 ## 10 2 2 0.125 ## # ℹ 990 more rows dim(p_joint)[1] == m_s * m_a ## [1] TRUE # compute marginal conditional choice probability from joint probability p_marginal_2 &lt;- compute_p_marginal( p_joint = p_joint, A = A, S = S ) max(abs(p_marginal - p_marginal_2)) ## [1] 0 Write a function compute_sigma(p_marginal, A, S) that computes \\(\\Sigma(p)\\) given a joint conditional choice probability. Then, write a function compute_D(p_marginal) that returns a list of \\(D_i(p)\\). # compute Sigma for ex-ante value function calculation sigma &lt;- compute_sigma( p_marginal = p_marginal, A = A, S = S ) head(sigma) ## 6 x 1000 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 . . . . ## [2,] . . . . . . . . 0.125 0.125 0.125 0.125 ## [3,] . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . ## [2,] 0.125 0.125 0.125 0.125 . . . . . . . . ## [3,] . . . . 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 ## [4,] . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . ## [4,] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 . . . . ## [5,] . . . . . . . . 0.125 0.125 0.125 0.125 ## [6,] . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . ## [5,] 0.125 0.125 0.125 0.125 . . . . . . . . . ## [6,] . . . . 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . dim(sigma)[1] == m_s ## [1] TRUE dim(sigma)[2] == m_s * m_a ## [1] TRUE # compute D for ex-ante value function calculation D &lt;- compute_D(p_marginal) head(D[[N]]) ## [,1] ## [1,] 1.270363 ## [2,] 1.270363 ## [3,] 1.270363 ## [4,] 1.270363 ## [5,] 1.270363 ## [6,] 1.270363 dim(D[[N]])[1] == m_s ## [1] TRUE Write a function compute_exante_value_game(p_marginal, A, S, PI, G, delta) that returns a list of matrices whose \\(i\\)-th element represents the ex-ante value function given a conditional choice probability for firm \\(i\\). # compute ex-ante value funciton for each firm V &lt;- compute_exante_value_game( p_marginal = p_marginal, A = A, S = S, PI = PI, G = G, delta = delta ) head(V[[N]]) ## 6 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## l1 10.786330 ## l2 10.175982 ## l3 9.606812 ## l4 9.255459 ## l5 9.115332 ## l6 10.175982 dim(V[[N]])[1] == m_s ## [1] TRUE The optimal conditional choice probability is written as a function of an ex-ante value function and a conditional choice probability of others as follows: \\[ \\Lambda_i^{(\\theta_1, \\theta_2)}(V_i, p_{-i})(a_i, s) := \\frac{\\exp\\{\\sum_{a_{-i}}p_{-i}(a_{-i}|s)[\\pi_i(a_i, a_{-i}, s) + \\delta \\sum_{s&#39;}V_i(s&#39;)g(a_i, a_{-i}, s, s&#39;)]\\}}{\\sum_{a_i&#39;}\\exp\\{\\sum_{a_{-i}}p_{-i}(a_{-i}|s)[\\pi_i(a_i&#39;, a_{-i}, s) + \\delta \\sum_{s&#39;}V_i(s&#39;)g(a_i&#39;, a_{-i}, s, s&#39;)]\\}}, \\] where \\(V\\) is an ex-ante value function. Write a function compute_profile_value_game(V, PI, G, delta, S, A) that returns a data frame that contains information on value function at a state and action profile for each firm. In the output value, i is the index of a firm, l is the index of a state profile, k is the index of an action profile, and value is the value for the firm at the state and action profile. # compute state-action-profile value function value &lt;- compute_profile_value_game( V = V, PI = PI, G = G, delta = delta, S = S, A = A ) value ## # A tibble: 3,000 × 4 ## i l k value ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 10.2 ## 2 1 1 2 9.63 ## 3 1 1 3 9.90 ## 4 1 1 4 9.13 ## 5 1 1 5 9.90 ## 6 1 1 6 9.13 ## 7 1 1 7 9.55 ## 8 1 1 8 8.64 ## 9 1 2 1 13.0 ## 10 1 2 2 12.1 ## # ℹ 2,990 more rows dim(value)[1] == N * m_s * m_a ## [1] TRUE Write a function compute_choice_value_game(p_marginal, V, PI, G, delta, A, S) that computes a data frame that contains information on a choice-specific value function given an ex-ante value function and a conditional choice probability of others. # compute choice-specific value function value &lt;- compute_choice_value_game( p_marginal = p_marginal, V = V, PI = PI, G = G, delta = delta, A = A, S = S ) value ## # A tibble: 750 × 4 ## i l a value ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 9.90 ## 2 1 1 1 9.13 ## 3 1 2 0 12.4 ## 4 1 2 1 11.4 ## 5 1 3 0 14.5 ## 6 1 3 1 13.2 ## 7 1 4 0 16.0 ## 8 1 4 1 14.3 ## 9 1 5 0 16.8 ## 10 1 5 1 14.8 ## # ℹ 740 more rows Write a function compute_ccp_game(p_marginal, V, PI, G, delta, A, S) that computes a data frame that contains information on a conditional choice probability given an ex-ante value function and a conditional choice probability of others. # compute conditional choice probability p_marginal &lt;- compute_ccp_game( p_marginal = p_marginal, V = V, PI = PI, G = G, delta = delta, A = A, S = S ) p_marginal ## # A tibble: 750 × 4 ## i l a p ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 0.683 ## 2 1 1 1 0.317 ## 3 1 2 0 0.734 ## 4 1 2 1 0.266 ## 5 1 3 0 0.794 ## 6 1 3 1 0.206 ## 7 1 4 0 0.840 ## 8 1 4 1 0.160 ## 9 1 5 0 0.881 ## 10 1 5 1 0.119 ## # ℹ 740 more rows Write a function solve_dynamic_game(PI, G, L, K, delta, lambda, A, S) that find the equilibrium conditional choice probability and ex-ante value function by iterating the update of an ex-ante value function and a best-response conditional choice probability. The iteration should stop when \\(\\max_s|V^{(r + 1)}(s) - V^{(r)}(s)| &lt; \\lambda\\) with \\(\\lambda = 10^{-10}\\). There is no theoretical guarantee for the convergence. # solve the dynamic game model output &lt;- solve_dynamic_game( PI = PI, G = G, L = L, K = K, delta = delta, lambda = lambda, A = A, S = S ) saveRDS( output, file = &quot;lecture/data/a8/equilibrium.rds&quot; %&gt;% here::here() ) output &lt;- readRDS( file = &quot;lecture/data/a8/equilibrium.rds&quot; %&gt;% here::here() ) p_marginal &lt;- output$p_marginal; head(p_marginal) ## # A tibble: 6 × 4 ## i l a p ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 0.534 ## 2 1 1 1 0.466 ## 3 1 2 0 0.545 ## 4 1 2 1 0.455 ## 5 1 3 0 0.629 ## 6 1 3 1 0.371 V &lt;- output$V[[N]]; head(V) ## 6 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## l1 18.98883 ## l2 18.51236 ## l3 18.08141 ## l4 17.77417 ## l5 17.59426 ## l6 18.51236 # compute joint conitional choice probability p_joint &lt;- compute_p_joint( p_marginal = p_marginal, A = A, S = S ); head(p_joint) ## # A tibble: 6 × 3 ## l k p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0.152 ## 2 1 2 0.133 ## 3 1 3 0.133 ## 4 1 4 0.116 ## 5 1 5 0.133 ## 6 1 6 0.116 Write a function simulate_dynamic_game(p_joint, l, G, N, T, S, A, seed) that simulate the data for a market starting from an initial state for \\(T\\) periods. The function should accept a value of seed and set the seed at the beginning of the procedure inside the function, because the process is stochastic. To match the generated random numbers, for each period, generate action using rmultinom and then state using rmultinom. # simulate a dynamic game # set initial state profile l &lt;- 1 # draw simulation for a firm seed &lt;- 1 df &lt;- simulate_dynamic_game( p_joint = p_joint, l = l, G = G, N = N, T = T, S = S, A = A, seed = seed ) df ## # A tibble: 300 × 6 ## t i l k s a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 4 1 1 ## 2 1 2 1 4 1 1 ## 3 1 3 1 4 1 0 ## 4 2 1 2 1 2 0 ## 5 2 2 2 1 1 0 ## 6 2 3 2 1 1 0 ## 7 3 1 2 5 2 0 ## 8 3 2 2 5 1 0 ## 9 3 3 2 5 1 1 ## 10 4 1 2 3 2 0 ## # ℹ 290 more rows Write a function simulate_dynamic_decision_across_markets(p_joint, l, G, N, T, M, S, A, seed) that returns simulation data for \\(M\\) markets. For firm \\(m\\), set the seed at \\(m\\) # simulate data across markets df &lt;- simulate_dynamic_decision_across_markets( p_joint = p_joint, l = l, G = G, N = N, T = T, M = M, S = S, A = A ) saveRDS( df, file = &quot;lecture/data/a8/df.rds&quot; %&gt;% here::here() ) df &lt;- readRDS( file = &quot;lecture/data/a8/df.rds&quot; %&gt;% here::here() ) df ## # A tibble: 300,000 × 7 ## m t i l k s a ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 1 4 1 1 ## 2 1 1 2 1 4 1 1 ## 3 1 1 3 1 4 1 0 ## 4 1 2 1 2 1 2 0 ## 5 1 2 2 2 1 1 0 ## 6 1 2 3 2 1 1 0 ## 7 1 3 1 2 5 2 0 ## 8 1 3 2 2 5 1 0 ## 9 1 3 3 2 5 1 1 ## 10 1 4 1 2 3 2 0 ## # ℹ 299,990 more rows summary(df) ## m t i l k ## Min. : 1.0 Min. : 1.00 Min. :1 Min. : 1.00 Min. :1.000 ## 1st Qu.: 250.8 1st Qu.: 25.75 1st Qu.:1 1st Qu.: 44.00 1st Qu.:1.000 ## Median : 500.5 Median : 50.50 Median :2 Median : 75.00 Median :2.000 ## Mean : 500.5 Mean : 50.50 Mean :2 Mean : 71.89 Mean :2.497 ## 3rd Qu.: 750.2 3rd Qu.: 75.25 3rd Qu.:3 3rd Qu.:102.00 3rd Qu.:4.000 ## Max. :1000.0 Max. :100.00 Max. :3 Max. :125.00 Max. :8.000 ## s a ## Min. :1.000 Min. :0.0000 ## 1st Qu.:2.000 1st Qu.:0.0000 ## Median :3.000 Median :0.0000 ## Mean :3.288 Mean :0.2131 ## 3rd Qu.:5.000 3rd Qu.:0.0000 ## Max. :5.000 Max. :1.0000 Write a function estimate_ccp_marginal_game(df) that returns a non-parametric estimate of the marginal conditional choice probability for each firm in the data. Compare the estimated conditional choice probability and the true conditional choice probability by a bar plot. # non-parametrically estimate the conditional choice probability p_marginal_est &lt;- estimate_ccp_marginal_game(df = df) check_ccp &lt;- p_marginal_est %&gt;% dplyr::rename(estimate = p) %&gt;% dplyr::left_join( p_marginal, by = c( &quot;i&quot;, &quot;l&quot;, &quot;a&quot; ) ) %&gt;% dplyr::rename(true = p) %&gt;% dplyr::filter(a == 1) ggplot( data = check_ccp, aes( x = true, y = estimate ) ) + geom_point() + labs(fill = &quot;Value&quot;) + xlab(&quot;true&quot;) + ylab(&quot;estimate&quot;) + theme_classic() Write a function estimate_G_marginal(df) that returns a non-parametric estimate of the marginal transition probability matrix. Compare the estimated transition matrix and the true transition matrix by a bar plot. # non-parametrically estimate individual transition probability G_marginal_est &lt;- estimate_G_marginal(df = df) check_G &lt;- data.frame( type = &quot;true&quot;, reshape2::melt(G_marginal) ) check_G_est &lt;- data.frame( type = &quot;estimate&quot;, reshape2::melt(G_marginal_est) ) check_G &lt;- rbind( check_G, check_G_est ) check_G$variable &lt;- paste( check_G$Var1, check_G$Var2, sep = &quot;_&quot; ) ggplot( data = check_G, aes( x = variable, y = value, fill = type ) ) + geom_bar( stat = &quot;identity&quot;, position = &quot;dodge&quot; ) + labs(fill = &quot;Value&quot;) + xlab(&quot;action/state/state&quot;) + ylab(&quot;probability&quot;) + theme(axis.text.x = element_blank()) + theme_classic() 18.2 Estimate parameters Vectorize the parameters as follows: theta_1 &lt;- c( alpha, beta, eta ) theta_2 &lt;- c( kappa, gamma ) theta &lt;- c( theta_1, theta_2 ) We estimate the parameters by a CCP approach. Write a function estimate_theta_2_game(df) that returns the estimates of \\(\\kappa\\) and \\(\\gamma\\) directly from data by counting relevant events. # estimate theta_2 theta_2_est &lt;- estimate_theta_2_game( df = df ); theta_2_est ## [1] 0.09946371 0.60228274 The objective function of the minimum distance estimator based on the conditional choice probability approach is: \\[ \\frac{1}{N K m_s} \\sum_{i = 1}^N \\sum_{l = 1}^{m_s} \\sum_{k = 1}^{K}\\{\\hat{p}_i(a_k|s_l) - p_i^{(\\theta_1, \\theta_2)}(a_k|s_l)\\}^2, \\] where \\(\\hat{p}_i\\) is the non-parametric estimate of the marginal conditional choice probability and \\(p_i^{(\\theta_1, \\theta_2)}\\) is the marginal conditional choice probability under parameters \\(\\theta_1\\) and \\(\\theta_2\\) given \\(\\hat{p}_i\\). \\(a_k\\) is \\(k\\)-th action for a firm and \\(s_l\\) is \\(l\\)-th state profile. Write a function compute_CCP_objective_game(theta_1, theta_2, p_est, L, K, delta) that returns the objective function of the above minimum distance estimator given a non-parametric estimate of the conditional choice probability and \\(\\theta_1\\) and \\(\\theta_2\\). # compute the objective function of the minimum distance estimator based on the CCP approach objective &lt;- compute_CCP_objective_game( theta_1 = theta_1, theta_2 = theta_2, p_marginal_est = p_marginal_est, A = A, S = S, delta = delta, lambda = lambda ) saveRDS( objective, file = &quot;lecture/data/a8/objective.rds&quot; %&gt;% here::here() ) objective &lt;- readRDS( file = &quot;lecture/data/a8/objective.rds&quot; %&gt;% here::here() ) objective ## [1] 0.0003285307 Check the value of the objective function around the true parameter. # label label &lt;- c( &quot;\\\\alpha&quot;, &quot;\\\\beta&quot;, &quot;\\\\eta&quot; ) label &lt;- paste( &quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot; ) # compute the graph graph &lt;- foreach ( i = 1:length(theta_1) ) %do% { theta_i &lt;- theta_1[i] theta_i_list &lt;- theta_i * seq( 0.5, 2, by = 0.2 ) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .combine = &quot;rbind&quot; ) %dopar% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta_1 theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_CCP_objective_game( theta_j, theta_2, p_marginal_est, A, S, delta, lambda ) return(objective_ij) } df_graph &lt;- data.frame( x = theta_i_list, y = objective_i ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y ) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a8/CCP_graph.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a8/CCP_graph.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] ## ## [[3]] Estimate the parameters by minimizing the objective function. To keep the model to be well-defined, impose an ad hoc lower and upper bounds such that \\(\\alpha \\in [0, 1], \\beta \\in [0, 5], \\delta \\in [0, 1]\\). lower &lt;- rep( 0, length(theta_1) ) upper &lt;- c(1, 5, 0.3) CCP_result &lt;- optim( par = theta_1, fn = compute_CCP_objective_game, method = &quot;L-BFGS-B&quot;, lower = lower, upper = upper, theta_2 = theta_2_est, p_marginal_est = p_marginal_est, A = A, S = S, delta = delta, lambda = lambda ) saveRDS( CCP_result, file = &quot;lecture/data/a8/CCP_result.rds&quot; %&gt;% here::here() ) CCP_result &lt;- readRDS( file = &quot;lecture/data/a8/CCP_result.rds&quot; %&gt;% here::here() ) CCP_result ## $par ## [1] 1.0000000 2.0205947 0.2964074 ## ## $value ## [1] 0.0003271323 ## ## $counts ## function gradient ## 17 17 ## ## $convergence ## [1] 0 ## ## $message ## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; compare &lt;- data.frame( true = theta_1, estimate = CCP_result$par ); compare ## true estimate ## 1 1.0 1.0000000 ## 2 2.0 2.0205947 ## 3 0.3 0.2964074 "],["assignment9.html", "Chapter 19 Assignment 9: Auction 19.1 Simulate data 19.2 Estimate the parameters", " Chapter 19 Assignment 9: Auction 19.1 Simulate data We simulate bid data from a second- and first-price sealed bid auctions. First, we draw bid data from a second-price sealed bid auctions. Suppose that for each auction \\(t = 1, \\cdots, T\\), there are \\(i = 2, \\cdots, n_t\\) potential bidders. At each auction, an auctioneer allocates one item and sets the reserve price at \\(r_t\\). When the signal for bidder \\(i\\) in auction \\(t\\) is \\(x_{it}\\), her expected value of the item is \\(x_{it}\\). A signal \\(x_{it}\\) is drawn from an i.i.d. beta distribution \\(B(\\alpha, \\beta)\\). Let \\(F_X(\\cdot; \\alpha, \\beta)\\) be its distribution and \\(f_X(\\cdot; \\alpha, \\beta)\\) be the density. A reserve is set at 0.2. \\(n_t\\) is drawn from a Poisson distribution with mean \\(\\lambda\\). If \\(n_t = 1\\), replace with \\(n_t = 2\\) to ensure at least two potential bidders. An equilibrium strategy is such that a bidder participates and bids \\(\\beta(x) = x\\) if \\(x \\ge r_t\\) and does not participate otherwise. Set the constants and parameters as follows: # set seed set.seed(1) # number of auctions T &lt;- 100 # parameter of value distribution alpha &lt;- 2 beta &lt;- 2 # prameters of number of potential bidders lambda &lt;- 10 Draw a vector of valuations and reservation prices. # number of bidders N &lt;- rpois( T, lambda ) N &lt;- ifelse( N == 1, 2, N ) # draw valuations valuation &lt;- foreach ( tt = 1:T, .combine = &quot;rbind&quot; ) %do% { n_t &lt;- N[tt] header &lt;- expand.grid( t = tt, i = 1:n_t ) return(header) } valuation &lt;- valuation %&gt;% tibble::as_tibble() %&gt;% dplyr::mutate(x = rbeta( length(i), alpha,beta ) ) ggplot( valuation, aes(x = x) ) + geom_histogram( fill = &quot;steelblue&quot;, alpha = 0.8 ) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # draw reserve prices reserve &lt;- 0.2 reserve &lt;- tibble::tibble( t = 1:T, r = reserve ) Write a function compute_winning_bids_second(valuation, reserve) that returns a winning bid from each second-price auction. It returns nothing for an auction in which no bid was above the reserve price. In the output, t refers to the auction index, m to the number of actual bidders, r to the reserve price, and w to the winning bid. # compute winning bids from second-price auction df_second_w &lt;- compute_winning_bids_second( valuation = valuation, reserve = reserve ) df_second_w ## # A tibble: 100 × 5 ## t n m r w ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8 8 0.2 0.637 ## 2 2 10 10 0.2 0.647 ## 3 3 7 5 0.2 0.484 ## 4 4 11 8 0.2 0.804 ## 5 5 14 12 0.2 0.920 ## 6 6 12 11 0.2 0.942 ## 7 7 11 9 0.2 0.810 ## 8 8 9 9 0.2 0.724 ## 9 9 14 14 0.2 0.880 ## 10 10 11 9 0.2 0.677 ## # ℹ 90 more rows ggplot( df_second_w, aes(x = w) ) + geom_histogram( fill = &quot;steelblue&quot;, alpha = 0.8 ) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Next, we simulate bid data from first-price sealed bid auctions. The setting is the same as the second-price auctions expect for the auction rule. An equilibrium bidding strategy is to participate and bid: \\[ \\beta(x) = x - \\frac{\\int_{r_t}^x F_X(t)^{N - 1}}{F_X(x)^{N - 1}}, \\] if \\(x \\ge r\\) and not to participate otherwise. Write a function bid_first(x, r, alpha, beta, n) that returns the equilibrium bid. To integrate a function, use integrate function in R. It returns 0 if \\(x &lt; r\\). # compute bid from first-price auction n &lt;- N[1] m &lt;- N[1] x &lt;- valuation[1, &quot;x&quot;] %&gt;% as.numeric(); x ## [1] 0.3902289 r &lt;- reserve[1, &quot;r&quot;] %&gt;% as.numeric(); r ## [1] 0.2 b &lt;- bid_first( x = x, r = r, alpha = alpha, beta = beta, n = n ); b ## [1] 0.3596662 x &lt;- r/2; x ## [1] 0.1 b &lt;- bid_first( x = x, r = r, alpha = alpha, beta = beta, n = n ); b ## [1] 0 b &lt;- bid_first( x = 1, r = r, alpha = alpha, beta = beta, n = n ); b ## [1] 0.7978258 Write a function compute_bids_first(valuation, reserve, alpha, beta) that returns bids from each first-price auctions. It returns bids below the reserve price. # compute bid data from first-price auctions df_first &lt;- compute_bids_first( valuation = valuation, reserve = reserve, alpha = alpha, beta = beta ) df_first ## # A tibble: 994 × 7 ## t i x r n m b ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0.390 0.2 8 8 0.360 ## 2 1 2 0.410 0.2 8 8 0.378 ## 3 1 3 0.422 0.2 8 8 0.388 ## 4 1 4 0.637 0.2 8 8 0.577 ## 5 1 5 0.450 0.2 8 8 0.413 ## 6 1 6 0.359 0.2 8 8 0.332 ## 7 1 7 0.837 0.2 8 8 0.731 ## 8 1 8 0.440 0.2 8 8 0.404 ## 9 2 1 0.449 0.2 10 10 0.420 ## 10 2 2 0.472 0.2 10 10 0.441 ## # ℹ 984 more rows ggplot( df_first, aes(x = b) ) + geom_histogram( fill = &quot;steelblue&quot;, alpha = 0.8 ) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Write a function compute_winning_bids_first(valuation, reserve, alpha, beta) that returns only the winning bids from each first-price auction. It will call compute_bids_first inside the function. It does not return anything when no bidder bids above the reserve price. # compute winning bids from first-price auctions df_first_w &lt;- compute_winning_bids_first( valuation = valuation, reserve = reserve, alpha = alpha, beta = beta ) df_first_w ## # A tibble: 100 × 5 ## t n m r w ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8 8 0.2 0.731 ## 2 2 10 10 0.2 0.638 ## 3 3 7 5 0.2 0.525 ## 4 4 11 8 0.2 0.818 ## 5 5 14 12 0.2 0.842 ## 6 6 12 11 0.2 0.833 ## 7 7 11 9 0.2 0.772 ## 8 8 9 9 0.2 0.753 ## 9 9 14 14 0.2 0.849 ## 10 10 11 9 0.2 0.803 ## # ℹ 90 more rows ggplot( df_first_w, aes(x = w) ) + geom_histogram( fill = &quot;steelblue&quot;, alpha = 0.8 ) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 19.2 Estimate the parameters We first estimate the parameters from the winning bids data of second-price auctions. We estimate the parameters by maximizing a log-likelihood. \\[ l(\\alpha, \\beta) := \\frac{1}{T} \\sum_{t = 1}^T \\ln\\frac{h_t(w_t)^{1\\{m_t &gt; 1\\}} \\mathbb{P}\\{m_t = 1\\}^{1\\{m_t = 1\\}}}{1 - \\mathbb{P}\\{m_t = 0\\}}, \\] where: \\[ \\mathbb{P}\\{m_t = 0\\} := F_X(r_t)^{n_t}, \\] \\[ \\mathbb{P}\\{m_t = 1\\} := n_t F_X(r_t; \\alpha, \\beta)^{n_t - 1} [1 - F_X(r_t; \\alpha, \\beta)]. \\] \\[ h_t(w_t) := n_t (n_t - 1) F_X(w_t; \\alpha, \\beta)^{n_t - 2} [1 - F_X(w_t; \\alpha, \\beta)] f_X(w_t; \\alpha, \\beta). \\] Write a function compute_p_second_w(w, r, m, n, alpha, beta) that computes \\(\\mathbb{P}\\{m_t = 1\\}\\) if \\(m_t = 1\\) and \\(h_t(w_t)\\) if \\(m_t &gt; 1\\). # compute probability density for winning bids from a second-price auction w &lt;- df_second_w[1, ]$w r &lt;- df_second_w[1, ]$r m &lt;- df_second_w[1, ]$m n &lt;- df_second_w[1, ]$n compute_p_second_w( w = w, r = r, m = m, n = n, alpha = alpha, beta = beta ) ## [1] 2.752949 Write a function compute_m0(r, n, alpha, beta) that computes \\(\\mathbb{P}\\{m_t = 0\\}\\). # compute non-participation probability compute_m0( r = r, n = n, alpha = alpha, beta = beta ) ## [1] 1.368569e-08 Write a function compute_loglikelihood_second_price_w(theta, df_second_w) that computes the log-likelihood for a second-price auction winning bid data. # compute log-likelihood for winning bids from second-price auctions theta &lt;- c( alpha, beta ) compute_loglikelihood_second_price_w( theta = theta, df_second_w = df_second_w ) ## [1] 0.9849261 Compare the value of objective function around the true parameters. # label label &lt;- c( &quot;\\\\alpha&quot;, &quot;\\\\beta&quot; ) label &lt;- paste( &quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot; ) # compute the graph graph &lt;- foreach ( i = 1:length(theta) ) %do% { theta_i &lt;- theta[i] theta_i_list &lt;- theta_i * seq( 0.8, 1.2, by = 0.05 ) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .packages = c( &quot;EmpiricalIO&quot;, &quot;foreach&quot;, &quot;magrittr&quot; ), .combine = &quot;rbind&quot; ) %dopar% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_loglikelihood_second_price_w( theta_j, df_second_w ) return(objective_ij) } df_graph &lt;- data.frame( x = as.numeric(theta_i_list), y = as.numeric(objective_i) ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y ) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a9/second_parametric_graph.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a9/second_parametric_graph.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] Estimate the parameters by maximizing the log-likelihood. result_second_parametric &lt;- optim( par = theta, fn = compute_loglikelihood_second_price_w, df_second_w = df_second_w, method = &quot;L-BFGS-B&quot;, control = list(fnscale = -1) ) saveRDS( result_second_parametric, file = &quot;lecture/data/a9/result_second_parametric.rds&quot; %&gt;% here::here() ) result_second_parametric &lt;- readRDS( file = &quot;lecture/data/a9/result_second_parametric.rds&quot; %&gt;% here::here() ) result_second_parametric ## $par ## [1] 2.199238 2.078327 ## ## $value ## [1] 0.9883372 ## ## $counts ## function gradient ## 11 11 ## ## $convergence ## [1] 0 ## ## $message ## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; comparison &lt;- data.frame( true = theta, estimate = result_second_parametric$par ) comparison ## true estimate ## 1 2 2.199238 ## 2 2 2.078327 Next, we estimate the parameters from the winning bids data from first-price auctions. We estimate the parameters by maximizing a log-likelihood. Write a function inverse_bid_equation(x, b, r, alpha, beta, n) that returns \\(\\beta(x) - b\\) for a bid \\(b\\). Write a function inverse_bid_first(b, r, alpha, beta, n) that is an inverse function bid_first with respect to the signal, that is, \\[ \\eta(b) := \\beta^{-1}(b). \\] To do so, we can use a built-in function called uniroot, which solves \\(x\\) such that \\(f(x) = 0\\) for scalar \\(x\\). In uniroot, lower and upper are set at \\(r_t\\) and \\(1\\), respectively. r &lt;- df_first_w[1, &quot;r&quot;] %&gt;% as.numeric() n &lt;- df_first_w[1, &quot;n&quot;] %&gt;% as.integer() b &lt;- 0.5 * r + 0.5 x &lt;- 0.5 # compute inverse bid equation inverse_bid_equation( x = x, b = b, r = r, alpha = alpha, beta = beta, n = n ) ## [1] -0.1421105 # compute inverse bid inverse_bid_first( b = b, r = r, alpha = alpha, beta = beta, n = n ) ## [1] 0.6653238 The log-likelihood conditional on \\(m_t \\ge 1\\) is: \\[ l(\\alpha, \\beta) := \\frac{1}{T}\\sum_{t = 1}^T \\log \\frac{h_t(w_t)}{1 - F_X(r_t)^{n_t}}, \\] where the probability density of having \\(w_t\\) is: \\[ \\begin{split} h_t(w_t) &amp;= n_t F_X[\\eta_t(w_t)]^{n_t - 1} f_X[\\eta_t(w_t)] \\eta_t&#39;(w_t)\\\\ &amp;= \\frac{n_t F_X[\\eta_t(w_t)]^{n_t}}{(n_t - 1)[\\eta_t(w_t) - w_t]}, \\end{split} \\] where the second equation is from the first-order condition. Write a function compute_p_first_w(w, r, alpha, beta, n) that returns \\(h_t(w)\\). Remark that the equilibrium bid at specific parameters is bid_first(1, r, alpha, beta, n). If the observed wining bid w is above the upper limit, the function will issue an error. Therefore, inside the function compute_p_first_w(w, r, alpha, beta, n), check if w is above bid_first(1, r, alpha, beta, n) and if so return \\(10^{-6}\\). # compute probability density for a winning bid from a first-price auction w &lt;- 0.5 compute_p_first_w( w = w, r = r, alpha = alpha, beta = beta, n = n ) ## [1] 0.2720049 upper &lt;- bid_first( x = 1, r = r, alpha = alpha, beta = beta, n = n ) compute_p_first_w( w = upper + 1, r = r, alpha = alpha, beta = beta, n = n ) ## [1] 1e-06 Write a function compute_loglikelihood_first_price_w(theta, df_first_w) that computes the log-likelihood for a first-price auction winning bid data. # compute log-likelihood for winning bids for first-price auctions compute_loglikelihood_first_price_w( theta = theta, df_first_w = df_first_w ) ## [1] 1.597414 Compare the value of the objective function around the true parameters. theta &lt;- c( alpha, beta ) # label label &lt;- c( &quot;\\\\alpha&quot;, &quot;\\\\beta&quot; ) label &lt;- paste( &quot;$&quot;, label, &quot;$&quot;, sep = &quot;&quot; ) # compute the graph graph &lt;- foreach ( i = 1:length(theta) ) %do% { theta_i &lt;- theta[i] theta_i_list &lt;- theta_i * seq( 0.8, 1.2, by = 0.05 ) objective_i &lt;- foreach ( j = 1:length(theta_i_list), .packages = c( &quot;EmpiricalIO&quot;, &quot;foreach&quot;, &quot;magrittr&quot; ), .combine = &quot;rbind&quot; ) %dopar% { theta_ij &lt;- theta_i_list[j] theta_j &lt;- theta theta_j[i] &lt;- theta_ij objective_ij &lt;- compute_loglikelihood_first_price_w( theta_j, df_first_w ) return(objective_ij) } df_graph &lt;- data.frame( x = as.numeric(theta_i_list), y = as.numeric(objective_i) ) g &lt;- ggplot( data = df_graph, aes( x = x, y = y ) ) + geom_point() + geom_vline( xintercept = theta_i, linetype = &quot;dotted&quot; ) + ylab(&quot;objective function&quot;) + xlab(TeX(label[i])) + theme_classic() return(g) } saveRDS( graph, file = &quot;lecture/data/a9/first_parametric_graph.rds&quot; %&gt;% here::here() ) graph &lt;- readRDS( file = &quot;lecture/data/a9/first_parametric_graph.rds&quot; %&gt;% here::here() ) graph ## [[1]] ## ## [[2]] Estimate the parameters by maximizing the log-likelihood. Set the lower bounds at zero. Use the Nelder-Mead method. Otherwise the parameter search can go to extreme values because of the discontinuity at the point where the upper limit is below the observed bid. result_first_parametric &lt;- optim( par = theta, fn = compute_loglikelihood_first_price_w, df_first_w = df_first_w, method = &quot;Nelder-Mead&quot;, control = list(fnscale = -1) ) saveRDS( result_first_parametric, file = &quot;lecture/data/a9/result_first_parametric.rds&quot; %&gt;% here::here() ) result_first_parametric &lt;- readRDS( file = &quot;lecture/data/a9/result_first_parametric.rds&quot; %&gt;% here::here() ) result_first_parametric ## $par ## [1] 1.977676 2.004715 ## ## $value ## [1] 1.607161 ## ## $counts ## function gradient ## 91 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL comparison &lt;- data.frame( true = theta, estimate = result_first_parametric$par ) comparison ## true estimate ## 1 2 1.977676 ## 2 2 2.004715 Finally, we non-parametrically estimate the distribution of the valuation using bid data from first-price auctions df_first. Write a function F_b(b) that returns an empirical cumulative distribution at b. This can be obtained by using a function ecdf. Also, write a function f_b(b) that returns an empirical probability density at b. This can be obtained by combining functions approxfun and density. # cumulative distribution ggplot( df_first, aes(x = b) ) + stat_ecdf(color = &quot;steelblue&quot;) + xlab(&quot;bid&quot;) + ylab(&quot;cumulative distribution&quot;) + theme_classic() F_b &lt;- ecdf(df_first$b) F_b(0.4) ## [1] 0.4104628 F_b(0.6) ## [1] 0.7173038 # probability density ggplot( df_first, aes(x = b) ) + geom_density(fill = &quot;steelblue&quot;) + theme_classic() f_b &lt;- approxfun(density(df_first$b)) f_b(0.4) ## [1] 1.680124 f_b(0.6) ## [1] 1.469983 The equilibrium distribution and density of the highest rival’s bid are: \\[ H_b(b) := F_b(b)^{n - 1}, \\] \\[ h_b(b) := (n - 1) f_b(b) F_b(b)^{n - 2}. \\] Write a function H_b(b, n, F_b) and h_b(b, n, F_b, f_b) that return the equilibrium distribution and density of the highest rival’s bid at point b. H_b( b = 0.4, n = n, F_b = F_b ) ## [1] 0.001962983 h_b( b = 0.4, n = n, F_b = F_b, f_b ) ## [1] 0.05624476 When a bidder bids \\(b\\), the implied valuation of her is: \\[ x = b + \\frac{H_b(b)}{h_b(b)}. \\] Write a function compute_implied_valuation(b, n, r) that returns the implied valuation given a bid. Let it return \\(x = 0\\) if \\(b &lt; r\\), because we cannot know the value when the bid is below the reserve price. r &lt;- df_first[1, &quot;r&quot;] n &lt;- df_first[1, &quot;n&quot;] compute_implied_valuation( b = 0.4, n = n, r = r, F_b = F_b, f_b = f_b ) ## n ## 1 0.4349007 Obtain the vector of implied valuations from the vector of bids and draw the empirical cumulative distribution. Overlay it with the true empirical cumulative distribution of the valuations. valuation_implied &lt;- df_first %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate( x = compute_implied_valuation( b, n, r, F_b, f_b ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(x) %&gt;% dplyr::mutate(type = &quot;estimate&quot;) valuation_true &lt;- valuation %&gt;% dplyr::select(x) %&gt;% dplyr::mutate(type = &quot;true&quot;) valuation_plot &lt;- rbind( valuation_true, valuation_implied ) ggplot( valuation_plot, aes( x = x, color = type ) ) + stat_ecdf() + theme_classic() "]]
